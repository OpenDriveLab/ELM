| distributed init (rank 4, world 8): env://| distributed init (rank 1, world 8): env://

| distributed init (rank 3, world 8): env://
| distributed init (rank 6, world 8): env://
| distributed init (rank 5, world 8): env://
| distributed init (rank 0, world 8): env://
| distributed init (rank 7, world 8): env://
| distributed init (rank 2, world 8): env://
Using downloaded and verified file: /cpfs01/user/zhouyunsong/zhouys/Datasets/VLM/drivelm_train.json
Using downloaded and verified file: /cpfs01/user/zhouyunsong/zhouys/Datasets/VLM/drivelm_val.json
Using downloaded and verified file: /cpfs01/user/zhouyunsong/zhouys/Datasets/VLM/drivelm_val.json
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 28.014 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.7 seconds.
======
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 28.973 seconds.
======
Reverse indexing ...
Done reverse indexing in 6.6 seconds.
======
======
Loading NuScenes tables for version v1.0-trainval...
23 category,
8 attribute,
4 visibility,
64386 instance,
12 sensor,
10200 calibrated_sensor,
2631083 ego_pose,
68 log,
850 scene,
34149 sample,
2631083 sample_data,
1166187 sample_annotation,
4 map,
Done loading in 32.918 seconds.
======
Reverse indexing ...
Done reverse indexing in 7.4 seconds.
======
Position interpolate from 16x16 to 26x26
Train: data epoch: [0]  [   0/7110]  eta: 2 days, 12:09:20  lr: 0.000000  loss: 6.5016  time: 30.4586  data: 0.0000  max mem: 42072
Train: data epoch: [0]  [  50/7110]  eta: 3:10:35  lr: 0.000001  loss: 5.4006  time: 1.0307  data: 0.0000  max mem: 58438
Train: data epoch: [0]  [ 100/7110]  eta: 2:32:12  lr: 0.000001  loss: 4.0517  time: 1.0023  data: 0.0000  max mem: 58438
Train: data epoch: [0]  [ 150/7110]  eta: 2:18:59  lr: 0.000002  loss: 3.1542  time: 0.9915  data: 0.0000  max mem: 59788
Train: data epoch: [0]  [ 200/7110]  eta: 2:12:00  lr: 0.000002  loss: 2.9226  time: 0.9670  data: 0.0000  max mem: 64924
Train: data epoch: [0]  [ 250/7110]  eta: 2:07:30  lr: 0.000003  loss: 2.6839  time: 0.9972  data: 0.0000  max mem: 64924
Train: data epoch: [0]  [ 300/7110]  eta: 2:04:48  lr: 0.000003  loss: 2.1318  time: 0.9913  data: 0.0000  max mem: 64924
Train: data epoch: [0]  [ 350/7110]  eta: 2:01:58  lr: 0.000004  loss: 1.8734  time: 0.9485  data: 0.0000  max mem: 64924
Train: data epoch: [0]  [ 400/7110]  eta: 2:00:03  lr: 0.000004  loss: 1.5382  time: 1.0190  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [ 450/7110]  eta: 1:58:32  lr: 0.000005  loss: 1.8564  time: 1.0660  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [ 500/7110]  eta: 1:56:59  lr: 0.000005  loss: 1.6191  time: 0.9930  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [ 550/7110]  eta: 1:55:33  lr: 0.000006  loss: 1.3590  time: 0.9937  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [ 600/7110]  eta: 1:54:03  lr: 0.000006  loss: 1.8583  time: 0.9672  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [ 650/7110]  eta: 1:52:50  lr: 0.000007  loss: 1.3038  time: 1.0472  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [ 700/7110]  eta: 1:51:45  lr: 0.000007  loss: 0.4804  time: 1.0235  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [ 750/7110]  eta: 1:50:32  lr: 0.000008  loss: 1.6449  time: 0.9553  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [ 800/7110]  eta: 1:49:16  lr: 0.000008  loss: 0.4740  time: 0.9684  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [ 850/7110]  eta: 1:48:18  lr: 0.000009  loss: 1.6879  time: 0.9812  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [ 900/7110]  eta: 1:47:17  lr: 0.000009  loss: 1.5742  time: 0.9676  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [ 950/7110]  eta: 1:46:23  lr: 0.000010  loss: 0.6669  time: 1.0658  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [1000/7110]  eta: 1:45:19  lr: 0.000010  loss: 0.3440  time: 1.0035  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [1050/7110]  eta: 1:44:17  lr: 0.000010  loss: 1.0172  time: 0.9796  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [1100/7110]  eta: 1:43:08  lr: 0.000010  loss: 1.6321  time: 0.9595  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [1150/7110]  eta: 1:42:03  lr: 0.000010  loss: 0.9416  time: 0.9978  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [1200/7110]  eta: 1:41:10  lr: 0.000010  loss: 0.6218  time: 0.9741  data: 0.0000  max mem: 65356
Train: data epoch: [0]  [1250/7110]  eta: 1:40:14  lr: 0.000010  loss: 0.3461  time: 1.0324  data: 0.0000  max mem: 65357
Train: data epoch: [0]  [1300/7110]  eta: 1:39:14  lr: 0.000010  loss: 0.7285  time: 1.0129  data: 0.0000  max mem: 65357
Train: data epoch: [0]  [1350/7110]  eta: 1:38:16  lr: 0.000010  loss: 1.3667  time: 0.9535  data: 0.0000  max mem: 65357
Train: data epoch: [0]  [1400/7110]  eta: 1:37:19  lr: 0.000010  loss: 0.4522  time: 0.9966  data: 0.0000  max mem: 65357
Train: data epoch: [0]  [1450/7110]  eta: 1:36:17  lr: 0.000010  loss: 0.5644  time: 0.9768  data: 0.0000  max mem: 65357
Train: data epoch: [0]  [1500/7110]  eta: 1:35:23  lr: 0.000010  loss: 1.3005  time: 1.0216  data: 0.0000  max mem: 65357
Train: data epoch: [0]  [1550/7110]  eta: 1:34:25  lr: 0.000010  loss: 0.5594  time: 0.9618  data: 0.0000  max mem: 65357
Train: data epoch: [0]  [1600/7110]  eta: 1:33:28  lr: 0.000010  loss: 0.5559  time: 0.9722  data: 0.0000  max mem: 65357
Train: data epoch: [0]  [1650/7110]  eta: 1:32:30  lr: 0.000010  loss: 0.9278  time: 1.0097  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [1700/7110]  eta: 1:31:38  lr: 0.000010  loss: 0.5999  time: 1.0137  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [1750/7110]  eta: 1:30:37  lr: 0.000010  loss: 1.0397  time: 0.9309  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [1800/7110]  eta: 1:29:44  lr: 0.000010  loss: 1.2620  time: 0.9763  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [1850/7110]  eta: 1:28:53  lr: 0.000010  loss: 0.3886  time: 1.0244  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [1900/7110]  eta: 1:28:00  lr: 0.000010  loss: 0.1456  time: 1.0304  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [1950/7110]  eta: 1:27:05  lr: 0.000010  loss: 1.0779  time: 1.0211  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2000/7110]  eta: 1:26:19  lr: 0.000010  loss: 0.4749  time: 1.0022  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2050/7110]  eta: 1:25:27  lr: 0.000010  loss: 0.6130  time: 1.0110  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2100/7110]  eta: 1:24:31  lr: 0.000010  loss: 0.8592  time: 0.9805  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2150/7110]  eta: 1:23:36  lr: 0.000010  loss: 0.4593  time: 0.9534  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2200/7110]  eta: 1:22:45  lr: 0.000010  loss: 0.3571  time: 1.0000  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2250/7110]  eta: 1:21:52  lr: 0.000010  loss: 0.5673  time: 1.0239  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2300/7110]  eta: 1:20:57  lr: 0.000010  loss: 0.8432  time: 0.9760  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2350/7110]  eta: 1:20:07  lr: 0.000010  loss: 0.2927  time: 1.0056  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2400/7110]  eta: 1:19:14  lr: 0.000010  loss: 0.2978  time: 0.9678  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2450/7110]  eta: 1:18:19  lr: 0.000010  loss: 0.2470  time: 0.9848  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2500/7110]  eta: 1:17:27  lr: 0.000010  loss: 0.3668  time: 0.9464  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2550/7110]  eta: 1:16:36  lr: 0.000010  loss: 1.0870  time: 0.9780  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2600/7110]  eta: 1:15:43  lr: 0.000010  loss: 1.0982  time: 1.0211  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2650/7110]  eta: 1:14:50  lr: 0.000010  loss: 0.4978  time: 0.9387  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2700/7110]  eta: 1:13:59  lr: 0.000010  loss: 1.1012  time: 1.0181  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2750/7110]  eta: 1:13:07  lr: 0.000010  loss: 0.8816  time: 0.9712  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2800/7110]  eta: 1:12:17  lr: 0.000010  loss: 0.1559  time: 1.0145  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2850/7110]  eta: 1:11:27  lr: 0.000010  loss: 0.4475  time: 1.0214  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2900/7110]  eta: 1:10:38  lr: 0.000010  loss: 0.5591  time: 1.0550  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [2950/7110]  eta: 1:09:49  lr: 0.000010  loss: 0.7071  time: 1.0602  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3000/7110]  eta: 1:09:01  lr: 0.000010  loss: 0.5700  time: 1.0078  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3050/7110]  eta: 1:08:11  lr: 0.000010  loss: 0.7454  time: 1.0204  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3100/7110]  eta: 1:07:20  lr: 0.000010  loss: 0.5962  time: 0.9743  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3150/7110]  eta: 1:06:30  lr: 0.000010  loss: 0.4963  time: 1.0070  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3200/7110]  eta: 1:05:38  lr: 0.000010  loss: 1.0698  time: 0.9953  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3250/7110]  eta: 1:04:46  lr: 0.000010  loss: 0.5719  time: 0.9343  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3300/7110]  eta: 1:03:55  lr: 0.000010  loss: 0.4294  time: 1.0093  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3350/7110]  eta: 1:03:07  lr: 0.000010  loss: 0.4175  time: 1.0101  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3400/7110]  eta: 1:02:16  lr: 0.000010  loss: 0.7675  time: 0.9688  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3450/7110]  eta: 1:01:25  lr: 0.000010  loss: 0.7682  time: 0.9567  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3500/7110]  eta: 1:00:36  lr: 0.000010  loss: 0.7332  time: 1.0182  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3550/7110]  eta: 0:59:44  lr: 0.000010  loss: 0.1499  time: 1.0034  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3600/7110]  eta: 0:58:54  lr: 0.000010  loss: 0.6498  time: 1.0370  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3650/7110]  eta: 0:58:03  lr: 0.000010  loss: 0.4494  time: 1.0034  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3700/7110]  eta: 0:57:12  lr: 0.000010  loss: 0.9037  time: 0.9817  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3750/7110]  eta: 0:56:21  lr: 0.000010  loss: 1.0678  time: 0.9796  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3800/7110]  eta: 0:55:30  lr: 0.000010  loss: 0.2957  time: 0.9692  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3850/7110]  eta: 0:54:39  lr: 0.000010  loss: 0.1938  time: 0.9709  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3900/7110]  eta: 0:53:47  lr: 0.000010  loss: 0.9880  time: 0.9811  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [3950/7110]  eta: 0:52:56  lr: 0.000010  loss: 0.5571  time: 1.0035  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4000/7110]  eta: 0:52:05  lr: 0.000010  loss: 0.7357  time: 0.9282  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4050/7110]  eta: 0:51:14  lr: 0.000010  loss: 0.6105  time: 0.9521  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4100/7110]  eta: 0:50:24  lr: 0.000010  loss: 0.3309  time: 0.9905  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4150/7110]  eta: 0:49:33  lr: 0.000010  loss: 1.9514  time: 0.9669  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4200/7110]  eta: 0:48:43  lr: 0.000010  loss: 0.4184  time: 1.0145  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4250/7110]  eta: 0:47:53  lr: 0.000010  loss: 0.3231  time: 0.9757  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4300/7110]  eta: 0:47:01  lr: 0.000010  loss: 0.6304  time: 0.9631  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4350/7110]  eta: 0:46:10  lr: 0.000010  loss: 0.6268  time: 0.9483  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4400/7110]  eta: 0:45:19  lr: 0.000010  loss: 0.2668  time: 0.9801  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4450/7110]  eta: 0:44:29  lr: 0.000010  loss: 0.3888  time: 0.9937  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4500/7110]  eta: 0:43:39  lr: 0.000010  loss: 2.0125  time: 1.0190  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4550/7110]  eta: 0:42:49  lr: 0.000010  loss: 1.0871  time: 1.0560  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4600/7110]  eta: 0:41:58  lr: 0.000010  loss: 0.5307  time: 1.0194  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4650/7110]  eta: 0:41:08  lr: 0.000010  loss: 0.2036  time: 1.0346  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4700/7110]  eta: 0:40:17  lr: 0.000010  loss: 0.4397  time: 0.9794  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4750/7110]  eta: 0:39:28  lr: 0.000010  loss: 0.6668  time: 1.0262  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4800/7110]  eta: 0:38:37  lr: 0.000010  loss: 0.2257  time: 1.0023  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4850/7110]  eta: 0:37:47  lr: 0.000010  loss: 0.2105  time: 0.9614  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4900/7110]  eta: 0:36:57  lr: 0.000010  loss: 0.4173  time: 0.9980  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [4950/7110]  eta: 0:36:08  lr: 0.000010  loss: 1.2362  time: 0.9785  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5000/7110]  eta: 0:35:17  lr: 0.000010  loss: 0.6918  time: 0.9911  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5050/7110]  eta: 0:34:27  lr: 0.000010  loss: 0.5450  time: 1.0044  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5100/7110]  eta: 0:33:37  lr: 0.000010  loss: 0.3980  time: 1.0172  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5150/7110]  eta: 0:32:47  lr: 0.000010  loss: 1.1131  time: 0.9743  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5200/7110]  eta: 0:31:56  lr: 0.000010  loss: 0.3683  time: 0.9795  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5250/7110]  eta: 0:31:07  lr: 0.000010  loss: 1.9829  time: 1.0362  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5300/7110]  eta: 0:30:17  lr: 0.000010  loss: 0.4070  time: 1.0176  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5350/7110]  eta: 0:29:27  lr: 0.000010  loss: 0.4344  time: 0.9986  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5400/7110]  eta: 0:28:36  lr: 0.000010  loss: 0.1414  time: 0.9823  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5450/7110]  eta: 0:27:45  lr: 0.000010  loss: 0.7159  time: 0.9764  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5500/7110]  eta: 0:26:55  lr: 0.000010  loss: 0.1077  time: 0.9687  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5550/7110]  eta: 0:26:05  lr: 0.000010  loss: 0.4359  time: 1.0120  data: 0.0000  max mem: 65660
Train: data epoch: [0]  [5600/7110]  eta: 0:25:14  lr: 0.000010  loss: 0.1714  time: 0.9678  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [5650/7110]  eta: 0:24:24  lr: 0.000010  loss: 0.6747  time: 1.0114  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.2987  time: 0.9960  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.4057  time: 0.9826  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [5800/7110]  eta: 0:21:53  lr: 0.000010  loss: 0.1453  time: 0.9867  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.8887  time: 1.0071  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 1.0727  time: 0.9647  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.1757  time: 1.0356  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 2.0410  time: 1.0220  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 1.0010  time: 0.9509  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.7885  time: 1.0523  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.2775  time: 1.0009  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.2934  time: 0.9621  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.2545  time: 0.9928  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.8846  time: 0.9988  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.1417  time: 0.9509  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.7561  time: 0.9880  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.8289  time: 0.9936  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.5405  time: 0.9776  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.6291  time: 0.9902  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.6837  time: 0.9936  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.6745  time: 1.0140  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.7464  time: 0.9802  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 1.0512  time: 0.9939  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.6514  time: 0.9822  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.6837  time: 1.0058  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.3574  time: 0.9397  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 1.8058  time: 0.9475  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.6520  time: 0.9473  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.5125  time: 1.0030  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 1.0908  time: 0.9557  data: 0.0000  max mem: 65949
Train: data epoch: [0]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 1.5752  time: 1.0843  data: 0.0000  max mem: 65949
Train: data epoch: [0] Total time: 1:58:45 (1.0022 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 7:23:37    time: 24.3531  data: 22.0108  max mem: 65949
Evaluation  [  10/1093]  eta: 1:00:42    time: 3.3634  data: 2.0027  max mem: 65949
Evaluation  [  20/1093]  eta: 0:41:48    time: 1.2374  data: 0.0014  max mem: 65949
Evaluation  [  30/1093]  eta: 0:34:43    time: 1.1878  data: 0.0009  max mem: 65949
Evaluation  [  40/1093]  eta: 0:31:53    time: 1.2699  data: 0.0009  max mem: 65949
Evaluation  [  50/1093]  eta: 0:29:51    time: 1.3428  data: 0.0009  max mem: 65949
Evaluation  [  60/1093]  eta: 0:27:51    time: 1.2109  data: 0.0009  max mem: 65949
Evaluation  [  70/1093]  eta: 0:27:09    time: 1.2751  data: 0.0009  max mem: 65949
Evaluation  [  80/1093]  eta: 0:26:14    time: 1.3572  data: 0.0009  max mem: 65949
Evaluation  [  90/1093]  eta: 0:25:37    time: 1.3209  data: 0.0009  max mem: 65949
Evaluation  [ 100/1093]  eta: 0:25:17    time: 1.4222  data: 0.0009  max mem: 65949
Evaluation  [ 110/1093]  eta: 0:24:57    time: 1.4758  data: 0.0009  max mem: 65949
Evaluation  [ 120/1093]  eta: 0:24:20    time: 1.3637  data: 0.0009  max mem: 65949
Evaluation  [ 130/1093]  eta: 0:24:01    time: 1.3552  data: 0.0009  max mem: 65949
Evaluation  [ 140/1093]  eta: 0:23:19    time: 1.2736  data: 0.0009  max mem: 65949
Evaluation  [ 150/1093]  eta: 0:22:43    time: 1.1116  data: 0.0009  max mem: 65949
Evaluation  [ 160/1093]  eta: 0:22:23    time: 1.2379  data: 0.0009  max mem: 65949
Evaluation  [ 170/1093]  eta: 0:21:54    time: 1.2552  data: 0.0009  max mem: 65949
Evaluation  [ 180/1093]  eta: 0:21:34    time: 1.2412  data: 0.0009  max mem: 65949
Evaluation  [ 190/1093]  eta: 0:21:20    time: 1.3683  data: 0.0009  max mem: 65949
Evaluation  [ 200/1093]  eta: 0:20:55    time: 1.2939  data: 0.0009  max mem: 65949
Evaluation  [ 210/1093]  eta: 0:20:46    time: 1.3464  data: 0.0009  max mem: 65949
Evaluation  [ 220/1093]  eta: 0:20:18    time: 1.2993  data: 0.0009  max mem: 65949
Evaluation  [ 230/1093]  eta: 0:19:58    time: 1.1527  data: 0.0009  max mem: 65949
Evaluation  [ 240/1093]  eta: 0:19:37    time: 1.2162  data: 0.0009  max mem: 65949
Evaluation  [ 250/1093]  eta: 0:19:25    time: 1.3168  data: 0.0009  max mem: 65949
Evaluation  [ 260/1093]  eta: 0:19:19    time: 1.5202  data: 0.0009  max mem: 65949
Evaluation  [ 270/1093]  eta: 0:18:57    time: 1.3754  data: 0.0009  max mem: 65949
Evaluation  [ 280/1093]  eta: 0:18:39    time: 1.1881  data: 0.0009  max mem: 65949
Evaluation  [ 290/1093]  eta: 0:18:29    time: 1.3703  data: 0.0009  max mem: 65949
Evaluation  [ 300/1093]  eta: 0:18:05    time: 1.2587  data: 0.0009  max mem: 65949
Evaluation  [ 310/1093]  eta: 0:17:47    time: 1.0984  data: 0.0009  max mem: 65949
Evaluation  [ 320/1093]  eta: 0:17:31    time: 1.2243  data: 0.0009  max mem: 65949
Evaluation  [ 330/1093]  eta: 0:17:14    time: 1.2386  data: 0.0009  max mem: 65949
Evaluation  [ 340/1093]  eta: 0:17:01    time: 1.2913  data: 0.0009  max mem: 65949
Evaluation  [ 350/1093]  eta: 0:16:46    time: 1.3405  data: 0.0009  max mem: 65949
Evaluation  [ 360/1093]  eta: 0:16:30    time: 1.2572  data: 0.0009  max mem: 65949
Evaluation  [ 370/1093]  eta: 0:16:14    time: 1.2320  data: 0.0009  max mem: 65949
Evaluation  [ 380/1093]  eta: 0:16:03    time: 1.3604  data: 0.0009  max mem: 65949
Evaluation  [ 390/1093]  eta: 0:15:46    time: 1.3202  data: 0.0009  max mem: 65949
Evaluation  [ 400/1093]  eta: 0:15:34    time: 1.3056  data: 0.0009  max mem: 65949
Evaluation  [ 410/1093]  eta: 0:15:20    time: 1.3493  data: 0.0009  max mem: 65949
Evaluation  [ 420/1093]  eta: 0:15:07    time: 1.3496  data: 0.0009  max mem: 65949
Evaluation  [ 430/1093]  eta: 0:14:54    time: 1.3894  data: 0.0009  max mem: 65949
Evaluation  [ 440/1093]  eta: 0:14:36    time: 1.1974  data: 0.0009  max mem: 65949
Evaluation  [ 450/1093]  eta: 0:14:21    time: 1.1444  data: 0.0009  max mem: 65949
Evaluation  [ 460/1093]  eta: 0:14:07    time: 1.2687  data: 0.0009  max mem: 65949
Evaluation  [ 470/1093]  eta: 0:13:53    time: 1.3067  data: 0.0009  max mem: 65949
Evaluation  [ 480/1093]  eta: 0:13:39    time: 1.3016  data: 0.0009  max mem: 65949
Evaluation  [ 490/1093]  eta: 0:13:26    time: 1.3187  data: 0.0010  max mem: 65949
Evaluation  [ 500/1093]  eta: 0:13:15    time: 1.4285  data: 0.0010  max mem: 65949
Evaluation  [ 510/1093]  eta: 0:13:01    time: 1.3898  data: 0.0010  max mem: 65949
Evaluation  [ 520/1093]  eta: 0:12:47    time: 1.3023  data: 0.0010  max mem: 65949
Evaluation  [ 530/1093]  eta: 0:12:33    time: 1.3217  data: 0.0010  max mem: 65949
Evaluation  [ 540/1093]  eta: 0:12:21    time: 1.3607  data: 0.0009  max mem: 65949
Evaluation  [ 550/1093]  eta: 0:12:08    time: 1.3913  data: 0.0010  max mem: 65949
Evaluation  [ 560/1093]  eta: 0:11:51    time: 1.2097  data: 0.0010  max mem: 65949
Evaluation  [ 570/1093]  eta: 0:11:37    time: 1.1503  data: 0.0010  max mem: 65949
Evaluation  [ 580/1093]  eta: 0:11:23    time: 1.2374  data: 0.0009  max mem: 65949
Evaluation  [ 590/1093]  eta: 0:11:07    time: 1.1367  data: 0.0009  max mem: 65949
Evaluation  [ 600/1093]  eta: 0:10:56    time: 1.3164  data: 0.0009  max mem: 65949
Evaluation  [ 610/1093]  eta: 0:10:43    time: 1.4925  data: 0.0009  max mem: 65949
Evaluation  [ 620/1093]  eta: 0:10:30    time: 1.3423  data: 0.0009  max mem: 65949
Evaluation  [ 630/1093]  eta: 0:10:17    time: 1.3291  data: 0.0009  max mem: 65949
Evaluation  [ 640/1093]  eta: 0:10:02    time: 1.2861  data: 0.0009  max mem: 65949
Evaluation  [ 650/1093]  eta: 0:09:50    time: 1.3313  data: 0.0010  max mem: 65949
Evaluation  [ 660/1093]  eta: 0:09:36    time: 1.3328  data: 0.0010  max mem: 65949
Evaluation  [ 670/1093]  eta: 0:09:23    time: 1.3317  data: 0.0009  max mem: 65949
Evaluation  [ 680/1093]  eta: 0:09:10    time: 1.4232  data: 0.0009  max mem: 65949
Evaluation  [ 690/1093]  eta: 0:08:58    time: 1.4510  data: 0.0009  max mem: 65949
Evaluation  [ 700/1093]  eta: 0:08:44    time: 1.3508  data: 0.0010  max mem: 65949
Evaluation  [ 710/1093]  eta: 0:08:29    time: 1.1645  data: 0.0010  max mem: 65949
Evaluation  [ 720/1093]  eta: 0:08:15    time: 1.1012  data: 0.0009  max mem: 65949
Evaluation  [ 730/1093]  eta: 0:08:00    time: 1.0640  data: 0.0009  max mem: 65949
Evaluation  [ 740/1093]  eta: 0:07:46    time: 1.1238  data: 0.0009  max mem: 65949
Evaluation  [ 750/1093]  eta: 0:07:33    time: 1.2365  data: 0.0009  max mem: 65949
Evaluation  [ 760/1093]  eta: 0:07:20    time: 1.2980  data: 0.0010  max mem: 65949
Evaluation  [ 770/1093]  eta: 0:07:06    time: 1.2990  data: 0.0010  max mem: 65949
Evaluation  [ 780/1093]  eta: 0:06:53    time: 1.3687  data: 0.0010  max mem: 65949
Evaluation  [ 790/1093]  eta: 0:06:39    time: 1.2641  data: 0.0009  max mem: 65949
Evaluation  [ 800/1093]  eta: 0:06:27    time: 1.3019  data: 0.0009  max mem: 65949
Evaluation  [ 810/1093]  eta: 0:06:14    time: 1.4435  data: 0.0010  max mem: 65949
Evaluation  [ 820/1093]  eta: 0:06:00    time: 1.3199  data: 0.0010  max mem: 65949
Evaluation  [ 830/1093]  eta: 0:05:48    time: 1.3457  data: 0.0010  max mem: 65949
Evaluation  [ 840/1093]  eta: 0:05:33    time: 1.2374  data: 0.0010  max mem: 65949
Evaluation  [ 850/1093]  eta: 0:05:20    time: 1.2146  data: 0.0009  max mem: 65949
Evaluation  [ 860/1093]  eta: 0:05:07    time: 1.2471  data: 0.0009  max mem: 65949
Evaluation  [ 870/1093]  eta: 0:04:53    time: 1.1936  data: 0.0009  max mem: 65949
Evaluation  [ 880/1093]  eta: 0:04:41    time: 1.3889  data: 0.0010  max mem: 65949
Evaluation  [ 890/1093]  eta: 0:04:27    time: 1.2670  data: 0.0010  max mem: 65949
Evaluation  [ 900/1093]  eta: 0:04:14    time: 1.2132  data: 0.0010  max mem: 65949
Evaluation  [ 910/1093]  eta: 0:04:01    time: 1.3443  data: 0.0010  max mem: 65949
Evaluation  [ 920/1093]  eta: 0:03:47    time: 1.2412  data: 0.0009  max mem: 65949
Evaluation  [ 930/1093]  eta: 0:03:34    time: 1.1752  data: 0.0009  max mem: 65949
Evaluation  [ 940/1093]  eta: 0:03:21    time: 1.2360  data: 0.0009  max mem: 65949
Evaluation  [ 950/1093]  eta: 0:03:07    time: 1.2918  data: 0.0010  max mem: 65949
Evaluation  [ 960/1093]  eta: 0:02:55    time: 1.4039  data: 0.0010  max mem: 65949
Evaluation  [ 970/1093]  eta: 0:02:41    time: 1.3188  data: 0.0010  max mem: 65949
Evaluation  [ 980/1093]  eta: 0:02:28    time: 1.1302  data: 0.0010  max mem: 65949
Evaluation  [ 990/1093]  eta: 0:02:15    time: 1.2755  data: 0.0010  max mem: 65949
Evaluation  [1000/1093]  eta: 0:02:02    time: 1.4213  data: 0.0010  max mem: 65949
Evaluation  [1010/1093]  eta: 0:01:49    time: 1.3704  data: 0.0010  max mem: 65949
Evaluation  [1020/1093]  eta: 0:01:35    time: 1.1942  data: 0.0010  max mem: 65949
Evaluation  [1030/1093]  eta: 0:01:22    time: 1.1151  data: 0.0010  max mem: 65949
Evaluation  [1040/1093]  eta: 0:01:09    time: 1.2432  data: 0.0009  max mem: 65949
Evaluation  [1050/1093]  eta: 0:00:56    time: 1.2717  data: 0.0009  max mem: 65949
Evaluation  [1060/1093]  eta: 0:00:43    time: 1.2696  data: 0.0009  max mem: 65949
Evaluation  [1070/1093]  eta: 0:00:30    time: 1.3546  data: 0.0009  max mem: 65949
Evaluation  [1080/1093]  eta: 0:00:17    time: 1.3342  data: 0.0009  max mem: 65949
Evaluation  [1090/1093]  eta: 0:00:03    time: 1.2842  data: 0.0009  max mem: 65949
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2483  data: 0.0403  max mem: 65949
Evaluation Total time: 0:23:52 (1.3102 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_0_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [1]  [   0/7110]  eta: 2 days, 5:31:01  lr: 0.000010  loss: 1.6831  time: 27.0973  data: 0.0001  max mem: 65949
Train: data epoch: [1]  [  50/7110]  eta: 2:56:18  lr: 0.000010  loss: 0.1599  time: 0.9545  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 100/7110]  eta: 2:24:22  lr: 0.000010  loss: 0.5076  time: 0.9450  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 150/7110]  eta: 2:15:14  lr: 0.000010  loss: 0.4027  time: 1.0218  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 200/7110]  eta: 2:10:09  lr: 0.000010  loss: 0.2822  time: 0.9781  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 250/7110]  eta: 2:06:37  lr: 0.000010  loss: 0.6142  time: 0.9894  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 300/7110]  eta: 2:04:06  lr: 0.000010  loss: 0.4208  time: 0.9955  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 350/7110]  eta: 2:01:41  lr: 0.000010  loss: 0.8009  time: 1.0171  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 400/7110]  eta: 1:59:34  lr: 0.000010  loss: 0.2627  time: 0.9831  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 450/7110]  eta: 1:57:59  lr: 0.000010  loss: 0.5428  time: 0.9817  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 500/7110]  eta: 1:56:37  lr: 0.000010  loss: 0.2247  time: 0.9874  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 550/7110]  eta: 1:55:11  lr: 0.000010  loss: 0.4284  time: 1.0022  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 600/7110]  eta: 1:54:11  lr: 0.000010  loss: 0.2252  time: 1.0548  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 650/7110]  eta: 1:53:26  lr: 0.000010  loss: 0.6761  time: 1.0749  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 700/7110]  eta: 1:52:17  lr: 0.000010  loss: 0.3348  time: 0.9994  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 750/7110]  eta: 1:51:00  lr: 0.000010  loss: 0.2756  time: 0.9145  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 800/7110]  eta: 1:49:48  lr: 0.000010  loss: 0.5122  time: 0.9906  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 850/7110]  eta: 1:48:54  lr: 0.000010  loss: 0.4038  time: 1.0502  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 900/7110]  eta: 1:47:51  lr: 0.000010  loss: 0.8268  time: 0.9786  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [ 950/7110]  eta: 1:46:47  lr: 0.000010  loss: 1.0066  time: 1.0113  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1000/7110]  eta: 1:45:42  lr: 0.000010  loss: 0.3823  time: 0.9985  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1050/7110]  eta: 1:44:49  lr: 0.000010  loss: 0.5889  time: 1.0579  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1100/7110]  eta: 1:43:51  lr: 0.000010  loss: 1.0790  time: 1.0366  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1150/7110]  eta: 1:42:52  lr: 0.000010  loss: 0.1917  time: 0.9915  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1200/7110]  eta: 1:41:55  lr: 0.000010  loss: 0.4105  time: 1.0050  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1250/7110]  eta: 1:41:01  lr: 0.000010  loss: 1.9203  time: 1.0586  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1300/7110]  eta: 1:40:05  lr: 0.000010  loss: 0.2353  time: 1.0254  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1350/7110]  eta: 1:39:04  lr: 0.000010  loss: 0.6041  time: 1.0154  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1400/7110]  eta: 1:37:59  lr: 0.000010  loss: 0.2690  time: 0.9060  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1450/7110]  eta: 1:37:01  lr: 0.000010  loss: 0.4565  time: 0.9556  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1500/7110]  eta: 1:36:07  lr: 0.000010  loss: 0.2391  time: 1.0038  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1550/7110]  eta: 1:35:08  lr: 0.000010  loss: 0.4177  time: 1.0212  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1600/7110]  eta: 1:34:10  lr: 0.000010  loss: 0.4152  time: 0.9804  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1650/7110]  eta: 1:33:15  lr: 0.000010  loss: 0.4058  time: 0.9622  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1700/7110]  eta: 1:32:24  lr: 0.000010  loss: 0.3466  time: 1.0546  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1750/7110]  eta: 1:31:29  lr: 0.000010  loss: 0.7797  time: 0.9719  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1800/7110]  eta: 1:30:41  lr: 0.000010  loss: 0.3130  time: 1.0626  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1850/7110]  eta: 1:29:45  lr: 0.000010  loss: 0.1864  time: 1.0093  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1900/7110]  eta: 1:28:47  lr: 0.000010  loss: 0.0729  time: 0.9482  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [1950/7110]  eta: 1:27:46  lr: 0.000010  loss: 0.1764  time: 0.9278  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2000/7110]  eta: 1:26:50  lr: 0.000010  loss: 1.8131  time: 0.9515  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2050/7110]  eta: 1:25:57  lr: 0.000010  loss: 1.5916  time: 0.9934  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2100/7110]  eta: 1:25:04  lr: 0.000010  loss: 0.6533  time: 1.0002  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2150/7110]  eta: 1:24:03  lr: 0.000010  loss: 0.3465  time: 0.9341  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2200/7110]  eta: 1:23:11  lr: 0.000010  loss: 1.0681  time: 1.0465  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2250/7110]  eta: 1:22:20  lr: 0.000010  loss: 0.9675  time: 1.0240  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2300/7110]  eta: 1:21:29  lr: 0.000010  loss: 0.6885  time: 1.0239  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2350/7110]  eta: 1:20:37  lr: 0.000010  loss: 0.7784  time: 1.0471  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2400/7110]  eta: 1:19:45  lr: 0.000010  loss: 0.1556  time: 1.0129  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2450/7110]  eta: 1:18:52  lr: 0.000010  loss: 0.3043  time: 0.9984  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2500/7110]  eta: 1:18:00  lr: 0.000010  loss: 0.4866  time: 0.9889  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2550/7110]  eta: 1:17:08  lr: 0.000010  loss: 0.2405  time: 0.9855  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2600/7110]  eta: 1:16:15  lr: 0.000010  loss: 1.1429  time: 1.0387  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2650/7110]  eta: 1:15:25  lr: 0.000010  loss: 0.2278  time: 1.0305  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2700/7110]  eta: 1:14:34  lr: 0.000010  loss: 0.3915  time: 0.9813  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2750/7110]  eta: 1:13:38  lr: 0.000010  loss: 0.5994  time: 0.9588  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2800/7110]  eta: 1:12:46  lr: 0.000010  loss: 0.8722  time: 1.0083  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2850/7110]  eta: 1:11:54  lr: 0.000010  loss: 0.2898  time: 1.0436  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2900/7110]  eta: 1:11:02  lr: 0.000010  loss: 0.3341  time: 1.0082  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [2950/7110]  eta: 1:10:11  lr: 0.000010  loss: 0.4191  time: 0.9671  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3000/7110]  eta: 1:09:20  lr: 0.000010  loss: 0.2696  time: 0.9194  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3050/7110]  eta: 1:08:29  lr: 0.000010  loss: 0.6686  time: 0.9611  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3100/7110]  eta: 1:07:35  lr: 0.000010  loss: 0.3944  time: 0.9504  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3150/7110]  eta: 1:06:45  lr: 0.000010  loss: 0.3453  time: 1.0657  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3200/7110]  eta: 1:05:53  lr: 0.000010  loss: 0.2607  time: 0.9999  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3250/7110]  eta: 1:05:02  lr: 0.000010  loss: 0.3317  time: 1.0110  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3300/7110]  eta: 1:04:11  lr: 0.000010  loss: 0.2757  time: 1.0193  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3350/7110]  eta: 1:03:21  lr: 0.000010  loss: 0.2722  time: 1.0091  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3400/7110]  eta: 1:02:29  lr: 0.000010  loss: 0.3669  time: 0.9872  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3450/7110]  eta: 1:01:37  lr: 0.000010  loss: 0.1580  time: 0.9538  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3500/7110]  eta: 1:00:45  lr: 0.000010  loss: 0.3591  time: 0.9862  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3550/7110]  eta: 0:59:56  lr: 0.000010  loss: 0.1643  time: 1.0701  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3600/7110]  eta: 0:59:06  lr: 0.000010  loss: 0.2581  time: 1.0114  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3650/7110]  eta: 0:58:14  lr: 0.000010  loss: 0.6676  time: 0.9866  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3700/7110]  eta: 0:57:22  lr: 0.000010  loss: 0.7057  time: 0.9828  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3750/7110]  eta: 0:56:31  lr: 0.000010  loss: 0.2121  time: 1.0283  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3800/7110]  eta: 0:55:41  lr: 0.000010  loss: 1.0491  time: 0.9625  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3850/7110]  eta: 0:54:49  lr: 0.000010  loss: 0.3845  time: 0.9313  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3900/7110]  eta: 0:53:57  lr: 0.000010  loss: 0.2315  time: 0.9994  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [3950/7110]  eta: 0:53:06  lr: 0.000010  loss: 0.2051  time: 0.9850  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4000/7110]  eta: 0:52:16  lr: 0.000010  loss: 0.7154  time: 0.9642  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4050/7110]  eta: 0:51:25  lr: 0.000010  loss: 0.6045  time: 0.9713  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4100/7110]  eta: 0:50:33  lr: 0.000010  loss: 0.2305  time: 0.9051  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4150/7110]  eta: 0:49:43  lr: 0.000010  loss: 0.5672  time: 1.0008  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4200/7110]  eta: 0:48:51  lr: 0.000010  loss: 0.2207  time: 0.9856  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4250/7110]  eta: 0:48:00  lr: 0.000010  loss: 0.5962  time: 0.9784  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4300/7110]  eta: 0:47:09  lr: 0.000010  loss: 0.3920  time: 0.9753  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4350/7110]  eta: 0:46:19  lr: 0.000010  loss: 0.1635  time: 1.0363  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4400/7110]  eta: 0:45:29  lr: 0.000010  loss: 0.5832  time: 0.9966  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4450/7110]  eta: 0:44:38  lr: 0.000010  loss: 0.1057  time: 1.0013  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4500/7110]  eta: 0:43:49  lr: 0.000010  loss: 0.1725  time: 1.0146  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4550/7110]  eta: 0:42:58  lr: 0.000010  loss: 0.6851  time: 0.9834  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4600/7110]  eta: 0:42:08  lr: 0.000010  loss: 0.1989  time: 1.0194  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4650/7110]  eta: 0:41:18  lr: 0.000010  loss: 0.4824  time: 1.0372  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4700/7110]  eta: 0:40:26  lr: 0.000010  loss: 0.1218  time: 0.9284  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4750/7110]  eta: 0:39:35  lr: 0.000010  loss: 0.4243  time: 0.9739  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4800/7110]  eta: 0:38:44  lr: 0.000010  loss: 0.4313  time: 1.0063  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4850/7110]  eta: 0:37:54  lr: 0.000010  loss: 0.3706  time: 1.0337  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4900/7110]  eta: 0:37:04  lr: 0.000010  loss: 0.5535  time: 1.0518  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [4950/7110]  eta: 0:36:13  lr: 0.000010  loss: 0.5607  time: 0.9716  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5000/7110]  eta: 0:35:22  lr: 0.000010  loss: 0.2515  time: 1.0376  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5050/7110]  eta: 0:34:31  lr: 0.000010  loss: 0.6164  time: 0.9848  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5100/7110]  eta: 0:33:40  lr: 0.000010  loss: 0.1905  time: 0.9567  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5150/7110]  eta: 0:32:50  lr: 0.000010  loss: 0.2640  time: 0.9993  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5200/7110]  eta: 0:32:00  lr: 0.000010  loss: 0.2906  time: 1.0359  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5250/7110]  eta: 0:31:10  lr: 0.000010  loss: 0.2250  time: 1.0415  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5300/7110]  eta: 0:30:20  lr: 0.000010  loss: 0.9386  time: 1.0680  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5350/7110]  eta: 0:29:29  lr: 0.000010  loss: 0.2340  time: 1.0425  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5400/7110]  eta: 0:28:38  lr: 0.000010  loss: 0.5376  time: 0.9595  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5450/7110]  eta: 0:27:48  lr: 0.000010  loss: 0.8276  time: 1.0658  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5500/7110]  eta: 0:26:58  lr: 0.000010  loss: 0.3632  time: 0.9662  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5550/7110]  eta: 0:26:07  lr: 0.000010  loss: 0.7901  time: 0.9836  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5600/7110]  eta: 0:25:17  lr: 0.000010  loss: 0.0758  time: 1.0193  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5650/7110]  eta: 0:24:27  lr: 0.000010  loss: 0.2856  time: 1.0360  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5700/7110]  eta: 0:23:36  lr: 0.000010  loss: 0.7389  time: 0.9450  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5750/7110]  eta: 0:22:46  lr: 0.000010  loss: 0.2936  time: 1.0151  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5800/7110]  eta: 0:21:55  lr: 0.000010  loss: 0.0666  time: 1.0087  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5850/7110]  eta: 0:21:05  lr: 0.000010  loss: 0.3534  time: 0.9882  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5900/7110]  eta: 0:20:15  lr: 0.000010  loss: 0.4657  time: 1.0011  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [5950/7110]  eta: 0:19:24  lr: 0.000010  loss: 0.6104  time: 0.9548  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6000/7110]  eta: 0:18:34  lr: 0.000010  loss: 0.7499  time: 1.0257  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6050/7110]  eta: 0:17:44  lr: 0.000010  loss: 0.7035  time: 0.8963  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.3359  time: 1.0558  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.3401  time: 0.9930  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.3295  time: 0.9746  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.2761  time: 1.0644  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6300/7110]  eta: 0:13:33  lr: 0.000010  loss: 1.0732  time: 0.9477  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.0812  time: 0.9646  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.1287  time: 1.0068  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.1823  time: 0.9999  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.3011  time: 1.0006  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.2906  time: 1.0457  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.7357  time: 0.9843  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.3881  time: 0.9998  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.5665  time: 0.9940  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.5968  time: 0.9885  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.1821  time: 0.9664  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.2297  time: 0.9660  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.6086  time: 0.9663  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.2599  time: 1.0438  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.3870  time: 0.9672  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.4049  time: 1.0414  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.5271  time: 0.9927  data: 0.0000  max mem: 65949
Train: data epoch: [1]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.3705  time: 1.0494  data: 0.0000  max mem: 65949
Train: data epoch: [1] Total time: 1:58:48 (1.0027 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:17:02    time: 20.6973  data: 19.2545  max mem: 65949
Evaluation  [  10/1093]  eta: 0:58:26    time: 3.2378  data: 1.7512  max mem: 65949
Evaluation  [  20/1093]  eta: 0:41:03    time: 1.3758  data: 0.0009  max mem: 65949
Evaluation  [  30/1093]  eta: 0:33:49    time: 1.1787  data: 0.0009  max mem: 65949
Evaluation  [  40/1093]  eta: 0:31:02    time: 1.2147  data: 0.0009  max mem: 65949
Evaluation  [  50/1093]  eta: 0:29:16    time: 1.3346  data: 0.0009  max mem: 65949
Evaluation  [  60/1093]  eta: 0:27:25    time: 1.2329  data: 0.0009  max mem: 65949
Evaluation  [  70/1093]  eta: 0:26:50    time: 1.2935  data: 0.0009  max mem: 65949
Evaluation  [  80/1093]  eta: 0:26:22    time: 1.4698  data: 0.0009  max mem: 65949
Evaluation  [  90/1093]  eta: 0:25:51    time: 1.4508  data: 0.0009  max mem: 65949
Evaluation  [ 100/1093]  eta: 0:25:31    time: 1.4618  data: 0.0010  max mem: 65949
Evaluation  [ 110/1093]  eta: 0:25:12    time: 1.5009  data: 0.0010  max mem: 65949
Evaluation  [ 120/1093]  eta: 0:24:33    time: 1.3729  data: 0.0009  max mem: 65949
Evaluation  [ 130/1093]  eta: 0:24:19    time: 1.3895  data: 0.0009  max mem: 65949
Evaluation  [ 140/1093]  eta: 0:23:45    time: 1.3810  data: 0.0010  max mem: 65949
Evaluation  [ 150/1093]  eta: 0:23:24    time: 1.3180  data: 0.0009  max mem: 65949
Evaluation  [ 160/1093]  eta: 0:23:08    time: 1.4321  data: 0.0010  max mem: 65949
Evaluation  [ 170/1093]  eta: 0:22:36    time: 1.3181  data: 0.0010  max mem: 65949
Evaluation  [ 180/1093]  eta: 0:22:15    time: 1.2652  data: 0.0009  max mem: 65949
Evaluation  [ 190/1093]  eta: 0:21:59    time: 1.3931  data: 0.0009  max mem: 65949
Evaluation  [ 200/1093]  eta: 0:21:35    time: 1.3392  data: 0.0009  max mem: 65949
Evaluation  [ 210/1093]  eta: 0:21:29    time: 1.4444  data: 0.0010  max mem: 65949
Evaluation  [ 220/1093]  eta: 0:21:05    time: 1.4384  data: 0.0010  max mem: 65949
Evaluation  [ 230/1093]  eta: 0:20:43    time: 1.2395  data: 0.0011  max mem: 65949
Evaluation  [ 240/1093]  eta: 0:20:26    time: 1.3060  data: 0.0011  max mem: 65949
Evaluation  [ 250/1093]  eta: 0:20:09    time: 1.3590  data: 0.0010  max mem: 65949
Evaluation  [ 260/1093]  eta: 0:20:03    time: 1.5299  data: 0.0010  max mem: 65949
Evaluation  [ 270/1093]  eta: 0:19:41    time: 1.4471  data: 0.0010  max mem: 65949
Evaluation  [ 280/1093]  eta: 0:19:23    time: 1.2556  data: 0.0010  max mem: 65949
Evaluation  [ 290/1093]  eta: 0:19:13    time: 1.4608  data: 0.0010  max mem: 65949
Evaluation  [ 300/1093]  eta: 0:18:51    time: 1.3638  data: 0.0009  max mem: 65949
Evaluation  [ 310/1093]  eta: 0:18:31    time: 1.1676  data: 0.0009  max mem: 65949
Evaluation  [ 320/1093]  eta: 0:18:17    time: 1.3136  data: 0.0009  max mem: 65949
Evaluation  [ 330/1093]  eta: 0:18:00    time: 1.3663  data: 0.0009  max mem: 65949
Evaluation  [ 340/1093]  eta: 0:17:47    time: 1.3894  data: 0.0010  max mem: 65949
Evaluation  [ 350/1093]  eta: 0:17:35    time: 1.4794  data: 0.0010  max mem: 65949
Evaluation  [ 360/1093]  eta: 0:17:16    time: 1.3377  data: 0.0009  max mem: 65949
Evaluation  [ 370/1093]  eta: 0:17:01    time: 1.2756  data: 0.0009  max mem: 65949
Evaluation  [ 380/1093]  eta: 0:16:50    time: 1.4709  data: 0.0010  max mem: 65949
Evaluation  [ 390/1093]  eta: 0:16:36    time: 1.5058  data: 0.0010  max mem: 65949
Evaluation  [ 400/1093]  eta: 0:16:23    time: 1.4674  data: 0.0011  max mem: 65949
Evaluation  [ 410/1093]  eta: 0:16:08    time: 1.4475  data: 0.0010  max mem: 65949
Evaluation  [ 420/1093]  eta: 0:15:52    time: 1.3517  data: 0.0010  max mem: 65949
Evaluation  [ 430/1093]  eta: 0:15:37    time: 1.3268  data: 0.0010  max mem: 65949
Evaluation  [ 440/1093]  eta: 0:15:18    time: 1.2228  data: 0.0010  max mem: 65949
Evaluation  [ 450/1093]  eta: 0:15:04    time: 1.2333  data: 0.0009  max mem: 65949
Evaluation  [ 460/1093]  eta: 0:14:47    time: 1.2992  data: 0.0009  max mem: 65949
Evaluation  [ 470/1093]  eta: 0:14:31    time: 1.2198  data: 0.0009  max mem: 65949
Evaluation  [ 480/1093]  eta: 0:14:17    time: 1.3144  data: 0.0009  max mem: 65949
Evaluation  [ 490/1093]  eta: 0:14:04    time: 1.4461  data: 0.0009  max mem: 65949
Evaluation  [ 500/1093]  eta: 0:13:53    time: 1.5500  data: 0.0010  max mem: 65949
Evaluation  [ 510/1093]  eta: 0:13:39    time: 1.5519  data: 0.0010  max mem: 65949
Evaluation  [ 520/1093]  eta: 0:13:26    time: 1.4640  data: 0.0010  max mem: 65949
Evaluation  [ 530/1093]  eta: 0:13:12    time: 1.4441  data: 0.0009  max mem: 65949
Evaluation  [ 540/1093]  eta: 0:12:58    time: 1.4203  data: 0.0009  max mem: 65949
Evaluation  [ 550/1093]  eta: 0:12:44    time: 1.4164  data: 0.0010  max mem: 65949
Evaluation  [ 560/1093]  eta: 0:12:26    time: 1.2284  data: 0.0010  max mem: 65949
Evaluation  [ 570/1093]  eta: 0:12:11    time: 1.1580  data: 0.0010  max mem: 65949
Evaluation  [ 580/1093]  eta: 0:11:57    time: 1.3338  data: 0.0010  max mem: 65949
Evaluation  [ 590/1093]  eta: 0:11:40    time: 1.2085  data: 0.0010  max mem: 65949
Evaluation  [ 600/1093]  eta: 0:11:27    time: 1.2962  data: 0.0011  max mem: 65949
Evaluation  [ 610/1093]  eta: 0:11:13    time: 1.4723  data: 0.0011  max mem: 65949
Evaluation  [ 620/1093]  eta: 0:11:00    time: 1.3937  data: 0.0009  max mem: 65949
Evaluation  [ 630/1093]  eta: 0:10:46    time: 1.3994  data: 0.0009  max mem: 65949
Evaluation  [ 640/1093]  eta: 0:10:32    time: 1.3881  data: 0.0009  max mem: 65949
Evaluation  [ 650/1093]  eta: 0:10:18    time: 1.4494  data: 0.0009  max mem: 65949
Evaluation  [ 660/1093]  eta: 0:10:03    time: 1.3783  data: 0.0009  max mem: 65949
Evaluation  [ 670/1093]  eta: 0:09:50    time: 1.3701  data: 0.0010  max mem: 65949
Evaluation  [ 680/1093]  eta: 0:09:37    time: 1.4817  data: 0.0010  max mem: 65949
Evaluation  [ 690/1093]  eta: 0:09:23    time: 1.4914  data: 0.0009  max mem: 65949
Evaluation  [ 700/1093]  eta: 0:09:09    time: 1.4334  data: 0.0010  max mem: 65949
Evaluation  [ 710/1093]  eta: 0:08:53    time: 1.2050  data: 0.0010  max mem: 65949
Evaluation  [ 720/1093]  eta: 0:08:37    time: 1.0515  data: 0.0009  max mem: 65949
Evaluation  [ 730/1093]  eta: 0:08:22    time: 1.0870  data: 0.0009  max mem: 65949
Evaluation  [ 740/1093]  eta: 0:08:07    time: 1.1532  data: 0.0009  max mem: 65949
Evaluation  [ 750/1093]  eta: 0:07:53    time: 1.2055  data: 0.0010  max mem: 65949
Evaluation  [ 760/1093]  eta: 0:07:39    time: 1.2900  data: 0.0010  max mem: 65949
Evaluation  [ 770/1093]  eta: 0:07:25    time: 1.3758  data: 0.0010  max mem: 65949
Evaluation  [ 780/1093]  eta: 0:07:12    time: 1.4614  data: 0.0010  max mem: 65949
Evaluation  [ 790/1093]  eta: 0:06:57    time: 1.2514  data: 0.0010  max mem: 65949
Evaluation  [ 800/1093]  eta: 0:06:44    time: 1.3137  data: 0.0010  max mem: 65949
Evaluation  [ 810/1093]  eta: 0:06:31    time: 1.5665  data: 0.0010  max mem: 65949
Evaluation  [ 820/1093]  eta: 0:06:17    time: 1.5013  data: 0.0009  max mem: 65949
Evaluation  [ 830/1093]  eta: 0:06:04    time: 1.5301  data: 0.0009  max mem: 65949
Evaluation  [ 840/1093]  eta: 0:05:50    time: 1.4210  data: 0.0010  max mem: 65949
Evaluation  [ 850/1093]  eta: 0:05:36    time: 1.3486  data: 0.0010  max mem: 65949
Evaluation  [ 860/1093]  eta: 0:05:22    time: 1.2966  data: 0.0010  max mem: 65949
Evaluation  [ 870/1093]  eta: 0:05:08    time: 1.3014  data: 0.0010  max mem: 65949
Evaluation  [ 880/1093]  eta: 0:04:54    time: 1.4694  data: 0.0010  max mem: 65949
Evaluation  [ 890/1093]  eta: 0:04:40    time: 1.3377  data: 0.0010  max mem: 65949
Evaluation  [ 900/1093]  eta: 0:04:26    time: 1.2973  data: 0.0009  max mem: 65949
Evaluation  [ 910/1093]  eta: 0:04:13    time: 1.4320  data: 0.0009  max mem: 65949
Evaluation  [ 920/1093]  eta: 0:03:59    time: 1.4158  data: 0.0010  max mem: 65949
Evaluation  [ 930/1093]  eta: 0:03:45    time: 1.3468  data: 0.0009  max mem: 65949
Evaluation  [ 940/1093]  eta: 0:03:31    time: 1.3136  data: 0.0009  max mem: 65949
Evaluation  [ 950/1093]  eta: 0:03:17    time: 1.3538  data: 0.0009  max mem: 65949
Evaluation  [ 960/1093]  eta: 0:03:03    time: 1.4340  data: 0.0009  max mem: 65949
Evaluation  [ 970/1093]  eta: 0:02:50    time: 1.4451  data: 0.0009  max mem: 65949
Evaluation  [ 980/1093]  eta: 0:02:36    time: 1.3071  data: 0.0010  max mem: 65949
Evaluation  [ 990/1093]  eta: 0:02:22    time: 1.2809  data: 0.0010  max mem: 65949
Evaluation  [1000/1093]  eta: 0:02:08    time: 1.4123  data: 0.0010  max mem: 65949
Evaluation  [1010/1093]  eta: 0:01:54    time: 1.4891  data: 0.0010  max mem: 65949
Evaluation  [1020/1093]  eta: 0:01:40    time: 1.3824  data: 0.0010  max mem: 65949
Evaluation  [1030/1093]  eta: 0:01:26    time: 1.2182  data: 0.0009  max mem: 65949
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2583  data: 0.0009  max mem: 65949
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3123  data: 0.0010  max mem: 65949
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3663  data: 0.0010  max mem: 65949
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4297  data: 0.0010  max mem: 65949
Evaluation  [1080/1093]  eta: 0:00:17    time: 1.3471  data: 0.0010  max mem: 65949
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.2727  data: 0.0010  max mem: 65949
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2552  data: 0.0407  max mem: 65949
Evaluation Total time: 0:25:05 (1.3776 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_1_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [2]  [   0/7110]  eta: 2 days, 6:10:49  lr: 0.000010  loss: 0.3512  time: 27.4331  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [  50/7110]  eta: 2:58:47  lr: 0.000010  loss: 0.1884  time: 0.9857  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 100/7110]  eta: 2:28:12  lr: 0.000010  loss: 0.1077  time: 1.0511  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 150/7110]  eta: 2:17:03  lr: 0.000010  loss: 0.4832  time: 0.9499  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 200/7110]  eta: 2:11:58  lr: 0.000010  loss: 0.1913  time: 1.0682  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 250/7110]  eta: 2:07:43  lr: 0.000010  loss: 0.0397  time: 0.9709  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 300/7110]  eta: 2:04:02  lr: 0.000010  loss: 1.7040  time: 0.9973  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 350/7110]  eta: 2:00:56  lr: 0.000010  loss: 0.5149  time: 0.9583  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 400/7110]  eta: 1:59:35  lr: 0.000010  loss: 0.2276  time: 1.0236  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 450/7110]  eta: 1:57:54  lr: 0.000010  loss: 0.1727  time: 1.0300  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 500/7110]  eta: 1:56:28  lr: 0.000010  loss: 0.3193  time: 1.0509  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 550/7110]  eta: 1:54:54  lr: 0.000010  loss: 0.2503  time: 1.0106  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 600/7110]  eta: 1:53:31  lr: 0.000010  loss: 0.4222  time: 1.0042  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 650/7110]  eta: 1:52:56  lr: 0.000010  loss: 1.7939  time: 1.0892  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 700/7110]  eta: 1:51:32  lr: 0.000010  loss: 0.6913  time: 0.9753  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 750/7110]  eta: 1:50:10  lr: 0.000010  loss: 0.3009  time: 0.9898  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 800/7110]  eta: 1:49:06  lr: 0.000010  loss: 1.1818  time: 1.0142  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 850/7110]  eta: 1:47:56  lr: 0.000010  loss: 0.1810  time: 0.9939  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 900/7110]  eta: 1:47:01  lr: 0.000010  loss: 0.8964  time: 1.0055  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [ 950/7110]  eta: 1:46:01  lr: 0.000010  loss: 0.5045  time: 1.0188  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1000/7110]  eta: 1:44:51  lr: 0.000010  loss: 0.2730  time: 1.0140  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1050/7110]  eta: 1:43:53  lr: 0.000010  loss: 0.2282  time: 1.0328  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1100/7110]  eta: 1:42:47  lr: 0.000010  loss: 0.7263  time: 0.9284  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1150/7110]  eta: 1:41:47  lr: 0.000010  loss: 0.2185  time: 0.9899  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1200/7110]  eta: 1:40:41  lr: 0.000010  loss: 0.2191  time: 0.9876  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1250/7110]  eta: 1:39:51  lr: 0.000010  loss: 0.1604  time: 1.0840  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1300/7110]  eta: 1:38:48  lr: 0.000010  loss: 0.3145  time: 1.0434  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1350/7110]  eta: 1:37:49  lr: 0.000010  loss: 0.4521  time: 1.0026  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1400/7110]  eta: 1:37:04  lr: 0.000010  loss: 0.3433  time: 1.0352  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1450/7110]  eta: 1:35:58  lr: 0.000010  loss: 0.2308  time: 0.9473  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1500/7110]  eta: 1:34:58  lr: 0.000010  loss: 0.9823  time: 0.9672  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1550/7110]  eta: 1:34:01  lr: 0.000010  loss: 0.6010  time: 0.9452  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1600/7110]  eta: 1:33:08  lr: 0.000010  loss: 0.2162  time: 1.0165  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1650/7110]  eta: 1:32:17  lr: 0.000010  loss: 0.2236  time: 1.0192  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1700/7110]  eta: 1:31:20  lr: 0.000010  loss: 0.4633  time: 0.9489  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1750/7110]  eta: 1:30:28  lr: 0.000010  loss: 0.2312  time: 1.0171  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1800/7110]  eta: 1:29:34  lr: 0.000010  loss: 0.9073  time: 1.0274  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1850/7110]  eta: 1:28:43  lr: 0.000010  loss: 0.3277  time: 0.9964  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1900/7110]  eta: 1:27:55  lr: 0.000010  loss: 0.5081  time: 1.0851  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [1950/7110]  eta: 1:27:02  lr: 0.000010  loss: 0.1659  time: 0.9862  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2000/7110]  eta: 1:26:09  lr: 0.000010  loss: 0.3003  time: 1.0181  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2050/7110]  eta: 1:25:16  lr: 0.000010  loss: 0.5692  time: 1.0417  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2100/7110]  eta: 1:24:20  lr: 0.000010  loss: 0.2273  time: 1.0118  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2150/7110]  eta: 1:23:26  lr: 0.000010  loss: 0.4261  time: 0.9373  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2200/7110]  eta: 1:22:32  lr: 0.000010  loss: 0.1889  time: 1.0078  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2250/7110]  eta: 1:21:35  lr: 0.000010  loss: 0.2680  time: 0.9131  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2300/7110]  eta: 1:20:45  lr: 0.000010  loss: 0.5181  time: 0.9747  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2350/7110]  eta: 1:19:54  lr: 0.000010  loss: 0.7888  time: 0.9697  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2400/7110]  eta: 1:19:02  lr: 0.000010  loss: 0.0467  time: 0.9682  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2450/7110]  eta: 1:18:14  lr: 0.000010  loss: 0.2454  time: 1.0360  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2500/7110]  eta: 1:17:24  lr: 0.000010  loss: 0.1642  time: 0.9828  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2550/7110]  eta: 1:16:32  lr: 0.000010  loss: 0.2044  time: 1.0116  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2600/7110]  eta: 1:15:40  lr: 0.000010  loss: 0.3945  time: 1.0042  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2650/7110]  eta: 1:14:49  lr: 0.000010  loss: 0.1712  time: 1.0318  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2700/7110]  eta: 1:13:57  lr: 0.000010  loss: 0.2647  time: 0.9988  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2750/7110]  eta: 1:13:03  lr: 0.000010  loss: 0.3730  time: 0.9825  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2800/7110]  eta: 1:12:15  lr: 0.000010  loss: 0.6729  time: 1.0500  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2850/7110]  eta: 1:11:21  lr: 0.000010  loss: 0.4123  time: 0.9887  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2900/7110]  eta: 1:10:31  lr: 0.000010  loss: 0.5665  time: 1.0827  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [2950/7110]  eta: 1:09:38  lr: 0.000010  loss: 0.1289  time: 0.9964  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3000/7110]  eta: 1:08:48  lr: 0.000010  loss: 0.6873  time: 1.0087  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3050/7110]  eta: 1:07:57  lr: 0.000010  loss: 0.3802  time: 1.0244  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3100/7110]  eta: 1:07:04  lr: 0.000010  loss: 0.5976  time: 0.9878  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3150/7110]  eta: 1:06:14  lr: 0.000010  loss: 0.5147  time: 0.9741  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3200/7110]  eta: 1:05:23  lr: 0.000010  loss: 0.6760  time: 0.9921  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3250/7110]  eta: 1:04:31  lr: 0.000010  loss: 0.7056  time: 0.9651  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3300/7110]  eta: 1:03:40  lr: 0.000010  loss: 0.2169  time: 0.9952  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3350/7110]  eta: 1:02:49  lr: 0.000010  loss: 0.0760  time: 0.9644  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3400/7110]  eta: 1:01:56  lr: 0.000010  loss: 0.4864  time: 0.9256  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3450/7110]  eta: 1:01:08  lr: 0.000010  loss: 0.3591  time: 1.0689  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3500/7110]  eta: 1:00:18  lr: 0.000010  loss: 0.6770  time: 1.0100  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3550/7110]  eta: 0:59:28  lr: 0.000010  loss: 0.8198  time: 1.0130  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3600/7110]  eta: 0:58:38  lr: 0.000010  loss: 0.5543  time: 0.9428  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3650/7110]  eta: 0:57:48  lr: 0.000010  loss: 0.5771  time: 1.0080  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3700/7110]  eta: 0:56:58  lr: 0.000010  loss: 0.5101  time: 0.9841  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3750/7110]  eta: 0:56:07  lr: 0.000010  loss: 0.6554  time: 0.9204  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3800/7110]  eta: 0:55:16  lr: 0.000010  loss: 0.1847  time: 0.9975  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3850/7110]  eta: 0:54:25  lr: 0.000010  loss: 0.2617  time: 0.9998  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3900/7110]  eta: 0:53:34  lr: 0.000010  loss: 0.4461  time: 0.9981  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [3950/7110]  eta: 0:52:46  lr: 0.000010  loss: 1.5503  time: 1.0518  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4000/7110]  eta: 0:51:56  lr: 0.000010  loss: 0.1588  time: 1.0174  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4050/7110]  eta: 0:51:06  lr: 0.000010  loss: 0.2269  time: 0.9843  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4100/7110]  eta: 0:50:16  lr: 0.000010  loss: 0.4724  time: 1.0159  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4150/7110]  eta: 0:49:25  lr: 0.000010  loss: 0.1431  time: 1.0082  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4200/7110]  eta: 0:48:35  lr: 0.000010  loss: 0.3827  time: 1.0813  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4250/7110]  eta: 0:47:46  lr: 0.000010  loss: 0.5958  time: 1.0001  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4300/7110]  eta: 0:46:56  lr: 0.000010  loss: 1.1263  time: 0.9503  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4350/7110]  eta: 0:46:05  lr: 0.000010  loss: 0.4248  time: 0.9465  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4400/7110]  eta: 0:45:16  lr: 0.000010  loss: 0.1553  time: 1.0173  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4450/7110]  eta: 0:44:25  lr: 0.000010  loss: 0.6232  time: 0.9485  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4500/7110]  eta: 0:43:34  lr: 0.000010  loss: 0.5105  time: 0.9999  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4550/7110]  eta: 0:42:44  lr: 0.000010  loss: 0.0712  time: 0.9625  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4600/7110]  eta: 0:41:53  lr: 0.000010  loss: 0.3379  time: 1.0125  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4650/7110]  eta: 0:41:03  lr: 0.000010  loss: 0.2187  time: 1.0037  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4700/7110]  eta: 0:40:14  lr: 0.000010  loss: 0.1909  time: 1.0159  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4750/7110]  eta: 0:39:24  lr: 0.000010  loss: 0.2241  time: 1.0142  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4800/7110]  eta: 0:38:34  lr: 0.000010  loss: 0.0639  time: 0.9974  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4850/7110]  eta: 0:37:44  lr: 0.000010  loss: 0.3532  time: 0.9594  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4900/7110]  eta: 0:36:53  lr: 0.000010  loss: 0.4222  time: 1.0109  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [4950/7110]  eta: 0:36:03  lr: 0.000010  loss: 0.0498  time: 0.9703  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5000/7110]  eta: 0:35:13  lr: 0.000010  loss: 0.5108  time: 0.9622  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5050/7110]  eta: 0:34:23  lr: 0.000010  loss: 0.3093  time: 1.0423  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5100/7110]  eta: 0:33:33  lr: 0.000010  loss: 0.3181  time: 0.9873  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5150/7110]  eta: 0:32:43  lr: 0.000010  loss: 0.5262  time: 0.9867  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5200/7110]  eta: 0:31:52  lr: 0.000010  loss: 0.3994  time: 0.9748  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5250/7110]  eta: 0:31:02  lr: 0.000010  loss: 0.1171  time: 0.9668  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5300/7110]  eta: 0:30:12  lr: 0.000010  loss: 0.2912  time: 1.0007  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5350/7110]  eta: 0:29:22  lr: 0.000010  loss: 0.3298  time: 1.0535  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5400/7110]  eta: 0:28:31  lr: 0.000010  loss: 0.5023  time: 0.9721  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5450/7110]  eta: 0:27:41  lr: 0.000010  loss: 0.1507  time: 0.9882  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5500/7110]  eta: 0:26:51  lr: 0.000010  loss: 0.2014  time: 0.9441  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5550/7110]  eta: 0:26:01  lr: 0.000010  loss: 0.6599  time: 1.0110  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5600/7110]  eta: 0:25:11  lr: 0.000010  loss: 0.2089  time: 0.9762  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5650/7110]  eta: 0:24:22  lr: 0.000010  loss: 1.7735  time: 0.9995  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5700/7110]  eta: 0:23:32  lr: 0.000010  loss: 0.5394  time: 1.0099  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5750/7110]  eta: 0:22:41  lr: 0.000010  loss: 0.5935  time: 0.9570  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5800/7110]  eta: 0:21:51  lr: 0.000010  loss: 0.2827  time: 1.0271  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5850/7110]  eta: 0:21:01  lr: 0.000010  loss: 0.2715  time: 1.0279  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5900/7110]  eta: 0:20:12  lr: 0.000010  loss: 0.1274  time: 1.0574  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [5950/7110]  eta: 0:19:21  lr: 0.000010  loss: 0.5055  time: 1.0158  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6000/7110]  eta: 0:18:31  lr: 0.000010  loss: 0.4708  time: 1.0321  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6050/7110]  eta: 0:17:41  lr: 0.000010  loss: 0.4021  time: 0.9987  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6100/7110]  eta: 0:16:51  lr: 0.000010  loss: 0.3277  time: 1.0400  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6150/7110]  eta: 0:16:01  lr: 0.000010  loss: 0.2053  time: 0.9631  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6200/7110]  eta: 0:15:11  lr: 0.000010  loss: 0.1394  time: 0.9657  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6250/7110]  eta: 0:14:20  lr: 0.000010  loss: 0.7902  time: 0.9553  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6300/7110]  eta: 0:13:30  lr: 0.000010  loss: 0.1132  time: 0.9639  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6350/7110]  eta: 0:12:40  lr: 0.000010  loss: 0.4736  time: 1.0067  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6400/7110]  eta: 0:11:50  lr: 0.000010  loss: 0.2781  time: 0.9711  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6450/7110]  eta: 0:11:00  lr: 0.000010  loss: 0.6252  time: 1.0165  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6500/7110]  eta: 0:10:10  lr: 0.000010  loss: 0.2068  time: 1.0744  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6550/7110]  eta: 0:09:20  lr: 0.000010  loss: 2.0386  time: 1.0207  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.2437  time: 0.9812  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.1917  time: 0.9578  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.3816  time: 1.0092  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.1627  time: 1.0328  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.4692  time: 0.9661  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.3054  time: 0.9990  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.5560  time: 0.9873  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1163  time: 1.0128  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.1920  time: 1.0476  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.6872  time: 1.0295  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.4135  time: 0.9794  data: 0.0000  max mem: 65949
Train: data epoch: [2]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.7185  time: 1.1320  data: 0.0000  max mem: 65949
Train: data epoch: [2] Total time: 1:58:46 (1.0023 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:29:27    time: 21.3790  data: 19.9502  max mem: 65949
Evaluation  [  10/1093]  eta: 0:58:07    time: 3.2204  data: 1.8145  max mem: 65949
Evaluation  [  20/1093]  eta: 0:41:34    time: 1.3722  data: 0.0009  max mem: 65949
Evaluation  [  30/1093]  eta: 0:34:41    time: 1.2640  data: 0.0009  max mem: 65949
Evaluation  [  40/1093]  eta: 0:31:59    time: 1.2960  data: 0.0009  max mem: 65949
Evaluation  [  50/1093]  eta: 0:30:18    time: 1.4114  data: 0.0010  max mem: 65949
Evaluation  [  60/1093]  eta: 0:28:36    time: 1.3307  data: 0.0010  max mem: 65949
Evaluation  [  70/1093]  eta: 0:27:51    time: 1.3551  data: 0.0009  max mem: 65949
Evaluation  [  80/1093]  eta: 0:27:15    time: 1.4705  data: 0.0010  max mem: 65949
Evaluation  [  90/1093]  eta: 0:26:39    time: 1.4529  data: 0.0010  max mem: 65949
Evaluation  [ 100/1093]  eta: 0:26:16    time: 1.4813  data: 0.0010  max mem: 65949
Evaluation  [ 110/1093]  eta: 0:25:57    time: 1.5408  data: 0.0009  max mem: 65949
Evaluation  [ 120/1093]  eta: 0:25:20    time: 1.4347  data: 0.0010  max mem: 65949
Evaluation  [ 130/1093]  eta: 0:25:08    time: 1.4624  data: 0.0010  max mem: 65949
Evaluation  [ 140/1093]  eta: 0:24:28    time: 1.4078  data: 0.0010  max mem: 65949
Evaluation  [ 150/1093]  eta: 0:24:02    time: 1.2908  data: 0.0010  max mem: 65949
Evaluation  [ 160/1093]  eta: 0:23:45    time: 1.4337  data: 0.0010  max mem: 65949
Evaluation  [ 170/1093]  eta: 0:23:17    time: 1.3957  data: 0.0010  max mem: 65949
Evaluation  [ 180/1093]  eta: 0:22:57    time: 1.3595  data: 0.0010  max mem: 65949
Evaluation  [ 190/1093]  eta: 0:22:40    time: 1.4450  data: 0.0010  max mem: 65949
Evaluation  [ 200/1093]  eta: 0:22:15    time: 1.3795  data: 0.0010  max mem: 65949
Evaluation  [ 210/1093]  eta: 0:22:09    time: 1.4997  data: 0.0010  max mem: 65949
Evaluation  [ 220/1093]  eta: 0:21:42    time: 1.4533  data: 0.0009  max mem: 65949
Evaluation  [ 230/1093]  eta: 0:21:20    time: 1.2527  data: 0.0009  max mem: 65949
Evaluation  [ 240/1093]  eta: 0:21:01    time: 1.3284  data: 0.0009  max mem: 65949
Evaluation  [ 250/1093]  eta: 0:20:45    time: 1.4037  data: 0.0009  max mem: 65949
Evaluation  [ 260/1093]  eta: 0:20:38    time: 1.5793  data: 0.0010  max mem: 65949
Evaluation  [ 270/1093]  eta: 0:20:15    time: 1.4612  data: 0.0010  max mem: 65949
Evaluation  [ 280/1093]  eta: 0:19:57    time: 1.2959  data: 0.0010  max mem: 65949
Evaluation  [ 290/1093]  eta: 0:19:48    time: 1.5289  data: 0.0009  max mem: 65949
Evaluation  [ 300/1093]  eta: 0:19:26    time: 1.4557  data: 0.0011  max mem: 65949
Evaluation  [ 310/1093]  eta: 0:19:08    time: 1.2724  data: 0.0011  max mem: 65949
Evaluation  [ 320/1093]  eta: 0:18:55    time: 1.4367  data: 0.0009  max mem: 65949
Evaluation  [ 330/1093]  eta: 0:18:38    time: 1.4608  data: 0.0010  max mem: 65949
Evaluation  [ 340/1093]  eta: 0:18:25    time: 1.4456  data: 0.0010  max mem: 65949
Evaluation  [ 350/1093]  eta: 0:18:09    time: 1.4754  data: 0.0010  max mem: 65949
Evaluation  [ 360/1093]  eta: 0:17:50    time: 1.3361  data: 0.0010  max mem: 65949
Evaluation  [ 370/1093]  eta: 0:17:36    time: 1.3639  data: 0.0010  max mem: 65949
Evaluation  [ 380/1093]  eta: 0:17:24    time: 1.5471  data: 0.0010  max mem: 65949
Evaluation  [ 390/1093]  eta: 0:17:10    time: 1.5398  data: 0.0010  max mem: 65949
Evaluation  [ 400/1093]  eta: 0:16:56    time: 1.5087  data: 0.0010  max mem: 65949
Evaluation  [ 410/1093]  eta: 0:16:42    time: 1.5037  data: 0.0010  max mem: 65949
Evaluation  [ 420/1093]  eta: 0:16:27    time: 1.4685  data: 0.0010  max mem: 65949
Evaluation  [ 430/1093]  eta: 0:16:11    time: 1.4366  data: 0.0010  max mem: 65949
Evaluation  [ 440/1093]  eta: 0:15:52    time: 1.2687  data: 0.0009  max mem: 65949
Evaluation  [ 450/1093]  eta: 0:15:36    time: 1.2704  data: 0.0009  max mem: 65949
Evaluation  [ 460/1093]  eta: 0:15:20    time: 1.3574  data: 0.0010  max mem: 65949
Evaluation  [ 470/1093]  eta: 0:15:04    time: 1.3261  data: 0.0010  max mem: 65949
Evaluation  [ 480/1093]  eta: 0:14:50    time: 1.4091  data: 0.0010  max mem: 65949
Evaluation  [ 490/1093]  eta: 0:14:36    time: 1.5111  data: 0.0009  max mem: 65949
Evaluation  [ 500/1093]  eta: 0:14:24    time: 1.5914  data: 0.0009  max mem: 65949
Evaluation  [ 510/1093]  eta: 0:14:11    time: 1.6089  data: 0.0009  max mem: 65949
Evaluation  [ 520/1093]  eta: 0:13:57    time: 1.5381  data: 0.0009  max mem: 65949
Evaluation  [ 530/1093]  eta: 0:13:42    time: 1.4742  data: 0.0009  max mem: 65949
Evaluation  [ 540/1093]  eta: 0:13:27    time: 1.4529  data: 0.0009  max mem: 65949
Evaluation  [ 550/1093]  eta: 0:13:12    time: 1.4485  data: 0.0009  max mem: 65949
Evaluation  [ 560/1093]  eta: 0:12:55    time: 1.2782  data: 0.0010  max mem: 65949
Evaluation  [ 570/1093]  eta: 0:12:39    time: 1.2567  data: 0.0010  max mem: 65949
Evaluation  [ 580/1093]  eta: 0:12:25    time: 1.4035  data: 0.0009  max mem: 65949
Evaluation  [ 590/1093]  eta: 0:12:07    time: 1.2828  data: 0.0010  max mem: 65949
Evaluation  [ 600/1093]  eta: 0:11:54    time: 1.3892  data: 0.0010  max mem: 65949
Evaluation  [ 610/1093]  eta: 0:11:40    time: 1.5417  data: 0.0009  max mem: 65949
Evaluation  [ 620/1093]  eta: 0:11:25    time: 1.4422  data: 0.0009  max mem: 65949
Evaluation  [ 630/1093]  eta: 0:11:11    time: 1.4329  data: 0.0010  max mem: 65949
Evaluation  [ 640/1093]  eta: 0:10:55    time: 1.3754  data: 0.0009  max mem: 65949
Evaluation  [ 650/1093]  eta: 0:10:42    time: 1.4732  data: 0.0009  max mem: 65949
Evaluation  [ 660/1093]  eta: 0:10:26    time: 1.4486  data: 0.0009  max mem: 65949
Evaluation  [ 670/1093]  eta: 0:10:12    time: 1.4013  data: 0.0009  max mem: 65949
Evaluation  [ 680/1093]  eta: 0:09:58    time: 1.4899  data: 0.0010  max mem: 65949
Evaluation  [ 690/1093]  eta: 0:09:44    time: 1.4903  data: 0.0010  max mem: 65949
Evaluation  [ 700/1093]  eta: 0:09:29    time: 1.4685  data: 0.0010  max mem: 65949
Evaluation  [ 710/1093]  eta: 0:09:13    time: 1.2911  data: 0.0010  max mem: 65949
Evaluation  [ 720/1093]  eta: 0:08:57    time: 1.1885  data: 0.0010  max mem: 65949
Evaluation  [ 730/1093]  eta: 0:08:42    time: 1.1704  data: 0.0009  max mem: 65949
Evaluation  [ 740/1093]  eta: 0:08:27    time: 1.2421  data: 0.0009  max mem: 65949
Evaluation  [ 750/1093]  eta: 0:08:12    time: 1.3405  data: 0.0009  max mem: 65949
Evaluation  [ 760/1093]  eta: 0:07:57    time: 1.3565  data: 0.0010  max mem: 65949
Evaluation  [ 770/1093]  eta: 0:07:43    time: 1.3993  data: 0.0010  max mem: 65949
Evaluation  [ 780/1093]  eta: 0:07:29    time: 1.4641  data: 0.0010  max mem: 65949
Evaluation  [ 790/1093]  eta: 0:07:13    time: 1.3194  data: 0.0010  max mem: 65949
Evaluation  [ 800/1093]  eta: 0:07:00    time: 1.4003  data: 0.0009  max mem: 65949
Evaluation  [ 810/1093]  eta: 0:06:46    time: 1.5924  data: 0.0009  max mem: 65949
Evaluation  [ 820/1093]  eta: 0:06:32    time: 1.5037  data: 0.0009  max mem: 65949
Evaluation  [ 830/1093]  eta: 0:06:18    time: 1.5431  data: 0.0010  max mem: 65949
Evaluation  [ 840/1093]  eta: 0:06:03    time: 1.4411  data: 0.0010  max mem: 65949
Evaluation  [ 850/1093]  eta: 0:05:49    time: 1.3612  data: 0.0010  max mem: 65949
Evaluation  [ 860/1093]  eta: 0:05:34    time: 1.3543  data: 0.0010  max mem: 65949
Evaluation  [ 870/1093]  eta: 0:05:19    time: 1.3671  data: 0.0010  max mem: 65949
Evaluation  [ 880/1093]  eta: 0:05:05    time: 1.5222  data: 0.0010  max mem: 65949
Evaluation  [ 890/1093]  eta: 0:04:50    time: 1.3501  data: 0.0009  max mem: 65949
Evaluation  [ 900/1093]  eta: 0:04:36    time: 1.2975  data: 0.0009  max mem: 65949
Evaluation  [ 910/1093]  eta: 0:04:22    time: 1.4607  data: 0.0009  max mem: 65949
Evaluation  [ 920/1093]  eta: 0:04:08    time: 1.4670  data: 0.0009  max mem: 65949
Evaluation  [ 930/1093]  eta: 0:03:53    time: 1.4202  data: 0.0009  max mem: 65949
Evaluation  [ 940/1093]  eta: 0:03:39    time: 1.3612  data: 0.0009  max mem: 65949
Evaluation  [ 950/1093]  eta: 0:03:24    time: 1.3872  data: 0.0010  max mem: 65949
Evaluation  [ 960/1093]  eta: 0:03:10    time: 1.4843  data: 0.0010  max mem: 65949
Evaluation  [ 970/1093]  eta: 0:02:56    time: 1.4798  data: 0.0009  max mem: 65949
Evaluation  [ 980/1093]  eta: 0:02:41    time: 1.3558  data: 0.0009  max mem: 65949
Evaluation  [ 990/1093]  eta: 0:02:27    time: 1.3760  data: 0.0010  max mem: 65949
Evaluation  [1000/1093]  eta: 0:02:13    time: 1.4619  data: 0.0009  max mem: 65949
Evaluation  [1010/1093]  eta: 0:01:58    time: 1.4776  data: 0.0010  max mem: 65949
Evaluation  [1020/1093]  eta: 0:01:44    time: 1.3763  data: 0.0010  max mem: 65949
Evaluation  [1030/1093]  eta: 0:01:30    time: 1.2206  data: 0.0009  max mem: 65949
Evaluation  [1040/1093]  eta: 0:01:15    time: 1.2930  data: 0.0009  max mem: 65949
Evaluation  [1050/1093]  eta: 0:01:01    time: 1.3729  data: 0.0010  max mem: 65949
Evaluation  [1060/1093]  eta: 0:00:47    time: 1.3882  data: 0.0010  max mem: 65949
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4515  data: 0.0009  max mem: 65949
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.4087  data: 0.0010  max mem: 65949
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3661  data: 0.0010  max mem: 65949
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3376  data: 0.0403  max mem: 65949
Evaluation Total time: 0:26:00 (1.4273 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_2_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [3]  [   0/7110]  eta: 2 days, 5:42:33  lr: 0.000010  loss: 0.3300  time: 27.1946  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [  50/7110]  eta: 3:02:13  lr: 0.000010  loss: 1.1682  time: 1.0289  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 100/7110]  eta: 2:30:20  lr: 0.000010  loss: 0.6526  time: 1.0186  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 150/7110]  eta: 2:19:42  lr: 0.000010  loss: 0.3184  time: 1.0206  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 200/7110]  eta: 2:13:10  lr: 0.000010  loss: 0.5862  time: 0.9886  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 250/7110]  eta: 2:08:24  lr: 0.000010  loss: 0.3222  time: 0.9789  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 300/7110]  eta: 2:04:57  lr: 0.000010  loss: 0.1643  time: 1.0080  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 350/7110]  eta: 2:02:15  lr: 0.000010  loss: 0.4516  time: 0.9915  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 400/7110]  eta: 1:59:50  lr: 0.000010  loss: 0.3158  time: 0.9789  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 450/7110]  eta: 1:58:14  lr: 0.000010  loss: 0.4399  time: 1.0227  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 500/7110]  eta: 1:56:29  lr: 0.000010  loss: 0.1417  time: 1.0009  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 550/7110]  eta: 1:55:06  lr: 0.000010  loss: 0.3393  time: 0.9991  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 600/7110]  eta: 1:54:01  lr: 0.000010  loss: 0.4336  time: 1.0770  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 650/7110]  eta: 1:52:18  lr: 0.000010  loss: 0.1818  time: 0.9258  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 700/7110]  eta: 1:50:50  lr: 0.000010  loss: 0.3367  time: 0.9242  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 750/7110]  eta: 1:49:53  lr: 0.000010  loss: 0.0983  time: 0.9587  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 800/7110]  eta: 1:49:02  lr: 0.000010  loss: 0.7803  time: 1.0822  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 850/7110]  eta: 1:48:08  lr: 0.000010  loss: 0.2204  time: 1.0438  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 900/7110]  eta: 1:47:01  lr: 0.000010  loss: 0.4458  time: 0.9867  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [ 950/7110]  eta: 1:45:49  lr: 0.000010  loss: 0.9975  time: 0.9738  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1000/7110]  eta: 1:44:51  lr: 0.000010  loss: 0.4160  time: 1.0032  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1050/7110]  eta: 1:43:49  lr: 0.000010  loss: 0.4705  time: 0.9917  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1100/7110]  eta: 1:42:42  lr: 0.000010  loss: 0.3499  time: 0.9483  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1150/7110]  eta: 1:41:37  lr: 0.000010  loss: 0.4994  time: 0.9537  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1200/7110]  eta: 1:40:40  lr: 0.000010  loss: 0.6410  time: 1.0001  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1250/7110]  eta: 1:39:45  lr: 0.000010  loss: 0.4696  time: 1.0207  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1300/7110]  eta: 1:38:53  lr: 0.000010  loss: 1.5588  time: 1.0464  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1350/7110]  eta: 1:37:48  lr: 0.000010  loss: 0.2724  time: 0.9204  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1400/7110]  eta: 1:36:45  lr: 0.000010  loss: 0.0947  time: 0.9220  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1450/7110]  eta: 1:35:48  lr: 0.000010  loss: 0.4941  time: 0.9668  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1500/7110]  eta: 1:34:56  lr: 0.000010  loss: 0.4871  time: 1.0106  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1550/7110]  eta: 1:33:56  lr: 0.000010  loss: 0.2325  time: 0.9667  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1600/7110]  eta: 1:32:59  lr: 0.000010  loss: 0.0304  time: 0.9931  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1650/7110]  eta: 1:32:05  lr: 0.000010  loss: 0.1226  time: 0.9847  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1700/7110]  eta: 1:31:13  lr: 0.000010  loss: 0.0336  time: 1.0307  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1750/7110]  eta: 1:30:22  lr: 0.000010  loss: 0.2015  time: 0.9972  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1800/7110]  eta: 1:29:23  lr: 0.000010  loss: 0.1064  time: 0.9222  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1850/7110]  eta: 1:28:37  lr: 0.000010  loss: 0.1349  time: 1.0395  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1900/7110]  eta: 1:27:41  lr: 0.000010  loss: 0.3069  time: 0.9531  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [1950/7110]  eta: 1:26:53  lr: 0.000010  loss: 0.1785  time: 1.0696  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2000/7110]  eta: 1:26:00  lr: 0.000010  loss: 0.5653  time: 0.9789  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2050/7110]  eta: 1:25:12  lr: 0.000010  loss: 0.1834  time: 1.0826  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2100/7110]  eta: 1:24:22  lr: 0.000010  loss: 0.2294  time: 0.9766  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2150/7110]  eta: 1:23:29  lr: 0.000010  loss: 0.3736  time: 1.0073  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2200/7110]  eta: 1:22:32  lr: 0.000010  loss: 0.5716  time: 0.9288  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2250/7110]  eta: 1:21:41  lr: 0.000010  loss: 0.3125  time: 1.0209  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2300/7110]  eta: 1:20:49  lr: 0.000010  loss: 0.2840  time: 0.9933  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2350/7110]  eta: 1:20:00  lr: 0.000010  loss: 0.6419  time: 1.0176  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2400/7110]  eta: 1:19:07  lr: 0.000010  loss: 0.5071  time: 0.9820  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2450/7110]  eta: 1:18:17  lr: 0.000010  loss: 0.6727  time: 1.0518  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2500/7110]  eta: 1:17:29  lr: 0.000010  loss: 0.5653  time: 0.9980  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2550/7110]  eta: 1:16:38  lr: 0.000010  loss: 1.7355  time: 1.0336  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2600/7110]  eta: 1:15:46  lr: 0.000010  loss: 0.2201  time: 1.0047  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2650/7110]  eta: 1:14:58  lr: 0.000010  loss: 0.1747  time: 1.0495  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2700/7110]  eta: 1:14:05  lr: 0.000010  loss: 0.4743  time: 0.9694  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2750/7110]  eta: 1:13:15  lr: 0.000010  loss: 0.1894  time: 0.9860  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2800/7110]  eta: 1:12:23  lr: 0.000010  loss: 0.0996  time: 1.0007  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2850/7110]  eta: 1:11:32  lr: 0.000010  loss: 0.2647  time: 0.9746  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2900/7110]  eta: 1:10:41  lr: 0.000010  loss: 0.2903  time: 0.9994  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [2950/7110]  eta: 1:09:48  lr: 0.000010  loss: 0.6494  time: 0.9410  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3000/7110]  eta: 1:08:58  lr: 0.000010  loss: 0.9649  time: 1.0240  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3050/7110]  eta: 1:08:07  lr: 0.000010  loss: 0.2428  time: 1.0222  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3100/7110]  eta: 1:07:15  lr: 0.000010  loss: 0.2374  time: 0.9307  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3150/7110]  eta: 1:06:23  lr: 0.000010  loss: 0.3243  time: 0.9878  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3200/7110]  eta: 1:05:32  lr: 0.000010  loss: 0.1786  time: 0.9924  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3250/7110]  eta: 1:04:45  lr: 0.000010  loss: 0.2609  time: 1.0382  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3300/7110]  eta: 1:03:52  lr: 0.000010  loss: 0.4117  time: 0.9936  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3350/7110]  eta: 1:03:01  lr: 0.000010  loss: 0.2861  time: 1.0122  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3400/7110]  eta: 1:02:12  lr: 0.000010  loss: 0.6546  time: 1.0581  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3450/7110]  eta: 1:01:23  lr: 0.000010  loss: 0.2529  time: 1.0490  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3500/7110]  eta: 1:00:32  lr: 0.000010  loss: 0.4950  time: 1.0322  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3550/7110]  eta: 0:59:41  lr: 0.000010  loss: 0.6849  time: 1.0089  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3600/7110]  eta: 0:58:49  lr: 0.000010  loss: 0.2095  time: 0.9766  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3650/7110]  eta: 0:57:56  lr: 0.000010  loss: 0.4658  time: 0.9367  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3700/7110]  eta: 0:57:05  lr: 0.000010  loss: 0.1925  time: 0.9936  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3750/7110]  eta: 0:56:15  lr: 0.000010  loss: 0.5202  time: 0.9909  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3800/7110]  eta: 0:55:24  lr: 0.000010  loss: 0.4919  time: 1.0212  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3850/7110]  eta: 0:54:32  lr: 0.000010  loss: 0.1550  time: 0.9950  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3900/7110]  eta: 0:53:43  lr: 0.000010  loss: 0.1990  time: 1.0558  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [3950/7110]  eta: 0:52:53  lr: 0.000010  loss: 0.2418  time: 1.0678  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4000/7110]  eta: 0:52:03  lr: 0.000010  loss: 0.4932  time: 1.0075  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4050/7110]  eta: 0:51:12  lr: 0.000010  loss: 0.4548  time: 0.9408  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4100/7110]  eta: 0:50:20  lr: 0.000010  loss: 0.2381  time: 0.9974  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4150/7110]  eta: 0:49:31  lr: 0.000010  loss: 0.5898  time: 1.0454  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4200/7110]  eta: 0:48:41  lr: 0.000010  loss: 1.0311  time: 1.0478  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4250/7110]  eta: 0:47:51  lr: 0.000010  loss: 0.3703  time: 1.0226  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4300/7110]  eta: 0:47:01  lr: 0.000010  loss: 1.0133  time: 1.0354  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4350/7110]  eta: 0:46:11  lr: 0.000010  loss: 0.6467  time: 1.0263  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4400/7110]  eta: 0:45:21  lr: 0.000010  loss: 0.5819  time: 1.0127  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4450/7110]  eta: 0:44:31  lr: 0.000010  loss: 0.0856  time: 1.0252  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4500/7110]  eta: 0:43:40  lr: 0.000010  loss: 0.5631  time: 1.0048  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4550/7110]  eta: 0:42:50  lr: 0.000010  loss: 0.4616  time: 1.0388  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4600/7110]  eta: 0:42:00  lr: 0.000010  loss: 0.1250  time: 0.9766  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4650/7110]  eta: 0:41:09  lr: 0.000010  loss: 0.2936  time: 0.9580  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4700/7110]  eta: 0:40:18  lr: 0.000010  loss: 0.4548  time: 0.9768  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4750/7110]  eta: 0:39:27  lr: 0.000010  loss: 0.4140  time: 0.9735  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4800/7110]  eta: 0:38:38  lr: 0.000010  loss: 0.9630  time: 1.0229  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4850/7110]  eta: 0:37:48  lr: 0.000010  loss: 0.4893  time: 1.0325  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4900/7110]  eta: 0:36:58  lr: 0.000010  loss: 0.4702  time: 1.0259  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [4950/7110]  eta: 0:36:08  lr: 0.000010  loss: 0.4437  time: 0.9840  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5000/7110]  eta: 0:35:18  lr: 0.000010  loss: 0.1036  time: 1.0282  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5050/7110]  eta: 0:34:28  lr: 0.000010  loss: 0.2312  time: 0.9739  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5100/7110]  eta: 0:33:39  lr: 0.000010  loss: 0.5299  time: 1.0128  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5150/7110]  eta: 0:32:48  lr: 0.000010  loss: 0.6925  time: 0.9673  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5200/7110]  eta: 0:31:58  lr: 0.000010  loss: 0.5523  time: 1.0585  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5250/7110]  eta: 0:31:08  lr: 0.000010  loss: 1.5353  time: 1.0278  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5300/7110]  eta: 0:30:18  lr: 0.000010  loss: 0.2212  time: 1.0032  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5350/7110]  eta: 0:29:28  lr: 0.000010  loss: 0.6883  time: 1.0145  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5400/7110]  eta: 0:28:37  lr: 0.000010  loss: 0.2206  time: 0.9576  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5450/7110]  eta: 0:27:46  lr: 0.000010  loss: 0.1735  time: 1.0208  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5500/7110]  eta: 0:26:56  lr: 0.000010  loss: 0.2245  time: 1.0882  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5550/7110]  eta: 0:26:06  lr: 0.000010  loss: 0.2455  time: 0.9837  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5600/7110]  eta: 0:25:16  lr: 0.000010  loss: 0.2167  time: 1.0316  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5650/7110]  eta: 0:24:25  lr: 0.000010  loss: 0.0789  time: 1.0447  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5700/7110]  eta: 0:23:35  lr: 0.000010  loss: 0.3848  time: 1.0068  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.1491  time: 0.9385  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.1135  time: 1.0645  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 1.3498  time: 1.0007  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.3680  time: 0.9930  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [5950/7110]  eta: 0:19:24  lr: 0.000010  loss: 0.0752  time: 0.9815  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6000/7110]  eta: 0:18:34  lr: 0.000010  loss: 0.4967  time: 1.0323  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.2089  time: 0.9466  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.1511  time: 0.9466  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.4524  time: 0.9884  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.3275  time: 0.9689  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.1900  time: 0.9788  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.2856  time: 0.9702  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 1.5914  time: 1.0393  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.0823  time: 1.0394  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.3888  time: 0.9405  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.5206  time: 0.9705  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.2365  time: 1.0021  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.1774  time: 0.9842  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.4459  time: 1.0100  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.2595  time: 0.9820  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.0133  time: 0.9947  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.7363  time: 1.0592  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.8215  time: 1.0177  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.5517  time: 1.0604  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.4407  time: 0.9986  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.4169  time: 1.0043  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1134  time: 1.0035  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 1.3203  time: 1.0221  data: 0.0000  max mem: 65949
Train: data epoch: [3]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.6512  time: 1.1379  data: 0.0000  max mem: 65949
Train: data epoch: [3] Total time: 1:58:56 (1.0037 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:04:54    time: 20.0319  data: 18.7853  max mem: 65949
Evaluation  [  10/1093]  eta: 0:55:52    time: 3.0954  data: 1.7086  max mem: 65949
Evaluation  [  20/1093]  eta: 0:40:07    time: 1.3541  data: 0.0009  max mem: 65949
Evaluation  [  30/1093]  eta: 0:33:57    time: 1.2684  data: 0.0010  max mem: 65949
Evaluation  [  40/1093]  eta: 0:31:30    time: 1.3254  data: 0.0010  max mem: 65949
Evaluation  [  50/1093]  eta: 0:29:50    time: 1.4057  data: 0.0009  max mem: 65949
Evaluation  [  60/1093]  eta: 0:28:08    time: 1.3045  data: 0.0010  max mem: 65949
Evaluation  [  70/1093]  eta: 0:27:24    time: 1.3286  data: 0.0010  max mem: 65949
Evaluation  [  80/1093]  eta: 0:26:36    time: 1.3985  data: 0.0010  max mem: 65949
Evaluation  [  90/1093]  eta: 0:25:59    time: 1.3705  data: 0.0010  max mem: 65949
Evaluation  [ 100/1093]  eta: 0:25:38    time: 1.4383  data: 0.0010  max mem: 65949
Evaluation  [ 110/1093]  eta: 0:25:18    time: 1.4955  data: 0.0010  max mem: 65949
Evaluation  [ 120/1093]  eta: 0:24:43    time: 1.4016  data: 0.0010  max mem: 65949
Evaluation  [ 130/1093]  eta: 0:24:30    time: 1.4278  data: 0.0010  max mem: 65949
Evaluation  [ 140/1093]  eta: 0:23:51    time: 1.3670  data: 0.0009  max mem: 65949
Evaluation  [ 150/1093]  eta: 0:23:26    time: 1.2603  data: 0.0009  max mem: 65949
Evaluation  [ 160/1093]  eta: 0:23:06    time: 1.3723  data: 0.0010  max mem: 65949
Evaluation  [ 170/1093]  eta: 0:22:40    time: 1.3467  data: 0.0010  max mem: 65949
Evaluation  [ 180/1093]  eta: 0:22:22    time: 1.3469  data: 0.0010  max mem: 65949
Evaluation  [ 190/1093]  eta: 0:22:07    time: 1.4360  data: 0.0009  max mem: 65949
Evaluation  [ 200/1093]  eta: 0:21:44    time: 1.3722  data: 0.0009  max mem: 65949
Evaluation  [ 210/1093]  eta: 0:21:40    time: 1.5021  data: 0.0010  max mem: 65949
Evaluation  [ 220/1093]  eta: 0:21:14    time: 1.4498  data: 0.0010  max mem: 65949
Evaluation  [ 230/1093]  eta: 0:20:53    time: 1.2322  data: 0.0009  max mem: 65949
Evaluation  [ 240/1093]  eta: 0:20:34    time: 1.3127  data: 0.0009  max mem: 65949
Evaluation  [ 250/1093]  eta: 0:20:21    time: 1.4014  data: 0.0010  max mem: 65949
Evaluation  [ 260/1093]  eta: 0:20:14    time: 1.5795  data: 0.0010  max mem: 65949
Evaluation  [ 270/1093]  eta: 0:19:52    time: 1.4591  data: 0.0010  max mem: 65949
Evaluation  [ 280/1093]  eta: 0:19:35    time: 1.2835  data: 0.0009  max mem: 65949
Evaluation  [ 290/1093]  eta: 0:19:27    time: 1.5111  data: 0.0009  max mem: 65949
Evaluation  [ 300/1093]  eta: 0:19:04    time: 1.4179  data: 0.0009  max mem: 65949
Evaluation  [ 310/1093]  eta: 0:18:47    time: 1.2382  data: 0.0010  max mem: 65949
Evaluation  [ 320/1093]  eta: 0:18:34    time: 1.4150  data: 0.0010  max mem: 65949
Evaluation  [ 330/1093]  eta: 0:18:17    time: 1.4153  data: 0.0010  max mem: 65949
Evaluation  [ 340/1093]  eta: 0:18:04    time: 1.4210  data: 0.0010  max mem: 65949
Evaluation  [ 350/1093]  eta: 0:17:49    time: 1.4624  data: 0.0009  max mem: 65949
Evaluation  [ 360/1093]  eta: 0:17:29    time: 1.2850  data: 0.0009  max mem: 65949
Evaluation  [ 370/1093]  eta: 0:17:14    time: 1.2895  data: 0.0009  max mem: 65949
Evaluation  [ 380/1093]  eta: 0:17:03    time: 1.5097  data: 0.0009  max mem: 65949
Evaluation  [ 390/1093]  eta: 0:16:49    time: 1.5334  data: 0.0009  max mem: 65949
Evaluation  [ 400/1093]  eta: 0:16:36    time: 1.4751  data: 0.0009  max mem: 65949
Evaluation  [ 410/1093]  eta: 0:16:22    time: 1.4796  data: 0.0010  max mem: 65949
Evaluation  [ 420/1093]  eta: 0:16:08    time: 1.4747  data: 0.0010  max mem: 65949
Evaluation  [ 430/1093]  eta: 0:15:53    time: 1.4160  data: 0.0010  max mem: 65949
Evaluation  [ 440/1093]  eta: 0:15:34    time: 1.2460  data: 0.0010  max mem: 65949
Evaluation  [ 450/1093]  eta: 0:15:19    time: 1.2699  data: 0.0010  max mem: 65949
Evaluation  [ 460/1093]  eta: 0:15:03    time: 1.3616  data: 0.0010  max mem: 65949
Evaluation  [ 470/1093]  eta: 0:14:48    time: 1.3331  data: 0.0010  max mem: 65949
Evaluation  [ 480/1093]  eta: 0:14:34    time: 1.4073  data: 0.0010  max mem: 65949
Evaluation  [ 490/1093]  eta: 0:14:21    time: 1.5092  data: 0.0009  max mem: 65949
Evaluation  [ 500/1093]  eta: 0:14:09    time: 1.5722  data: 0.0009  max mem: 65949
Evaluation  [ 510/1093]  eta: 0:13:56    time: 1.5522  data: 0.0010  max mem: 65949
Evaluation  [ 520/1093]  eta: 0:13:41    time: 1.4597  data: 0.0010  max mem: 65949
Evaluation  [ 530/1093]  eta: 0:13:27    time: 1.4521  data: 0.0009  max mem: 65949
Evaluation  [ 540/1093]  eta: 0:13:13    time: 1.4642  data: 0.0009  max mem: 65949
Evaluation  [ 550/1093]  eta: 0:12:59    time: 1.4390  data: 0.0009  max mem: 65949
Evaluation  [ 560/1093]  eta: 0:12:41    time: 1.2697  data: 0.0009  max mem: 65949
Evaluation  [ 570/1093]  eta: 0:12:26    time: 1.2352  data: 0.0009  max mem: 65949
Evaluation  [ 580/1093]  eta: 0:12:12    time: 1.3774  data: 0.0009  max mem: 65949
Evaluation  [ 590/1093]  eta: 0:11:55    time: 1.2750  data: 0.0010  max mem: 65949
Evaluation  [ 600/1093]  eta: 0:11:42    time: 1.3783  data: 0.0010  max mem: 65949
Evaluation  [ 610/1093]  eta: 0:11:28    time: 1.5130  data: 0.0009  max mem: 65949
Evaluation  [ 620/1093]  eta: 0:11:14    time: 1.4215  data: 0.0010  max mem: 65949
Evaluation  [ 630/1093]  eta: 0:11:00    time: 1.4342  data: 0.0010  max mem: 65949
Evaluation  [ 640/1093]  eta: 0:10:45    time: 1.3890  data: 0.0010  max mem: 65949
Evaluation  [ 650/1093]  eta: 0:10:31    time: 1.4345  data: 0.0010  max mem: 65949
Evaluation  [ 660/1093]  eta: 0:10:16    time: 1.3903  data: 0.0010  max mem: 65949
Evaluation  [ 670/1093]  eta: 0:10:02    time: 1.3214  data: 0.0010  max mem: 65949
Evaluation  [ 680/1093]  eta: 0:09:47    time: 1.4133  data: 0.0009  max mem: 65949
Evaluation  [ 690/1093]  eta: 0:09:34    time: 1.4761  data: 0.0009  max mem: 65949
Evaluation  [ 700/1093]  eta: 0:09:19    time: 1.4584  data: 0.0009  max mem: 65949
Evaluation  [ 710/1093]  eta: 0:09:04    time: 1.2696  data: 0.0009  max mem: 65949
Evaluation  [ 720/1093]  eta: 0:08:48    time: 1.1797  data: 0.0010  max mem: 65949
Evaluation  [ 730/1093]  eta: 0:08:32    time: 1.1485  data: 0.0010  max mem: 65949
Evaluation  [ 740/1093]  eta: 0:08:17    time: 1.1628  data: 0.0010  max mem: 65949
Evaluation  [ 750/1093]  eta: 0:08:03    time: 1.2779  data: 0.0010  max mem: 65949
Evaluation  [ 760/1093]  eta: 0:07:49    time: 1.3511  data: 0.0010  max mem: 65949
Evaluation  [ 770/1093]  eta: 0:07:35    time: 1.4010  data: 0.0009  max mem: 65949
Evaluation  [ 780/1093]  eta: 0:07:21    time: 1.4566  data: 0.0009  max mem: 65949
Evaluation  [ 790/1093]  eta: 0:07:06    time: 1.2957  data: 0.0010  max mem: 65949
Evaluation  [ 800/1093]  eta: 0:06:53    time: 1.3922  data: 0.0010  max mem: 65949
Evaluation  [ 810/1093]  eta: 0:06:39    time: 1.5859  data: 0.0010  max mem: 65949
Evaluation  [ 820/1093]  eta: 0:06:25    time: 1.4831  data: 0.0009  max mem: 65949
Evaluation  [ 830/1093]  eta: 0:06:11    time: 1.5415  data: 0.0010  max mem: 65949
Evaluation  [ 840/1093]  eta: 0:05:57    time: 1.4403  data: 0.0009  max mem: 65949
Evaluation  [ 850/1093]  eta: 0:05:43    time: 1.3387  data: 0.0009  max mem: 65949
Evaluation  [ 860/1093]  eta: 0:05:28    time: 1.2765  data: 0.0009  max mem: 65949
Evaluation  [ 870/1093]  eta: 0:05:14    time: 1.2835  data: 0.0009  max mem: 65949
Evaluation  [ 880/1093]  eta: 0:05:00    time: 1.4992  data: 0.0010  max mem: 65949
Evaluation  [ 890/1093]  eta: 0:04:45    time: 1.3464  data: 0.0010  max mem: 65949
Evaluation  [ 900/1093]  eta: 0:04:31    time: 1.2844  data: 0.0010  max mem: 65949
Evaluation  [ 910/1093]  eta: 0:04:17    time: 1.4439  data: 0.0010  max mem: 65949
Evaluation  [ 920/1093]  eta: 0:04:03    time: 1.4363  data: 0.0010  max mem: 65949
Evaluation  [ 930/1093]  eta: 0:03:49    time: 1.3852  data: 0.0010  max mem: 65949
Evaluation  [ 940/1093]  eta: 0:03:35    time: 1.3535  data: 0.0009  max mem: 65949
Evaluation  [ 950/1093]  eta: 0:03:21    time: 1.3848  data: 0.0009  max mem: 65949
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.4786  data: 0.0010  max mem: 65949
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4995  data: 0.0009  max mem: 65949
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3762  data: 0.0009  max mem: 65949
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3592  data: 0.0009  max mem: 65949
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4326  data: 0.0010  max mem: 65949
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4681  data: 0.0010  max mem: 65949
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3443  data: 0.0010  max mem: 65949
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1889  data: 0.0009  max mem: 65949
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2623  data: 0.0009  max mem: 65949
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3550  data: 0.0009  max mem: 65949
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3978  data: 0.0009  max mem: 65949
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4479  data: 0.0010  max mem: 65949
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.4014  data: 0.0009  max mem: 65949
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3474  data: 0.0009  max mem: 65949
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3227  data: 0.0432  max mem: 65949
Evaluation Total time: 0:25:34 (1.4043 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_3_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [4]  [   0/7110]  eta: 2 days, 6:46:58  lr: 0.000010  loss: 0.7539  time: 27.7382  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [  50/7110]  eta: 2:56:25  lr: 0.000010  loss: 0.3921  time: 0.9759  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 100/7110]  eta: 2:26:04  lr: 0.000010  loss: 0.7138  time: 0.9920  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 150/7110]  eta: 2:15:59  lr: 0.000010  loss: 0.6235  time: 1.0176  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 200/7110]  eta: 2:09:21  lr: 0.000010  loss: 0.1348  time: 0.9747  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 250/7110]  eta: 2:06:19  lr: 0.000010  loss: 0.6289  time: 1.0497  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 300/7110]  eta: 2:03:25  lr: 0.000010  loss: 0.5504  time: 1.0223  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 350/7110]  eta: 2:01:51  lr: 0.000010  loss: 0.1358  time: 1.0526  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 400/7110]  eta: 1:59:36  lr: 0.000010  loss: 0.0546  time: 0.9925  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 450/7110]  eta: 1:58:06  lr: 0.000010  loss: 0.0873  time: 1.0195  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 500/7110]  eta: 1:56:55  lr: 0.000010  loss: 0.6260  time: 1.0162  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 550/7110]  eta: 1:55:45  lr: 0.000010  loss: 0.2567  time: 1.0331  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 600/7110]  eta: 1:54:25  lr: 0.000010  loss: 0.3093  time: 1.0424  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 650/7110]  eta: 1:53:07  lr: 0.000010  loss: 0.3766  time: 0.9852  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 700/7110]  eta: 1:51:57  lr: 0.000010  loss: 0.4044  time: 1.0122  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 750/7110]  eta: 1:50:49  lr: 0.000010  loss: 0.3698  time: 1.0104  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 800/7110]  eta: 1:49:37  lr: 0.000010  loss: 0.3152  time: 0.9754  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 850/7110]  eta: 1:48:33  lr: 0.000010  loss: 0.0709  time: 1.0221  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 900/7110]  eta: 1:47:20  lr: 0.000010  loss: 0.1117  time: 0.9846  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [ 950/7110]  eta: 1:46:19  lr: 0.000010  loss: 0.0742  time: 1.0430  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1000/7110]  eta: 1:45:02  lr: 0.000010  loss: 0.0999  time: 1.0197  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1050/7110]  eta: 1:44:14  lr: 0.000010  loss: 0.6646  time: 1.0598  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1100/7110]  eta: 1:43:03  lr: 0.000010  loss: 0.1447  time: 0.9816  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1150/7110]  eta: 1:42:17  lr: 0.000010  loss: 0.5153  time: 1.1218  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1200/7110]  eta: 1:41:21  lr: 0.000010  loss: 0.9229  time: 0.9936  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1250/7110]  eta: 1:40:21  lr: 0.000010  loss: 0.3055  time: 0.9830  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1300/7110]  eta: 1:39:21  lr: 0.000010  loss: 0.1445  time: 0.9421  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1350/7110]  eta: 1:38:18  lr: 0.000010  loss: 0.6847  time: 0.9319  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1400/7110]  eta: 1:37:20  lr: 0.000010  loss: 0.1837  time: 1.0000  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1450/7110]  eta: 1:36:25  lr: 0.000010  loss: 0.7887  time: 0.9985  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1500/7110]  eta: 1:35:26  lr: 0.000010  loss: 0.2590  time: 1.0005  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1550/7110]  eta: 1:34:29  lr: 0.000010  loss: 0.7755  time: 1.0170  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1600/7110]  eta: 1:33:36  lr: 0.000010  loss: 0.1896  time: 0.9933  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1650/7110]  eta: 1:32:40  lr: 0.000010  loss: 0.4374  time: 0.9604  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1700/7110]  eta: 1:31:46  lr: 0.000010  loss: 0.0928  time: 0.9988  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1750/7110]  eta: 1:30:51  lr: 0.000010  loss: 0.7263  time: 1.0093  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1800/7110]  eta: 1:29:55  lr: 0.000010  loss: 0.1654  time: 1.0284  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1850/7110]  eta: 1:29:06  lr: 0.000010  loss: 0.2760  time: 1.0818  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1900/7110]  eta: 1:28:13  lr: 0.000010  loss: 0.6213  time: 1.0060  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [1950/7110]  eta: 1:27:20  lr: 0.000010  loss: 0.2158  time: 1.0109  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2000/7110]  eta: 1:26:33  lr: 0.000010  loss: 0.5214  time: 1.0335  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2050/7110]  eta: 1:25:42  lr: 0.000010  loss: 0.8158  time: 1.0475  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2100/7110]  eta: 1:24:43  lr: 0.000010  loss: 0.2286  time: 0.9680  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2150/7110]  eta: 1:23:43  lr: 0.000010  loss: 0.2206  time: 0.9310  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2200/7110]  eta: 1:22:53  lr: 0.000010  loss: 0.2890  time: 1.0130  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2250/7110]  eta: 1:22:00  lr: 0.000010  loss: 0.1143  time: 1.0172  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2300/7110]  eta: 1:21:10  lr: 0.000010  loss: 0.6238  time: 1.0637  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2350/7110]  eta: 1:20:22  lr: 0.000010  loss: 0.1740  time: 1.0328  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2400/7110]  eta: 1:19:34  lr: 0.000010  loss: 0.2201  time: 1.0662  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2450/7110]  eta: 1:18:42  lr: 0.000010  loss: 0.1524  time: 0.9798  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2500/7110]  eta: 1:17:49  lr: 0.000010  loss: 0.6191  time: 0.9992  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2550/7110]  eta: 1:16:56  lr: 0.000010  loss: 0.2427  time: 1.0090  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2600/7110]  eta: 1:16:07  lr: 0.000010  loss: 0.1222  time: 1.0534  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2650/7110]  eta: 1:15:19  lr: 0.000010  loss: 0.0857  time: 1.0810  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2700/7110]  eta: 1:14:28  lr: 0.000010  loss: 0.4360  time: 1.0163  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2750/7110]  eta: 1:13:37  lr: 0.000010  loss: 0.4459  time: 0.9974  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2800/7110]  eta: 1:12:42  lr: 0.000010  loss: 0.5936  time: 0.9353  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2850/7110]  eta: 1:11:50  lr: 0.000010  loss: 0.6329  time: 0.9521  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2900/7110]  eta: 1:10:58  lr: 0.000010  loss: 0.5287  time: 1.0090  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [2950/7110]  eta: 1:10:05  lr: 0.000010  loss: 0.0742  time: 1.0128  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3000/7110]  eta: 1:09:15  lr: 0.000010  loss: 0.0777  time: 1.0010  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3050/7110]  eta: 1:08:25  lr: 0.000010  loss: 0.2481  time: 0.9685  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3100/7110]  eta: 1:07:33  lr: 0.000010  loss: 0.7977  time: 0.9732  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3150/7110]  eta: 1:06:41  lr: 0.000010  loss: 0.4143  time: 0.9725  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3200/7110]  eta: 1:05:48  lr: 0.000010  loss: 0.4946  time: 1.0006  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3250/7110]  eta: 1:04:57  lr: 0.000010  loss: 0.1039  time: 0.9983  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3300/7110]  eta: 1:04:06  lr: 0.000010  loss: 0.3725  time: 0.9846  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3350/7110]  eta: 1:03:17  lr: 0.000010  loss: 0.1490  time: 1.0193  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3400/7110]  eta: 1:02:26  lr: 0.000010  loss: 0.3387  time: 0.9882  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3450/7110]  eta: 1:01:35  lr: 0.000010  loss: 0.2438  time: 0.9639  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3500/7110]  eta: 1:00:42  lr: 0.000010  loss: 0.2277  time: 0.9546  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3550/7110]  eta: 0:59:51  lr: 0.000010  loss: 0.1416  time: 1.0098  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3600/7110]  eta: 0:59:00  lr: 0.000010  loss: 0.5218  time: 0.9935  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3650/7110]  eta: 0:58:08  lr: 0.000010  loss: 0.1107  time: 0.9300  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3700/7110]  eta: 0:57:19  lr: 0.000010  loss: 0.3360  time: 1.0319  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3750/7110]  eta: 0:56:27  lr: 0.000010  loss: 0.2299  time: 1.0010  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3800/7110]  eta: 0:55:39  lr: 0.000010  loss: 0.1032  time: 1.0541  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3850/7110]  eta: 0:54:48  lr: 0.000010  loss: 0.1189  time: 0.9895  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3900/7110]  eta: 0:53:56  lr: 0.000010  loss: 0.5356  time: 0.9354  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [3950/7110]  eta: 0:53:04  lr: 0.000010  loss: 0.0758  time: 0.9609  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4000/7110]  eta: 0:52:13  lr: 0.000010  loss: 0.1975  time: 0.9690  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4050/7110]  eta: 0:51:23  lr: 0.000010  loss: 0.1452  time: 1.0010  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4100/7110]  eta: 0:50:30  lr: 0.000010  loss: 0.2213  time: 0.9588  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4150/7110]  eta: 0:49:40  lr: 0.000010  loss: 0.5829  time: 1.0250  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4200/7110]  eta: 0:48:50  lr: 0.000010  loss: 0.2213  time: 1.0641  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4250/7110]  eta: 0:48:00  lr: 0.000010  loss: 0.4362  time: 1.0310  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4300/7110]  eta: 0:47:09  lr: 0.000010  loss: 0.4638  time: 0.9634  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4350/7110]  eta: 0:46:17  lr: 0.000010  loss: 0.5533  time: 0.9626  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4400/7110]  eta: 0:45:27  lr: 0.000010  loss: 0.1948  time: 1.0778  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4450/7110]  eta: 0:44:38  lr: 0.000010  loss: 0.3953  time: 1.0410  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4500/7110]  eta: 0:43:47  lr: 0.000010  loss: 0.5560  time: 1.0014  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4550/7110]  eta: 0:42:57  lr: 0.000010  loss: 0.4425  time: 1.0595  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4600/7110]  eta: 0:42:06  lr: 0.000010  loss: 1.2018  time: 0.9702  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4650/7110]  eta: 0:41:15  lr: 0.000010  loss: 0.3100  time: 1.0156  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4700/7110]  eta: 0:40:25  lr: 0.000010  loss: 0.1912  time: 0.9885  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4750/7110]  eta: 0:39:34  lr: 0.000010  loss: 0.2366  time: 1.0254  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4800/7110]  eta: 0:38:44  lr: 0.000010  loss: 0.1068  time: 0.9614  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4850/7110]  eta: 0:37:53  lr: 0.000010  loss: 0.1758  time: 0.9749  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4900/7110]  eta: 0:37:04  lr: 0.000010  loss: 0.5112  time: 1.0777  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [4950/7110]  eta: 0:36:13  lr: 0.000010  loss: 0.0355  time: 0.9766  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5000/7110]  eta: 0:35:22  lr: 0.000010  loss: 0.4641  time: 1.0080  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5050/7110]  eta: 0:34:31  lr: 0.000010  loss: 0.1565  time: 1.0143  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5100/7110]  eta: 0:33:41  lr: 0.000010  loss: 0.5557  time: 0.9736  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5150/7110]  eta: 0:32:51  lr: 0.000010  loss: 0.6707  time: 0.9784  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5200/7110]  eta: 0:32:01  lr: 0.000010  loss: 0.1797  time: 1.0291  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5250/7110]  eta: 0:31:10  lr: 0.000010  loss: 0.1314  time: 0.9217  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5300/7110]  eta: 0:30:19  lr: 0.000010  loss: 0.2454  time: 0.9935  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5350/7110]  eta: 0:29:29  lr: 0.000010  loss: 0.3019  time: 0.9537  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5400/7110]  eta: 0:28:39  lr: 0.000010  loss: 0.3376  time: 0.9460  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5450/7110]  eta: 0:27:48  lr: 0.000010  loss: 0.1814  time: 1.0159  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5500/7110]  eta: 0:26:58  lr: 0.000010  loss: 0.5144  time: 0.9796  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5550/7110]  eta: 0:26:08  lr: 0.000010  loss: 0.1951  time: 1.0367  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5600/7110]  eta: 0:25:17  lr: 0.000010  loss: 0.3773  time: 1.0412  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5650/7110]  eta: 0:24:27  lr: 0.000010  loss: 0.1095  time: 1.0435  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5700/7110]  eta: 0:23:37  lr: 0.000010  loss: 0.1913  time: 1.0503  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5750/7110]  eta: 0:22:47  lr: 0.000010  loss: 0.3839  time: 1.0317  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5800/7110]  eta: 0:21:57  lr: 0.000010  loss: 0.1702  time: 1.1031  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5850/7110]  eta: 0:21:06  lr: 0.000010  loss: 0.0706  time: 0.9528  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5900/7110]  eta: 0:20:16  lr: 0.000010  loss: 0.2162  time: 0.8939  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [5950/7110]  eta: 0:19:25  lr: 0.000010  loss: 0.2853  time: 0.9598  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6000/7110]  eta: 0:18:35  lr: 0.000010  loss: 0.3892  time: 1.0190  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6050/7110]  eta: 0:17:45  lr: 0.000010  loss: 0.2506  time: 0.9347  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6100/7110]  eta: 0:16:54  lr: 0.000010  loss: 0.3178  time: 0.9310  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6150/7110]  eta: 0:16:04  lr: 0.000010  loss: 0.1114  time: 1.0680  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6200/7110]  eta: 0:15:14  lr: 0.000010  loss: 0.3304  time: 0.9867  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.2308  time: 1.0029  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6300/7110]  eta: 0:13:33  lr: 0.000010  loss: 0.9451  time: 1.0545  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6350/7110]  eta: 0:12:43  lr: 0.000010  loss: 0.9099  time: 0.9591  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6400/7110]  eta: 0:11:53  lr: 0.000010  loss: 0.2330  time: 1.0322  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6450/7110]  eta: 0:11:03  lr: 0.000010  loss: 0.2625  time: 1.0438  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6500/7110]  eta: 0:10:13  lr: 0.000010  loss: 0.6614  time: 0.9791  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.7940  time: 1.0149  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6600/7110]  eta: 0:08:32  lr: 0.000010  loss: 0.9321  time: 0.9934  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6650/7110]  eta: 0:07:42  lr: 0.000010  loss: 0.1708  time: 0.9788  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6700/7110]  eta: 0:06:52  lr: 0.000010  loss: 0.6369  time: 1.0150  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.3985  time: 1.0065  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.4891  time: 0.9964  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6850/7110]  eta: 0:04:21  lr: 0.000010  loss: 0.4435  time: 1.0543  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6900/7110]  eta: 0:03:31  lr: 0.000010  loss: 0.4642  time: 0.9338  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.5220  time: 0.9795  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.4042  time: 0.9591  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1543  time: 0.9833  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.6529  time: 0.9809  data: 0.0000  max mem: 65949
Train: data epoch: [4]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.3505  time: 1.1160  data: 0.0000  max mem: 65949
Train: data epoch: [4] Total time: 1:59:03 (1.0047 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:12:50    time: 20.4675  data: 19.1985  max mem: 65949
Evaluation  [  10/1093]  eta: 0:56:29    time: 3.1296  data: 1.7462  max mem: 65949
Evaluation  [  20/1093]  eta: 0:40:38    time: 1.3626  data: 0.0010  max mem: 65949
Evaluation  [  30/1093]  eta: 0:34:12    time: 1.2709  data: 0.0010  max mem: 65949
Evaluation  [  40/1093]  eta: 0:31:34    time: 1.3014  data: 0.0010  max mem: 65949
Evaluation  [  50/1093]  eta: 0:29:42    time: 1.3656  data: 0.0010  max mem: 65949
Evaluation  [  60/1093]  eta: 0:27:54    time: 1.2570  data: 0.0009  max mem: 65949
Evaluation  [  70/1093]  eta: 0:27:17    time: 1.3251  data: 0.0010  max mem: 65949
Evaluation  [  80/1093]  eta: 0:26:30    time: 1.4133  data: 0.0010  max mem: 65949
Evaluation  [  90/1093]  eta: 0:25:54    time: 1.3692  data: 0.0009  max mem: 65949
Evaluation  [ 100/1093]  eta: 0:25:29    time: 1.4210  data: 0.0009  max mem: 65949
Evaluation  [ 110/1093]  eta: 0:25:07    time: 1.4602  data: 0.0010  max mem: 65949
Evaluation  [ 120/1093]  eta: 0:24:33    time: 1.3865  data: 0.0010  max mem: 65949
Evaluation  [ 130/1093]  eta: 0:24:22    time: 1.4322  data: 0.0010  max mem: 65949
Evaluation  [ 140/1093]  eta: 0:23:45    time: 1.3775  data: 0.0010  max mem: 65949
Evaluation  [ 150/1093]  eta: 0:23:20    time: 1.2662  data: 0.0010  max mem: 65949
Evaluation  [ 160/1093]  eta: 0:23:00    time: 1.3722  data: 0.0009  max mem: 65949
Evaluation  [ 170/1093]  eta: 0:22:35    time: 1.3468  data: 0.0010  max mem: 65949
Evaluation  [ 180/1093]  eta: 0:22:18    time: 1.3567  data: 0.0010  max mem: 65949
Evaluation  [ 190/1093]  eta: 0:22:02    time: 1.4317  data: 0.0010  max mem: 65949
Evaluation  [ 200/1093]  eta: 0:21:39    time: 1.3580  data: 0.0010  max mem: 65949
Evaluation  [ 210/1093]  eta: 0:21:36    time: 1.5047  data: 0.0011  max mem: 65949
Evaluation  [ 220/1093]  eta: 0:21:10    time: 1.4471  data: 0.0010  max mem: 65949
Evaluation  [ 230/1093]  eta: 0:20:48    time: 1.2181  data: 0.0010  max mem: 65949
Evaluation  [ 240/1093]  eta: 0:20:30    time: 1.3131  data: 0.0010  max mem: 65949
Evaluation  [ 250/1093]  eta: 0:20:16    time: 1.3930  data: 0.0010  max mem: 65949
Evaluation  [ 260/1093]  eta: 0:20:09    time: 1.5623  data: 0.0010  max mem: 65949
Evaluation  [ 270/1093]  eta: 0:19:47    time: 1.4431  data: 0.0010  max mem: 65949
Evaluation  [ 280/1093]  eta: 0:19:29    time: 1.2636  data: 0.0010  max mem: 65949
Evaluation  [ 290/1093]  eta: 0:19:22    time: 1.5079  data: 0.0010  max mem: 65949
Evaluation  [ 300/1093]  eta: 0:18:59    time: 1.4096  data: 0.0010  max mem: 65949
Evaluation  [ 310/1093]  eta: 0:18:41    time: 1.2086  data: 0.0010  max mem: 65949
Evaluation  [ 320/1093]  eta: 0:18:28    time: 1.3905  data: 0.0009  max mem: 65949
Evaluation  [ 330/1093]  eta: 0:18:10    time: 1.3971  data: 0.0009  max mem: 65949
Evaluation  [ 340/1093]  eta: 0:17:58    time: 1.4033  data: 0.0009  max mem: 65949
Evaluation  [ 350/1093]  eta: 0:17:43    time: 1.4655  data: 0.0009  max mem: 65949
Evaluation  [ 360/1093]  eta: 0:17:24    time: 1.3132  data: 0.0009  max mem: 65949
Evaluation  [ 370/1093]  eta: 0:17:10    time: 1.3015  data: 0.0009  max mem: 65949
Evaluation  [ 380/1093]  eta: 0:16:58    time: 1.4903  data: 0.0009  max mem: 65949
Evaluation  [ 390/1093]  eta: 0:16:43    time: 1.4877  data: 0.0009  max mem: 65949
Evaluation  [ 400/1093]  eta: 0:16:30    time: 1.4393  data: 0.0009  max mem: 65949
Evaluation  [ 410/1093]  eta: 0:16:15    time: 1.4273  data: 0.0009  max mem: 65949
Evaluation  [ 420/1093]  eta: 0:16:02    time: 1.4286  data: 0.0009  max mem: 65949
Evaluation  [ 430/1093]  eta: 0:15:46    time: 1.4206  data: 0.0009  max mem: 65949
Evaluation  [ 440/1093]  eta: 0:15:28    time: 1.2499  data: 0.0009  max mem: 65949
Evaluation  [ 450/1093]  eta: 0:15:13    time: 1.2630  data: 0.0010  max mem: 65949
Evaluation  [ 460/1093]  eta: 0:14:57    time: 1.3469  data: 0.0010  max mem: 65949
Evaluation  [ 470/1093]  eta: 0:14:42    time: 1.3227  data: 0.0010  max mem: 65949
Evaluation  [ 480/1093]  eta: 0:14:28    time: 1.3872  data: 0.0010  max mem: 65949
Evaluation  [ 490/1093]  eta: 0:14:15    time: 1.4977  data: 0.0010  max mem: 65949
Evaluation  [ 500/1093]  eta: 0:14:03    time: 1.5691  data: 0.0010  max mem: 65949
Evaluation  [ 510/1093]  eta: 0:13:50    time: 1.5623  data: 0.0009  max mem: 65949
Evaluation  [ 520/1093]  eta: 0:13:36    time: 1.4707  data: 0.0010  max mem: 65949
Evaluation  [ 530/1093]  eta: 0:13:22    time: 1.4481  data: 0.0009  max mem: 65949
Evaluation  [ 540/1093]  eta: 0:13:08    time: 1.4642  data: 0.0009  max mem: 65949
Evaluation  [ 550/1093]  eta: 0:12:54    time: 1.4488  data: 0.0009  max mem: 65949
Evaluation  [ 560/1093]  eta: 0:12:36    time: 1.2578  data: 0.0009  max mem: 65949
Evaluation  [ 570/1093]  eta: 0:12:22    time: 1.2070  data: 0.0009  max mem: 65949
Evaluation  [ 580/1093]  eta: 0:12:07    time: 1.3642  data: 0.0010  max mem: 65949
Evaluation  [ 590/1093]  eta: 0:11:51    time: 1.2681  data: 0.0009  max mem: 65949
Evaluation  [ 600/1093]  eta: 0:11:38    time: 1.3761  data: 0.0010  max mem: 65949
Evaluation  [ 610/1093]  eta: 0:11:24    time: 1.5104  data: 0.0010  max mem: 65949
Evaluation  [ 620/1093]  eta: 0:11:09    time: 1.3794  data: 0.0010  max mem: 65949
Evaluation  [ 630/1093]  eta: 0:10:55    time: 1.3936  data: 0.0010  max mem: 65949
Evaluation  [ 640/1093]  eta: 0:10:40    time: 1.3844  data: 0.0010  max mem: 65949
Evaluation  [ 650/1093]  eta: 0:10:27    time: 1.4493  data: 0.0010  max mem: 65949
Evaluation  [ 660/1093]  eta: 0:10:12    time: 1.4127  data: 0.0009  max mem: 65949
Evaluation  [ 670/1093]  eta: 0:09:58    time: 1.3308  data: 0.0009  max mem: 65949
Evaluation  [ 680/1093]  eta: 0:09:44    time: 1.4204  data: 0.0010  max mem: 65949
Evaluation  [ 690/1093]  eta: 0:09:30    time: 1.4639  data: 0.0010  max mem: 65949
Evaluation  [ 700/1093]  eta: 0:09:16    time: 1.4443  data: 0.0010  max mem: 65949
Evaluation  [ 710/1093]  eta: 0:09:00    time: 1.2766  data: 0.0009  max mem: 65949
Evaluation  [ 720/1093]  eta: 0:08:45    time: 1.1687  data: 0.0009  max mem: 65949
Evaluation  [ 730/1093]  eta: 0:08:29    time: 1.1356  data: 0.0009  max mem: 65949
Evaluation  [ 740/1093]  eta: 0:08:15    time: 1.1685  data: 0.0010  max mem: 65949
Evaluation  [ 750/1093]  eta: 0:08:00    time: 1.2874  data: 0.0010  max mem: 65949
Evaluation  [ 760/1093]  eta: 0:07:46    time: 1.3601  data: 0.0010  max mem: 65949
Evaluation  [ 770/1093]  eta: 0:07:32    time: 1.3893  data: 0.0009  max mem: 65949
Evaluation  [ 780/1093]  eta: 0:07:18    time: 1.4450  data: 0.0009  max mem: 65949
Evaluation  [ 790/1093]  eta: 0:07:03    time: 1.2770  data: 0.0009  max mem: 65949
Evaluation  [ 800/1093]  eta: 0:06:50    time: 1.3656  data: 0.0010  max mem: 65949
Evaluation  [ 810/1093]  eta: 0:06:36    time: 1.5719  data: 0.0010  max mem: 65949
Evaluation  [ 820/1093]  eta: 0:06:23    time: 1.4774  data: 0.0010  max mem: 65949
Evaluation  [ 830/1093]  eta: 0:06:09    time: 1.5422  data: 0.0010  max mem: 65949
Evaluation  [ 840/1093]  eta: 0:05:55    time: 1.4368  data: 0.0009  max mem: 65949
Evaluation  [ 850/1093]  eta: 0:05:41    time: 1.3349  data: 0.0009  max mem: 65949
Evaluation  [ 860/1093]  eta: 0:05:26    time: 1.2907  data: 0.0009  max mem: 65949
Evaluation  [ 870/1093]  eta: 0:05:12    time: 1.2977  data: 0.0009  max mem: 65949
Evaluation  [ 880/1093]  eta: 0:04:58    time: 1.4988  data: 0.0010  max mem: 65949
Evaluation  [ 890/1093]  eta: 0:04:44    time: 1.3383  data: 0.0010  max mem: 65949
Evaluation  [ 900/1093]  eta: 0:04:30    time: 1.2621  data: 0.0009  max mem: 65949
Evaluation  [ 910/1093]  eta: 0:04:16    time: 1.4257  data: 0.0009  max mem: 65949
Evaluation  [ 920/1093]  eta: 0:04:02    time: 1.4566  data: 0.0009  max mem: 65949
Evaluation  [ 930/1093]  eta: 0:03:48    time: 1.4284  data: 0.0009  max mem: 65949
Evaluation  [ 940/1093]  eta: 0:03:34    time: 1.3566  data: 0.0009  max mem: 65949
Evaluation  [ 950/1093]  eta: 0:03:20    time: 1.3337  data: 0.0010  max mem: 65949
Evaluation  [ 960/1093]  eta: 0:03:06    time: 1.3954  data: 0.0010  max mem: 65949
Evaluation  [ 970/1093]  eta: 0:02:52    time: 1.4382  data: 0.0010  max mem: 65949
Evaluation  [ 980/1093]  eta: 0:02:38    time: 1.3545  data: 0.0011  max mem: 65949
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.3553  data: 0.0011  max mem: 65949
Evaluation  [1000/1093]  eta: 0:02:10    time: 1.4333  data: 0.0010  max mem: 65949
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4561  data: 0.0009  max mem: 65949
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3246  data: 0.0009  max mem: 65949
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1963  data: 0.0009  max mem: 65949
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2737  data: 0.0009  max mem: 65949
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3385  data: 0.0010  max mem: 65949
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3755  data: 0.0010  max mem: 65949
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4220  data: 0.0010  max mem: 65949
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3820  data: 0.0010  max mem: 65949
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3347  data: 0.0009  max mem: 65949
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3054  data: 0.0417  max mem: 65949
Evaluation Total time: 0:25:25 (1.3954 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_4_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [5]  [   0/7110]  eta: 2 days, 5:43:51  lr: 0.000010  loss: 0.4312  time: 27.2056  data: 0.0001  max mem: 65949
Train: data epoch: [5]  [  50/7110]  eta: 2:58:49  lr: 0.000010  loss: 0.3764  time: 0.9971  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 100/7110]  eta: 2:25:54  lr: 0.000010  loss: 0.1400  time: 0.9541  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 150/7110]  eta: 2:15:48  lr: 0.000010  loss: 0.2228  time: 0.9574  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 200/7110]  eta: 2:10:02  lr: 0.000010  loss: 0.1107  time: 1.0340  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 250/7110]  eta: 2:05:23  lr: 0.000010  loss: 0.1815  time: 0.9623  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 300/7110]  eta: 2:02:37  lr: 0.000010  loss: 0.5154  time: 0.9940  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 350/7110]  eta: 2:00:20  lr: 0.000010  loss: 0.1012  time: 0.9799  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 400/7110]  eta: 1:59:14  lr: 0.000010  loss: 0.0639  time: 1.0988  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 450/7110]  eta: 1:57:06  lr: 0.000010  loss: 0.1588  time: 0.9639  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 500/7110]  eta: 1:55:18  lr: 0.000010  loss: 0.4177  time: 0.9506  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 550/7110]  eta: 1:54:03  lr: 0.000010  loss: 0.4952  time: 1.0574  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 600/7110]  eta: 1:52:41  lr: 0.000010  loss: 0.0806  time: 1.0038  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 650/7110]  eta: 1:51:36  lr: 0.000010  loss: 0.0626  time: 1.0371  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 700/7110]  eta: 1:50:44  lr: 0.000010  loss: 0.5663  time: 0.9875  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 750/7110]  eta: 1:49:38  lr: 0.000010  loss: 0.2601  time: 0.9625  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 800/7110]  eta: 1:48:44  lr: 0.000010  loss: 0.2184  time: 1.0550  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 850/7110]  eta: 1:47:37  lr: 0.000010  loss: 0.6457  time: 0.9483  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 900/7110]  eta: 1:46:43  lr: 0.000010  loss: 0.1609  time: 1.0086  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [ 950/7110]  eta: 1:45:39  lr: 0.000010  loss: 0.3384  time: 1.0120  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1000/7110]  eta: 1:44:45  lr: 0.000010  loss: 0.2498  time: 1.0252  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1050/7110]  eta: 1:43:41  lr: 0.000010  loss: 0.1315  time: 0.9469  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1100/7110]  eta: 1:42:51  lr: 0.000010  loss: 0.1176  time: 1.0663  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1150/7110]  eta: 1:41:56  lr: 0.000010  loss: 0.3054  time: 0.9620  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1200/7110]  eta: 1:41:03  lr: 0.000010  loss: 0.2839  time: 0.9841  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1250/7110]  eta: 1:40:13  lr: 0.000010  loss: 0.1611  time: 1.0243  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1300/7110]  eta: 1:39:17  lr: 0.000010  loss: 0.1691  time: 1.0178  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1350/7110]  eta: 1:38:22  lr: 0.000010  loss: 0.4132  time: 1.0003  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1400/7110]  eta: 1:37:11  lr: 0.000010  loss: 0.4922  time: 0.9497  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1450/7110]  eta: 1:36:14  lr: 0.000010  loss: 0.1586  time: 1.0408  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1500/7110]  eta: 1:35:19  lr: 0.000010  loss: 0.1379  time: 1.0844  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1550/7110]  eta: 1:34:20  lr: 0.000010  loss: 0.2279  time: 0.9569  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1600/7110]  eta: 1:33:19  lr: 0.000010  loss: 0.2491  time: 0.9685  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1650/7110]  eta: 1:32:26  lr: 0.000010  loss: 0.1944  time: 1.0114  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1700/7110]  eta: 1:31:35  lr: 0.000010  loss: 0.0908  time: 1.0426  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1750/7110]  eta: 1:30:35  lr: 0.000010  loss: 0.1754  time: 0.9629  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1800/7110]  eta: 1:29:42  lr: 0.000010  loss: 0.3056  time: 1.0081  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1850/7110]  eta: 1:28:51  lr: 0.000010  loss: 0.1162  time: 1.0358  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1900/7110]  eta: 1:27:57  lr: 0.000010  loss: 0.6712  time: 0.9606  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [1950/7110]  eta: 1:27:10  lr: 0.000010  loss: 0.3750  time: 1.0767  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2000/7110]  eta: 1:26:16  lr: 0.000010  loss: 0.2192  time: 1.0091  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2050/7110]  eta: 1:25:26  lr: 0.000010  loss: 0.3036  time: 1.1012  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2100/7110]  eta: 1:24:31  lr: 0.000010  loss: 0.6628  time: 0.9682  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2150/7110]  eta: 1:23:39  lr: 0.000010  loss: 0.1819  time: 1.0051  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2200/7110]  eta: 1:22:52  lr: 0.000010  loss: 0.5670  time: 1.0984  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2250/7110]  eta: 1:22:00  lr: 0.000010  loss: 0.2203  time: 0.9671  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2300/7110]  eta: 1:21:08  lr: 0.000010  loss: 0.4203  time: 1.0029  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2350/7110]  eta: 1:20:20  lr: 0.000010  loss: 0.2012  time: 1.1008  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2400/7110]  eta: 1:19:27  lr: 0.000010  loss: 0.6671  time: 0.9751  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2450/7110]  eta: 1:18:39  lr: 0.000010  loss: 0.5031  time: 1.0126  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2500/7110]  eta: 1:17:45  lr: 0.000010  loss: 0.0370  time: 0.9963  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2550/7110]  eta: 1:16:51  lr: 0.000010  loss: 0.1209  time: 1.0257  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2600/7110]  eta: 1:16:04  lr: 0.000010  loss: 1.5672  time: 1.0700  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2650/7110]  eta: 1:15:12  lr: 0.000010  loss: 0.3818  time: 1.0085  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2700/7110]  eta: 1:14:21  lr: 0.000010  loss: 0.1092  time: 1.0414  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2750/7110]  eta: 1:13:28  lr: 0.000010  loss: 0.2524  time: 0.9763  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2800/7110]  eta: 1:12:37  lr: 0.000010  loss: 0.5671  time: 1.0381  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2850/7110]  eta: 1:11:43  lr: 0.000010  loss: 0.4505  time: 0.9627  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2900/7110]  eta: 1:10:49  lr: 0.000010  loss: 0.1032  time: 0.9568  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [2950/7110]  eta: 1:09:56  lr: 0.000010  loss: 0.2781  time: 0.9840  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3000/7110]  eta: 1:09:04  lr: 0.000010  loss: 0.1232  time: 0.9396  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3050/7110]  eta: 1:08:13  lr: 0.000010  loss: 0.0918  time: 0.9932  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3100/7110]  eta: 1:07:23  lr: 0.000010  loss: 0.5987  time: 0.9805  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3150/7110]  eta: 1:06:32  lr: 0.000010  loss: 0.3125  time: 0.9750  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3200/7110]  eta: 1:05:40  lr: 0.000010  loss: 0.1141  time: 0.9661  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3250/7110]  eta: 1:04:48  lr: 0.000010  loss: 0.2693  time: 0.9763  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3300/7110]  eta: 1:03:57  lr: 0.000010  loss: 0.0403  time: 1.0289  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3350/7110]  eta: 1:03:07  lr: 0.000010  loss: 0.1636  time: 1.0763  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3400/7110]  eta: 1:02:17  lr: 0.000010  loss: 0.2113  time: 1.0220  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3450/7110]  eta: 1:01:25  lr: 0.000010  loss: 0.1839  time: 0.9823  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3500/7110]  eta: 1:00:34  lr: 0.000010  loss: 0.2820  time: 0.9524  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3550/7110]  eta: 0:59:43  lr: 0.000010  loss: 0.0798  time: 1.0175  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3600/7110]  eta: 0:58:52  lr: 0.000010  loss: 0.1245  time: 0.9982  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3650/7110]  eta: 0:58:02  lr: 0.000010  loss: 0.2397  time: 1.0364  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3700/7110]  eta: 0:57:10  lr: 0.000010  loss: 0.1357  time: 0.9698  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3750/7110]  eta: 0:56:20  lr: 0.000010  loss: 0.5308  time: 0.9909  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3800/7110]  eta: 0:55:29  lr: 0.000010  loss: 0.2312  time: 0.9984  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3850/7110]  eta: 0:54:37  lr: 0.000010  loss: 0.1635  time: 0.9712  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3900/7110]  eta: 0:53:47  lr: 0.000010  loss: 0.5500  time: 1.0279  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [3950/7110]  eta: 0:52:56  lr: 0.000010  loss: 0.3999  time: 0.9938  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4000/7110]  eta: 0:52:05  lr: 0.000010  loss: 0.6029  time: 0.9643  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4050/7110]  eta: 0:51:15  lr: 0.000010  loss: 0.0565  time: 1.0018  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4100/7110]  eta: 0:50:25  lr: 0.000010  loss: 0.2339  time: 1.0338  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4150/7110]  eta: 0:49:36  lr: 0.000010  loss: 0.6460  time: 1.0097  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4200/7110]  eta: 0:48:45  lr: 0.000010  loss: 0.1434  time: 0.9792  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4250/7110]  eta: 0:47:54  lr: 0.000010  loss: 1.1750  time: 0.9970  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4300/7110]  eta: 0:47:04  lr: 0.000010  loss: 0.5574  time: 0.9460  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4350/7110]  eta: 0:46:14  lr: 0.000010  loss: 0.3925  time: 0.9737  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4400/7110]  eta: 0:45:23  lr: 0.000010  loss: 0.4942  time: 0.9681  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4450/7110]  eta: 0:44:31  lr: 0.000010  loss: 0.1649  time: 0.9507  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4500/7110]  eta: 0:43:40  lr: 0.000010  loss: 0.2349  time: 1.0205  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4550/7110]  eta: 0:42:51  lr: 0.000010  loss: 0.1062  time: 1.0770  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4600/7110]  eta: 0:42:02  lr: 0.000010  loss: 0.1927  time: 1.0185  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4650/7110]  eta: 0:41:11  lr: 0.000010  loss: 0.1772  time: 0.9948  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4700/7110]  eta: 0:40:21  lr: 0.000010  loss: 0.6205  time: 0.9702  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4750/7110]  eta: 0:39:31  lr: 0.000010  loss: 0.7056  time: 1.0071  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4800/7110]  eta: 0:38:41  lr: 0.000010  loss: 0.2830  time: 0.9789  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4850/7110]  eta: 0:37:50  lr: 0.000010  loss: 0.5393  time: 0.9529  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4900/7110]  eta: 0:37:00  lr: 0.000010  loss: 0.6134  time: 0.9768  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [4950/7110]  eta: 0:36:09  lr: 0.000010  loss: 0.1273  time: 1.0239  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5000/7110]  eta: 0:35:19  lr: 0.000010  loss: 0.1083  time: 0.9955  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5050/7110]  eta: 0:34:28  lr: 0.000010  loss: 0.2182  time: 0.9868  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5100/7110]  eta: 0:33:39  lr: 0.000010  loss: 0.3465  time: 1.0296  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5150/7110]  eta: 0:32:49  lr: 0.000010  loss: 0.2420  time: 1.0122  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5200/7110]  eta: 0:31:57  lr: 0.000010  loss: 0.2431  time: 0.9342  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5250/7110]  eta: 0:31:07  lr: 0.000010  loss: 0.1018  time: 1.0043  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5300/7110]  eta: 0:30:17  lr: 0.000010  loss: 0.1511  time: 0.9809  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5350/7110]  eta: 0:29:26  lr: 0.000010  loss: 0.2320  time: 0.9684  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5400/7110]  eta: 0:28:36  lr: 0.000010  loss: 0.6395  time: 0.9807  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5450/7110]  eta: 0:27:46  lr: 0.000010  loss: 0.2794  time: 0.9894  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5500/7110]  eta: 0:26:56  lr: 0.000010  loss: 0.5953  time: 1.0329  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5550/7110]  eta: 0:26:06  lr: 0.000010  loss: 0.1085  time: 1.0162  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5600/7110]  eta: 0:25:15  lr: 0.000010  loss: 0.2388  time: 0.9800  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5650/7110]  eta: 0:24:25  lr: 0.000010  loss: 0.5808  time: 1.0276  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5700/7110]  eta: 0:23:35  lr: 0.000010  loss: 0.3697  time: 1.0041  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5750/7110]  eta: 0:22:45  lr: 0.000010  loss: 0.2240  time: 1.0007  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5800/7110]  eta: 0:21:55  lr: 0.000010  loss: 0.3540  time: 1.0331  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 0.9270  time: 0.9452  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.5015  time: 0.9681  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.2302  time: 0.9928  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.8960  time: 0.9882  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.0818  time: 0.9380  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.1542  time: 0.9974  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.1734  time: 0.9838  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.1477  time: 1.0555  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.1661  time: 0.9611  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.4987  time: 1.0137  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.0583  time: 1.0558  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.4054  time: 0.9450  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.3548  time: 1.0226  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.3832  time: 1.0092  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.6516  time: 1.0239  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.2665  time: 1.0498  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.4446  time: 0.9671  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.5045  time: 1.0031  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.0700  time: 0.9341  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.5130  time: 1.0368  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1365  time: 1.0053  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1385  time: 1.0507  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.2951  time: 1.0297  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.4388  time: 1.0033  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 1.3344  time: 0.9963  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.3981  time: 0.9814  data: 0.0000  max mem: 65949
Train: data epoch: [5]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.1691  time: 1.0810  data: 0.0000  max mem: 65949
Train: data epoch: [5] Total time: 1:58:51 (1.0030 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:14:40    time: 20.5679  data: 19.3118  max mem: 65949
Evaluation  [  10/1093]  eta: 0:56:35    time: 3.1351  data: 1.7564  max mem: 65949
Evaluation  [  20/1093]  eta: 0:40:29    time: 1.3486  data: 0.0009  max mem: 65949
Evaluation  [  30/1093]  eta: 0:34:05    time: 1.2590  data: 0.0009  max mem: 65949
Evaluation  [  40/1093]  eta: 0:31:27    time: 1.2978  data: 0.0010  max mem: 65949
Evaluation  [  50/1093]  eta: 0:29:53    time: 1.4010  data: 0.0009  max mem: 65949
Evaluation  [  60/1093]  eta: 0:28:03    time: 1.2969  data: 0.0009  max mem: 65949
Evaluation  [  70/1093]  eta: 0:27:23    time: 1.3177  data: 0.0009  max mem: 65949
Evaluation  [  80/1093]  eta: 0:26:33    time: 1.3985  data: 0.0009  max mem: 65949
Evaluation  [  90/1093]  eta: 0:25:52    time: 1.3419  data: 0.0009  max mem: 65949
Evaluation  [ 100/1093]  eta: 0:25:29    time: 1.4092  data: 0.0009  max mem: 65949
Evaluation  [ 110/1093]  eta: 0:25:08    time: 1.4715  data: 0.0009  max mem: 65949
Evaluation  [ 120/1093]  eta: 0:24:35    time: 1.3967  data: 0.0010  max mem: 65949
Evaluation  [ 130/1093]  eta: 0:24:24    time: 1.4445  data: 0.0010  max mem: 65949
Evaluation  [ 140/1093]  eta: 0:23:45    time: 1.3715  data: 0.0009  max mem: 65949
Evaluation  [ 150/1093]  eta: 0:23:18    time: 1.2361  data: 0.0009  max mem: 65949
Evaluation  [ 160/1093]  eta: 0:22:59    time: 1.3525  data: 0.0010  max mem: 65949
Evaluation  [ 170/1093]  eta: 0:22:34    time: 1.3531  data: 0.0010  max mem: 65949
Evaluation  [ 180/1093]  eta: 0:22:18    time: 1.3642  data: 0.0009  max mem: 65949
Evaluation  [ 190/1093]  eta: 0:22:02    time: 1.4382  data: 0.0010  max mem: 65949
Evaluation  [ 200/1093]  eta: 0:21:38    time: 1.3455  data: 0.0010  max mem: 65949
Evaluation  [ 210/1093]  eta: 0:21:31    time: 1.4418  data: 0.0009  max mem: 65949
Evaluation  [ 220/1093]  eta: 0:21:06    time: 1.4201  data: 0.0009  max mem: 65949
Evaluation  [ 230/1093]  eta: 0:20:44    time: 1.2333  data: 0.0009  max mem: 65949
Evaluation  [ 240/1093]  eta: 0:20:25    time: 1.2848  data: 0.0009  max mem: 65949
Evaluation  [ 250/1093]  eta: 0:20:11    time: 1.3721  data: 0.0009  max mem: 65949
Evaluation  [ 260/1093]  eta: 0:20:04    time: 1.5538  data: 0.0009  max mem: 65949
Evaluation  [ 270/1093]  eta: 0:19:42    time: 1.4337  data: 0.0009  max mem: 65949
Evaluation  [ 280/1093]  eta: 0:19:25    time: 1.2794  data: 0.0009  max mem: 65949
Evaluation  [ 290/1093]  eta: 0:19:18    time: 1.5197  data: 0.0009  max mem: 65949
Evaluation  [ 300/1093]  eta: 0:18:56    time: 1.4187  data: 0.0009  max mem: 65949
Evaluation  [ 310/1093]  eta: 0:18:38    time: 1.2192  data: 0.0010  max mem: 65949
Evaluation  [ 320/1093]  eta: 0:18:25    time: 1.3957  data: 0.0010  max mem: 65949
Evaluation  [ 330/1093]  eta: 0:18:08    time: 1.3980  data: 0.0009  max mem: 65949
Evaluation  [ 340/1093]  eta: 0:17:55    time: 1.3942  data: 0.0010  max mem: 65949
Evaluation  [ 350/1093]  eta: 0:17:40    time: 1.4487  data: 0.0010  max mem: 65949
Evaluation  [ 360/1093]  eta: 0:17:21    time: 1.2828  data: 0.0010  max mem: 65949
Evaluation  [ 370/1093]  eta: 0:17:06    time: 1.2870  data: 0.0010  max mem: 65949
Evaluation  [ 380/1093]  eta: 0:16:55    time: 1.4932  data: 0.0010  max mem: 65949
Evaluation  [ 390/1093]  eta: 0:16:41    time: 1.5013  data: 0.0010  max mem: 65949
Evaluation  [ 400/1093]  eta: 0:16:28    time: 1.4570  data: 0.0010  max mem: 65949
Evaluation  [ 410/1093]  eta: 0:16:12    time: 1.4298  data: 0.0011  max mem: 65949
Evaluation  [ 420/1093]  eta: 0:15:59    time: 1.4142  data: 0.0011  max mem: 65949
Evaluation  [ 430/1093]  eta: 0:15:43    time: 1.3989  data: 0.0010  max mem: 65949
Evaluation  [ 440/1093]  eta: 0:15:25    time: 1.2399  data: 0.0011  max mem: 65949
Evaluation  [ 450/1093]  eta: 0:15:11    time: 1.2815  data: 0.0011  max mem: 65949
Evaluation  [ 460/1093]  eta: 0:14:56    time: 1.3905  data: 0.0010  max mem: 65949
Evaluation  [ 470/1093]  eta: 0:14:40    time: 1.3444  data: 0.0010  max mem: 65949
Evaluation  [ 480/1093]  eta: 0:14:27    time: 1.3941  data: 0.0010  max mem: 65949
Evaluation  [ 490/1093]  eta: 0:14:14    time: 1.5012  data: 0.0010  max mem: 65949
Evaluation  [ 500/1093]  eta: 0:14:02    time: 1.5619  data: 0.0010  max mem: 65949
Evaluation  [ 510/1093]  eta: 0:13:49    time: 1.5557  data: 0.0010  max mem: 65949
Evaluation  [ 520/1093]  eta: 0:13:35    time: 1.4891  data: 0.0010  max mem: 65949
Evaluation  [ 530/1093]  eta: 0:13:22    time: 1.4697  data: 0.0011  max mem: 65949
Evaluation  [ 540/1093]  eta: 0:13:07    time: 1.4531  data: 0.0011  max mem: 65949
Evaluation  [ 550/1093]  eta: 0:12:53    time: 1.4267  data: 0.0009  max mem: 65949
Evaluation  [ 560/1093]  eta: 0:12:36    time: 1.2667  data: 0.0009  max mem: 65949
Evaluation  [ 570/1093]  eta: 0:12:21    time: 1.2227  data: 0.0009  max mem: 65949
Evaluation  [ 580/1093]  eta: 0:12:06    time: 1.3394  data: 0.0009  max mem: 65949
Evaluation  [ 590/1093]  eta: 0:11:50    time: 1.2467  data: 0.0009  max mem: 65949
Evaluation  [ 600/1093]  eta: 0:11:37    time: 1.3770  data: 0.0009  max mem: 65949
Evaluation  [ 610/1093]  eta: 0:11:23    time: 1.5163  data: 0.0009  max mem: 65949
Evaluation  [ 620/1093]  eta: 0:11:08    time: 1.4007  data: 0.0009  max mem: 65949
Evaluation  [ 630/1093]  eta: 0:10:55    time: 1.4030  data: 0.0009  max mem: 65949
Evaluation  [ 640/1093]  eta: 0:10:39    time: 1.3625  data: 0.0010  max mem: 65949
Evaluation  [ 650/1093]  eta: 0:10:26    time: 1.4117  data: 0.0010  max mem: 65949
Evaluation  [ 660/1093]  eta: 0:10:11    time: 1.4033  data: 0.0010  max mem: 65949
Evaluation  [ 670/1093]  eta: 0:09:57    time: 1.3776  data: 0.0010  max mem: 65949
Evaluation  [ 680/1093]  eta: 0:09:43    time: 1.4479  data: 0.0010  max mem: 65949
Evaluation  [ 690/1093]  eta: 0:09:30    time: 1.4601  data: 0.0009  max mem: 65949
Evaluation  [ 700/1093]  eta: 0:09:15    time: 1.4525  data: 0.0010  max mem: 65949
Evaluation  [ 710/1093]  eta: 0:09:00    time: 1.2982  data: 0.0009  max mem: 65949
Evaluation  [ 720/1093]  eta: 0:08:45    time: 1.1590  data: 0.0009  max mem: 65949
Evaluation  [ 730/1093]  eta: 0:08:29    time: 1.1073  data: 0.0010  max mem: 65949
Evaluation  [ 740/1093]  eta: 0:08:14    time: 1.1635  data: 0.0010  max mem: 65949
Evaluation  [ 750/1093]  eta: 0:08:00    time: 1.2911  data: 0.0010  max mem: 65949
Evaluation  [ 760/1093]  eta: 0:07:46    time: 1.3589  data: 0.0010  max mem: 65949
Evaluation  [ 770/1093]  eta: 0:07:32    time: 1.3793  data: 0.0010  max mem: 65949
Evaluation  [ 780/1093]  eta: 0:07:18    time: 1.4409  data: 0.0010  max mem: 65949
Evaluation  [ 790/1093]  eta: 0:07:03    time: 1.2811  data: 0.0009  max mem: 65949
Evaluation  [ 800/1093]  eta: 0:06:50    time: 1.3785  data: 0.0010  max mem: 65949
Evaluation  [ 810/1093]  eta: 0:06:36    time: 1.5803  data: 0.0010  max mem: 65949
Evaluation  [ 820/1093]  eta: 0:06:22    time: 1.4760  data: 0.0009  max mem: 65949
Evaluation  [ 830/1093]  eta: 0:06:09    time: 1.5526  data: 0.0009  max mem: 65949
Evaluation  [ 840/1093]  eta: 0:05:55    time: 1.4543  data: 0.0009  max mem: 65949
Evaluation  [ 850/1093]  eta: 0:05:41    time: 1.3443  data: 0.0010  max mem: 65949
Evaluation  [ 860/1093]  eta: 0:05:26    time: 1.3564  data: 0.0009  max mem: 65949
Evaluation  [ 870/1093]  eta: 0:05:12    time: 1.3574  data: 0.0009  max mem: 65949
Evaluation  [ 880/1093]  eta: 0:04:59    time: 1.4979  data: 0.0009  max mem: 65949
Evaluation  [ 890/1093]  eta: 0:04:44    time: 1.3496  data: 0.0009  max mem: 65949
Evaluation  [ 900/1093]  eta: 0:04:30    time: 1.2733  data: 0.0009  max mem: 65949
Evaluation  [ 910/1093]  eta: 0:04:16    time: 1.4274  data: 0.0009  max mem: 65949
Evaluation  [ 920/1093]  eta: 0:04:02    time: 1.4461  data: 0.0010  max mem: 65949
Evaluation  [ 930/1093]  eta: 0:03:48    time: 1.4200  data: 0.0010  max mem: 65949
Evaluation  [ 940/1093]  eta: 0:03:34    time: 1.3626  data: 0.0009  max mem: 65949
Evaluation  [ 950/1093]  eta: 0:03:20    time: 1.3397  data: 0.0009  max mem: 65949
Evaluation  [ 960/1093]  eta: 0:03:06    time: 1.4172  data: 0.0009  max mem: 65949
Evaluation  [ 970/1093]  eta: 0:02:52    time: 1.4172  data: 0.0010  max mem: 65949
Evaluation  [ 980/1093]  eta: 0:02:38    time: 1.3245  data: 0.0010  max mem: 65949
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.3754  data: 0.0010  max mem: 65949
Evaluation  [1000/1093]  eta: 0:02:10    time: 1.4431  data: 0.0009  max mem: 65949
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4635  data: 0.0009  max mem: 65949
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3342  data: 0.0009  max mem: 65949
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1930  data: 0.0009  max mem: 65949
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2770  data: 0.0010  max mem: 65949
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3371  data: 0.0009  max mem: 65949
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3654  data: 0.0009  max mem: 65949
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4242  data: 0.0009  max mem: 65949
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3934  data: 0.0010  max mem: 65949
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3535  data: 0.0010  max mem: 65949
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3241  data: 0.0415  max mem: 65949
Evaluation Total time: 0:25:26 (1.3966 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_5_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [6]  [   0/7110]  eta: 2 days, 7:05:17  lr: 0.000010  loss: 0.8294  time: 27.8928  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [  50/7110]  eta: 2:57:25  lr: 0.000010  loss: 0.2121  time: 0.9922  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 100/7110]  eta: 2:26:16  lr: 0.000010  loss: 0.6262  time: 0.9761  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 150/7110]  eta: 2:15:44  lr: 0.000010  loss: 0.1792  time: 0.9616  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 200/7110]  eta: 2:09:18  lr: 0.000010  loss: 0.4604  time: 0.9678  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 250/7110]  eta: 2:05:55  lr: 0.000010  loss: 0.3241  time: 1.0498  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 300/7110]  eta: 2:02:31  lr: 0.000010  loss: 0.2617  time: 0.9281  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 350/7110]  eta: 2:00:22  lr: 0.000010  loss: 0.5020  time: 0.9880  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 400/7110]  eta: 1:58:02  lr: 0.000010  loss: 0.2116  time: 0.9765  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 450/7110]  eta: 1:57:13  lr: 0.000010  loss: 0.2069  time: 1.0453  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 500/7110]  eta: 1:55:35  lr: 0.000010  loss: 0.8290  time: 1.0063  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 550/7110]  eta: 1:53:56  lr: 0.000010  loss: 0.2672  time: 0.9571  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 600/7110]  eta: 1:52:38  lr: 0.000010  loss: 0.4145  time: 1.0176  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 650/7110]  eta: 1:51:25  lr: 0.000010  loss: 0.2733  time: 1.0203  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 700/7110]  eta: 1:50:15  lr: 0.000010  loss: 0.2465  time: 1.0164  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 750/7110]  eta: 1:49:01  lr: 0.000010  loss: 0.1967  time: 0.9462  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 800/7110]  eta: 1:48:06  lr: 0.000010  loss: 0.2186  time: 1.0975  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 850/7110]  eta: 1:46:57  lr: 0.000010  loss: 0.4816  time: 0.9711  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 900/7110]  eta: 1:45:44  lr: 0.000010  loss: 0.2611  time: 0.9625  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [ 950/7110]  eta: 1:44:31  lr: 0.000010  loss: 0.1794  time: 0.9340  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1000/7110]  eta: 1:43:39  lr: 0.000010  loss: 0.1711  time: 1.0140  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1050/7110]  eta: 1:43:03  lr: 0.000010  loss: 0.0549  time: 1.0569  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1100/7110]  eta: 1:42:05  lr: 0.000010  loss: 0.2680  time: 0.9596  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1150/7110]  eta: 1:41:06  lr: 0.000010  loss: 0.2595  time: 0.9973  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1200/7110]  eta: 1:40:11  lr: 0.000010  loss: 0.0982  time: 0.9687  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1250/7110]  eta: 1:39:19  lr: 0.000010  loss: 0.2620  time: 1.0054  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1300/7110]  eta: 1:38:29  lr: 0.000010  loss: 0.1597  time: 0.9921  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1350/7110]  eta: 1:37:29  lr: 0.000010  loss: 0.1092  time: 0.9674  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1400/7110]  eta: 1:36:31  lr: 0.000010  loss: 1.2660  time: 0.9787  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1450/7110]  eta: 1:35:40  lr: 0.000010  loss: 0.1101  time: 1.0026  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1500/7110]  eta: 1:34:46  lr: 0.000010  loss: 0.2208  time: 1.0030  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1550/7110]  eta: 1:33:58  lr: 0.000010  loss: 0.7589  time: 0.9920  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1600/7110]  eta: 1:33:09  lr: 0.000010  loss: 0.4750  time: 1.0313  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1650/7110]  eta: 1:32:14  lr: 0.000010  loss: 0.0854  time: 1.0215  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1700/7110]  eta: 1:31:20  lr: 0.000010  loss: 0.2261  time: 0.9915  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1750/7110]  eta: 1:30:32  lr: 0.000010  loss: 0.2661  time: 1.0228  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1800/7110]  eta: 1:29:40  lr: 0.000010  loss: 0.2311  time: 0.9848  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1850/7110]  eta: 1:28:49  lr: 0.000010  loss: 0.1513  time: 1.0229  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1900/7110]  eta: 1:27:56  lr: 0.000010  loss: 0.1785  time: 1.0105  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [1950/7110]  eta: 1:27:01  lr: 0.000010  loss: 0.2375  time: 0.9854  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2000/7110]  eta: 1:26:08  lr: 0.000010  loss: 0.2436  time: 0.9764  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2050/7110]  eta: 1:25:16  lr: 0.000010  loss: 0.4796  time: 1.0180  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2100/7110]  eta: 1:24:24  lr: 0.000010  loss: 0.3364  time: 0.9737  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2150/7110]  eta: 1:23:30  lr: 0.000010  loss: 0.0826  time: 0.9950  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2200/7110]  eta: 1:22:34  lr: 0.000010  loss: 0.3162  time: 0.9265  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2250/7110]  eta: 1:21:45  lr: 0.000010  loss: 0.1695  time: 0.9984  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2300/7110]  eta: 1:20:57  lr: 0.000010  loss: 0.0943  time: 1.0368  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2350/7110]  eta: 1:20:02  lr: 0.000010  loss: 0.3152  time: 0.9915  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2400/7110]  eta: 1:19:11  lr: 0.000010  loss: 0.2817  time: 0.9866  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2450/7110]  eta: 1:18:16  lr: 0.000010  loss: 0.3104  time: 0.9466  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2500/7110]  eta: 1:17:29  lr: 0.000010  loss: 0.3129  time: 1.0348  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2550/7110]  eta: 1:16:39  lr: 0.000010  loss: 0.4161  time: 1.0346  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2600/7110]  eta: 1:15:45  lr: 0.000010  loss: 0.2281  time: 0.9942  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2650/7110]  eta: 1:14:53  lr: 0.000010  loss: 0.0769  time: 0.9882  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2700/7110]  eta: 1:14:03  lr: 0.000010  loss: 0.1972  time: 1.0046  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2750/7110]  eta: 1:13:11  lr: 0.000010  loss: 0.1777  time: 0.9680  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2800/7110]  eta: 1:12:21  lr: 0.000010  loss: 1.6917  time: 1.0349  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2850/7110]  eta: 1:11:30  lr: 0.000010  loss: 0.4352  time: 0.9943  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2900/7110]  eta: 1:10:38  lr: 0.000010  loss: 0.6350  time: 0.9924  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [2950/7110]  eta: 1:09:46  lr: 0.000010  loss: 0.5896  time: 0.9521  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3000/7110]  eta: 1:08:55  lr: 0.000010  loss: 0.1214  time: 0.9793  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3050/7110]  eta: 1:08:03  lr: 0.000010  loss: 0.1974  time: 0.9795  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3100/7110]  eta: 1:07:12  lr: 0.000010  loss: 0.1220  time: 1.0060  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3150/7110]  eta: 1:06:21  lr: 0.000010  loss: 0.2239  time: 1.0298  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3200/7110]  eta: 1:05:30  lr: 0.000010  loss: 0.7604  time: 1.0237  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3250/7110]  eta: 1:04:39  lr: 0.000010  loss: 0.1282  time: 0.9499  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3300/7110]  eta: 1:03:47  lr: 0.000010  loss: 0.2116  time: 0.9806  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3350/7110]  eta: 1:02:58  lr: 0.000010  loss: 0.0746  time: 1.0377  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3400/7110]  eta: 1:02:06  lr: 0.000010  loss: 0.6323  time: 0.9865  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3450/7110]  eta: 1:01:13  lr: 0.000010  loss: 0.2254  time: 0.9471  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3500/7110]  eta: 1:00:22  lr: 0.000010  loss: 0.1336  time: 0.9457  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3550/7110]  eta: 0:59:32  lr: 0.000010  loss: 0.2558  time: 1.0100  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3600/7110]  eta: 0:58:43  lr: 0.000010  loss: 0.1115  time: 1.0108  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3650/7110]  eta: 0:57:55  lr: 0.000010  loss: 0.0648  time: 1.0346  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3700/7110]  eta: 0:57:06  lr: 0.000010  loss: 0.1392  time: 1.0303  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3750/7110]  eta: 0:56:17  lr: 0.000010  loss: 0.0702  time: 1.0045  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3800/7110]  eta: 0:55:27  lr: 0.000010  loss: 0.5405  time: 1.0152  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3850/7110]  eta: 0:54:35  lr: 0.000010  loss: 0.4008  time: 0.9669  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3900/7110]  eta: 0:53:44  lr: 0.000010  loss: 0.1630  time: 1.0040  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [3950/7110]  eta: 0:52:54  lr: 0.000010  loss: 0.9964  time: 0.9670  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4000/7110]  eta: 0:52:03  lr: 0.000010  loss: 0.0967  time: 0.9949  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4050/7110]  eta: 0:51:13  lr: 0.000010  loss: 0.5463  time: 0.9228  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4100/7110]  eta: 0:50:22  lr: 0.000010  loss: 0.1288  time: 0.9637  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4150/7110]  eta: 0:49:32  lr: 0.000010  loss: 0.4431  time: 1.0086  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4200/7110]  eta: 0:48:43  lr: 0.000010  loss: 0.1140  time: 1.0560  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4250/7110]  eta: 0:47:52  lr: 0.000010  loss: 0.1267  time: 1.0327  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4300/7110]  eta: 0:47:02  lr: 0.000010  loss: 0.8061  time: 0.9682  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4350/7110]  eta: 0:46:10  lr: 0.000010  loss: 0.1482  time: 0.9763  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4400/7110]  eta: 0:45:19  lr: 0.000010  loss: 0.4755  time: 0.9969  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4450/7110]  eta: 0:44:30  lr: 0.000010  loss: 0.2707  time: 1.0383  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4500/7110]  eta: 0:43:41  lr: 0.000010  loss: 0.1123  time: 1.0745  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4550/7110]  eta: 0:42:51  lr: 0.000010  loss: 0.4595  time: 1.0340  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4600/7110]  eta: 0:42:01  lr: 0.000010  loss: 0.5360  time: 1.0273  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4650/7110]  eta: 0:41:10  lr: 0.000010  loss: 0.4659  time: 1.0125  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4700/7110]  eta: 0:40:20  lr: 0.000010  loss: 0.2678  time: 1.0051  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4750/7110]  eta: 0:39:30  lr: 0.000010  loss: 0.1589  time: 0.9877  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4800/7110]  eta: 0:38:41  lr: 0.000010  loss: 0.0552  time: 1.0513  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4850/7110]  eta: 0:37:50  lr: 0.000010  loss: 0.5641  time: 0.9801  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4900/7110]  eta: 0:37:00  lr: 0.000010  loss: 0.2064  time: 0.9660  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [4950/7110]  eta: 0:36:09  lr: 0.000010  loss: 0.3593  time: 0.9900  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5000/7110]  eta: 0:35:20  lr: 0.000010  loss: 0.3322  time: 1.0324  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5050/7110]  eta: 0:34:28  lr: 0.000010  loss: 0.2385  time: 0.9251  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5100/7110]  eta: 0:33:38  lr: 0.000010  loss: 0.2340  time: 1.0055  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5150/7110]  eta: 0:32:47  lr: 0.000010  loss: 0.2014  time: 0.9782  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5200/7110]  eta: 0:31:57  lr: 0.000010  loss: 0.8637  time: 1.0215  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5250/7110]  eta: 0:31:07  lr: 0.000010  loss: 0.2562  time: 0.9937  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5300/7110]  eta: 0:30:17  lr: 0.000010  loss: 0.4847  time: 1.0343  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5350/7110]  eta: 0:29:27  lr: 0.000010  loss: 0.0677  time: 1.0129  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5400/7110]  eta: 0:28:37  lr: 0.000010  loss: 0.2835  time: 1.0360  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5450/7110]  eta: 0:27:47  lr: 0.000010  loss: 0.3441  time: 1.0534  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5500/7110]  eta: 0:26:56  lr: 0.000010  loss: 0.1512  time: 0.9977  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5550/7110]  eta: 0:26:06  lr: 0.000010  loss: 0.3806  time: 1.0124  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5600/7110]  eta: 0:25:16  lr: 0.000010  loss: 0.2774  time: 1.1307  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5650/7110]  eta: 0:24:26  lr: 0.000010  loss: 0.5802  time: 1.0137  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5700/7110]  eta: 0:23:36  lr: 0.000010  loss: 0.2065  time: 1.0440  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5750/7110]  eta: 0:22:46  lr: 0.000010  loss: 0.1685  time: 1.0170  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5800/7110]  eta: 0:21:56  lr: 0.000010  loss: 1.7768  time: 1.0746  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5850/7110]  eta: 0:21:06  lr: 0.000010  loss: 0.2953  time: 0.9568  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5900/7110]  eta: 0:20:15  lr: 0.000010  loss: 0.4604  time: 0.9871  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [5950/7110]  eta: 0:19:25  lr: 0.000010  loss: 1.4624  time: 1.0119  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6000/7110]  eta: 0:18:35  lr: 0.000010  loss: 0.0348  time: 0.9566  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6050/7110]  eta: 0:17:45  lr: 0.000010  loss: 0.5464  time: 1.0174  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6100/7110]  eta: 0:16:54  lr: 0.000010  loss: 0.3836  time: 0.9778  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6150/7110]  eta: 0:16:04  lr: 0.000010  loss: 0.1382  time: 0.9906  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.1715  time: 0.9697  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.1780  time: 0.9998  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6300/7110]  eta: 0:13:33  lr: 0.000010  loss: 0.1534  time: 1.0388  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.1538  time: 0.9917  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.3579  time: 0.9766  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.5786  time: 1.1080  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.6132  time: 0.9551  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.0525  time: 0.9196  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.2035  time: 1.0008  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.9800  time: 1.0369  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.2927  time: 0.9884  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.5470  time: 0.9749  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.0549  time: 0.9984  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1518  time: 0.9721  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.5273  time: 0.9843  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1425  time: 0.9857  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.5375  time: 0.9501  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.0841  time: 1.0413  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.0306  time: 0.9789  data: 0.0000  max mem: 65949
Train: data epoch: [6]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0803  time: 1.0757  data: 0.0000  max mem: 65949
Train: data epoch: [6] Total time: 1:58:51 (1.0030 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:32:52    time: 21.5665  data: 20.3182  max mem: 65949
Evaluation  [  10/1093]  eta: 0:58:22    time: 3.2338  data: 1.8480  max mem: 65949
Evaluation  [  20/1093]  eta: 0:41:30    time: 1.3591  data: 0.0009  max mem: 65949
Evaluation  [  30/1093]  eta: 0:34:37    time: 1.2500  data: 0.0009  max mem: 65949
Evaluation  [  40/1093]  eta: 0:31:49    time: 1.2800  data: 0.0015  max mem: 65949
Evaluation  [  50/1093]  eta: 0:30:14    time: 1.4075  data: 0.0016  max mem: 65949
Evaluation  [  60/1093]  eta: 0:28:13    time: 1.2813  data: 0.0010  max mem: 65949
Evaluation  [  70/1093]  eta: 0:27:26    time: 1.2784  data: 0.0010  max mem: 65949
Evaluation  [  80/1093]  eta: 0:26:35    time: 1.3811  data: 0.0010  max mem: 65949
Evaluation  [  90/1093]  eta: 0:25:59    time: 1.3582  data: 0.0009  max mem: 65949
Evaluation  [ 100/1093]  eta: 0:25:35    time: 1.4285  data: 0.0009  max mem: 65949
Evaluation  [ 110/1093]  eta: 0:25:13    time: 1.4733  data: 0.0009  max mem: 65949
Evaluation  [ 120/1093]  eta: 0:24:36    time: 1.3696  data: 0.0010  max mem: 65949
Evaluation  [ 130/1093]  eta: 0:24:19    time: 1.3841  data: 0.0010  max mem: 65949
Evaluation  [ 140/1093]  eta: 0:23:41    time: 1.3398  data: 0.0009  max mem: 65949
Evaluation  [ 150/1093]  eta: 0:23:17    time: 1.2571  data: 0.0009  max mem: 65949
Evaluation  [ 160/1093]  eta: 0:22:58    time: 1.3734  data: 0.0009  max mem: 65949
Evaluation  [ 170/1093]  eta: 0:22:30    time: 1.3198  data: 0.0010  max mem: 65949
Evaluation  [ 180/1093]  eta: 0:22:12    time: 1.3196  data: 0.0010  max mem: 65949
Evaluation  [ 190/1093]  eta: 0:21:56    time: 1.4110  data: 0.0009  max mem: 65949
Evaluation  [ 200/1093]  eta: 0:21:32    time: 1.3338  data: 0.0009  max mem: 65949
Evaluation  [ 210/1093]  eta: 0:21:24    time: 1.4279  data: 0.0010  max mem: 65949
Evaluation  [ 220/1093]  eta: 0:20:58    time: 1.3806  data: 0.0010  max mem: 65949
Evaluation  [ 230/1093]  eta: 0:20:37    time: 1.2113  data: 0.0009  max mem: 65949
Evaluation  [ 240/1093]  eta: 0:20:20    time: 1.3095  data: 0.0009  max mem: 65949
Evaluation  [ 250/1093]  eta: 0:20:04    time: 1.3786  data: 0.0009  max mem: 65949
Evaluation  [ 260/1093]  eta: 0:19:58    time: 1.5352  data: 0.0009  max mem: 65949
Evaluation  [ 270/1093]  eta: 0:19:36    time: 1.4275  data: 0.0010  max mem: 65949
Evaluation  [ 280/1093]  eta: 0:19:17    time: 1.2300  data: 0.0010  max mem: 65949
Evaluation  [ 290/1093]  eta: 0:19:09    time: 1.4700  data: 0.0011  max mem: 65949
Evaluation  [ 300/1093]  eta: 0:18:48    time: 1.4201  data: 0.0010  max mem: 65949
Evaluation  [ 310/1093]  eta: 0:18:30    time: 1.2253  data: 0.0010  max mem: 65949
Evaluation  [ 320/1093]  eta: 0:18:15    time: 1.3311  data: 0.0010  max mem: 65949
Evaluation  [ 330/1093]  eta: 0:17:58    time: 1.3392  data: 0.0010  max mem: 65949
Evaluation  [ 340/1093]  eta: 0:17:46    time: 1.3992  data: 0.0010  max mem: 65949
Evaluation  [ 350/1093]  eta: 0:17:32    time: 1.4522  data: 0.0010  max mem: 65949
Evaluation  [ 360/1093]  eta: 0:17:13    time: 1.3080  data: 0.0010  max mem: 65949
Evaluation  [ 370/1093]  eta: 0:16:57    time: 1.2515  data: 0.0009  max mem: 65949
Evaluation  [ 380/1093]  eta: 0:16:46    time: 1.4389  data: 0.0009  max mem: 65949
Evaluation  [ 390/1093]  eta: 0:16:31    time: 1.4817  data: 0.0010  max mem: 65949
Evaluation  [ 400/1093]  eta: 0:16:19    time: 1.4314  data: 0.0010  max mem: 65949
Evaluation  [ 410/1093]  eta: 0:16:04    time: 1.4342  data: 0.0010  max mem: 65949
Evaluation  [ 420/1093]  eta: 0:15:51    time: 1.4326  data: 0.0009  max mem: 65949
Evaluation  [ 430/1093]  eta: 0:15:37    time: 1.4544  data: 0.0009  max mem: 65949
Evaluation  [ 440/1093]  eta: 0:15:18    time: 1.2708  data: 0.0009  max mem: 65949
Evaluation  [ 450/1093]  eta: 0:15:03    time: 1.2308  data: 0.0010  max mem: 65949
Evaluation  [ 460/1093]  eta: 0:14:49    time: 1.3685  data: 0.0009  max mem: 65949
Evaluation  [ 470/1093]  eta: 0:14:33    time: 1.3215  data: 0.0009  max mem: 65949
Evaluation  [ 480/1093]  eta: 0:14:19    time: 1.3362  data: 0.0009  max mem: 65949
Evaluation  [ 490/1093]  eta: 0:14:07    time: 1.4799  data: 0.0009  max mem: 65949
Evaluation  [ 500/1093]  eta: 0:13:55    time: 1.5631  data: 0.0009  max mem: 65949
Evaluation  [ 510/1093]  eta: 0:13:42    time: 1.5420  data: 0.0009  max mem: 65949
Evaluation  [ 520/1093]  eta: 0:13:28    time: 1.4505  data: 0.0010  max mem: 65949
Evaluation  [ 530/1093]  eta: 0:13:15    time: 1.4484  data: 0.0010  max mem: 65949
Evaluation  [ 540/1093]  eta: 0:13:01    time: 1.4547  data: 0.0010  max mem: 65949
Evaluation  [ 550/1093]  eta: 0:12:47    time: 1.4362  data: 0.0010  max mem: 65949
Evaluation  [ 560/1093]  eta: 0:12:30    time: 1.2702  data: 0.0010  max mem: 65949
Evaluation  [ 570/1093]  eta: 0:12:15    time: 1.2154  data: 0.0009  max mem: 65949
Evaluation  [ 580/1093]  eta: 0:12:00    time: 1.3508  data: 0.0010  max mem: 65949
Evaluation  [ 590/1093]  eta: 0:11:44    time: 1.2236  data: 0.0010  max mem: 65949
Evaluation  [ 600/1093]  eta: 0:11:31    time: 1.3065  data: 0.0010  max mem: 65949
Evaluation  [ 610/1093]  eta: 0:11:17    time: 1.4780  data: 0.0010  max mem: 65949
Evaluation  [ 620/1093]  eta: 0:11:03    time: 1.4015  data: 0.0010  max mem: 65949
Evaluation  [ 630/1093]  eta: 0:10:49    time: 1.4054  data: 0.0009  max mem: 65949
Evaluation  [ 640/1093]  eta: 0:10:34    time: 1.3653  data: 0.0010  max mem: 65949
Evaluation  [ 650/1093]  eta: 0:10:20    time: 1.3683  data: 0.0010  max mem: 65949
Evaluation  [ 660/1093]  eta: 0:10:05    time: 1.3448  data: 0.0009  max mem: 65949
Evaluation  [ 670/1093]  eta: 0:09:52    time: 1.3390  data: 0.0009  max mem: 65949
Evaluation  [ 680/1093]  eta: 0:09:38    time: 1.4207  data: 0.0009  max mem: 65949
Evaluation  [ 690/1093]  eta: 0:09:24    time: 1.4624  data: 0.0010  max mem: 65949
Evaluation  [ 700/1093]  eta: 0:09:10    time: 1.4387  data: 0.0010  max mem: 65949
Evaluation  [ 710/1093]  eta: 0:08:55    time: 1.2513  data: 0.0010  max mem: 65949
Evaluation  [ 720/1093]  eta: 0:08:39    time: 1.1381  data: 0.0010  max mem: 65949
Evaluation  [ 730/1093]  eta: 0:08:24    time: 1.0999  data: 0.0009  max mem: 65949
Evaluation  [ 740/1093]  eta: 0:08:09    time: 1.1308  data: 0.0009  max mem: 65949
Evaluation  [ 750/1093]  eta: 0:07:55    time: 1.2816  data: 0.0009  max mem: 65949
Evaluation  [ 760/1093]  eta: 0:07:41    time: 1.3710  data: 0.0009  max mem: 65949
Evaluation  [ 770/1093]  eta: 0:07:27    time: 1.3989  data: 0.0009  max mem: 65949
Evaluation  [ 780/1093]  eta: 0:07:14    time: 1.4726  data: 0.0010  max mem: 65949
Evaluation  [ 790/1093]  eta: 0:06:59    time: 1.3054  data: 0.0009  max mem: 65949
Evaluation  [ 800/1093]  eta: 0:06:46    time: 1.3416  data: 0.0009  max mem: 65949
Evaluation  [ 810/1093]  eta: 0:06:32    time: 1.5390  data: 0.0010  max mem: 65949
Evaluation  [ 820/1093]  eta: 0:06:19    time: 1.4713  data: 0.0010  max mem: 65949
Evaluation  [ 830/1093]  eta: 0:06:06    time: 1.5256  data: 0.0010  max mem: 65949
Evaluation  [ 840/1093]  eta: 0:05:51    time: 1.4271  data: 0.0010  max mem: 65949
Evaluation  [ 850/1093]  eta: 0:05:37    time: 1.3417  data: 0.0010  max mem: 65949
Evaluation  [ 860/1093]  eta: 0:05:23    time: 1.3402  data: 0.0010  max mem: 65949
Evaluation  [ 870/1093]  eta: 0:05:09    time: 1.3414  data: 0.0009  max mem: 65949
Evaluation  [ 880/1093]  eta: 0:04:56    time: 1.5039  data: 0.0009  max mem: 65949
Evaluation  [ 890/1093]  eta: 0:04:41    time: 1.3488  data: 0.0010  max mem: 65949
Evaluation  [ 900/1093]  eta: 0:04:28    time: 1.2680  data: 0.0010  max mem: 65949
Evaluation  [ 910/1093]  eta: 0:04:14    time: 1.4285  data: 0.0011  max mem: 65949
Evaluation  [ 920/1093]  eta: 0:04:00    time: 1.4468  data: 0.0012  max mem: 65949
Evaluation  [ 930/1093]  eta: 0:03:46    time: 1.4076  data: 0.0011  max mem: 65949
Evaluation  [ 940/1093]  eta: 0:03:32    time: 1.3150  data: 0.0010  max mem: 65949
Evaluation  [ 950/1093]  eta: 0:03:18    time: 1.2732  data: 0.0010  max mem: 65949
Evaluation  [ 960/1093]  eta: 0:03:04    time: 1.3705  data: 0.0011  max mem: 65949
Evaluation  [ 970/1093]  eta: 0:02:50    time: 1.4371  data: 0.0011  max mem: 65949
Evaluation  [ 980/1093]  eta: 0:02:36    time: 1.3163  data: 0.0010  max mem: 65949
Evaluation  [ 990/1093]  eta: 0:02:22    time: 1.3376  data: 0.0010  max mem: 65949
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4624  data: 0.0010  max mem: 65949
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4414  data: 0.0010  max mem: 65949
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.3292  data: 0.0010  max mem: 65949
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1860  data: 0.0010  max mem: 65949
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2260  data: 0.0009  max mem: 65949
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3044  data: 0.0009  max mem: 65949
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3589  data: 0.0009  max mem: 65949
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4339  data: 0.0010  max mem: 65949
Evaluation  [1080/1093]  eta: 0:00:17    time: 1.3678  data: 0.0010  max mem: 65949
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3263  data: 0.0010  max mem: 65949
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2889  data: 0.0416  max mem: 65949
Evaluation Total time: 0:25:11 (1.3828 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_6_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [7]  [   0/7110]  eta: 2 days, 5:58:10  lr: 0.000010  loss: 0.3410  time: 27.3264  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [  50/7110]  eta: 2:52:58  lr: 0.000010  loss: 0.3580  time: 0.9582  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 100/7110]  eta: 2:23:57  lr: 0.000010  loss: 0.1552  time: 0.9785  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 150/7110]  eta: 2:14:06  lr: 0.000010  loss: 0.0904  time: 0.9655  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 200/7110]  eta: 2:09:25  lr: 0.000010  loss: 0.0752  time: 1.0054  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 250/7110]  eta: 2:06:01  lr: 0.000010  loss: 0.1222  time: 0.9858  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 300/7110]  eta: 2:04:03  lr: 0.000010  loss: 0.6408  time: 1.0339  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 350/7110]  eta: 2:01:56  lr: 0.000010  loss: 0.3917  time: 1.0095  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 400/7110]  eta: 1:59:56  lr: 0.000010  loss: 0.3724  time: 0.9713  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 450/7110]  eta: 1:58:46  lr: 0.000010  loss: 0.2017  time: 1.0726  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 500/7110]  eta: 1:56:52  lr: 0.000010  loss: 0.2773  time: 0.9651  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 550/7110]  eta: 1:55:27  lr: 0.000010  loss: 0.2711  time: 1.0584  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 600/7110]  eta: 1:53:49  lr: 0.000010  loss: 1.7943  time: 0.9875  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 650/7110]  eta: 1:52:20  lr: 0.000010  loss: 0.7381  time: 0.9912  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 700/7110]  eta: 1:51:02  lr: 0.000010  loss: 0.9293  time: 0.9999  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 750/7110]  eta: 1:50:09  lr: 0.000010  loss: 0.8148  time: 0.9560  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 800/7110]  eta: 1:49:10  lr: 0.000010  loss: 0.6472  time: 1.0296  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 850/7110]  eta: 1:47:58  lr: 0.000010  loss: 0.8269  time: 0.9592  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 900/7110]  eta: 1:46:47  lr: 0.000010  loss: 0.4933  time: 0.9931  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [ 950/7110]  eta: 1:45:30  lr: 0.000010  loss: 0.0936  time: 0.9138  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1000/7110]  eta: 1:44:18  lr: 0.000010  loss: 0.1521  time: 0.9723  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1050/7110]  eta: 1:43:15  lr: 0.000010  loss: 0.2189  time: 0.9767  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1100/7110]  eta: 1:42:18  lr: 0.000010  loss: 0.2585  time: 0.9804  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1150/7110]  eta: 1:41:28  lr: 0.000010  loss: 0.2916  time: 1.0467  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1200/7110]  eta: 1:40:24  lr: 0.000010  loss: 0.5775  time: 0.9281  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1250/7110]  eta: 1:39:28  lr: 0.000010  loss: 0.2101  time: 1.0025  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1300/7110]  eta: 1:38:35  lr: 0.000010  loss: 0.3056  time: 0.9677  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1350/7110]  eta: 1:37:43  lr: 0.000010  loss: 0.1542  time: 0.9680  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1400/7110]  eta: 1:36:54  lr: 0.000010  loss: 0.4235  time: 1.0363  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1450/7110]  eta: 1:36:03  lr: 0.000010  loss: 0.4297  time: 1.0272  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1500/7110]  eta: 1:35:10  lr: 0.000010  loss: 0.0635  time: 1.0333  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1550/7110]  eta: 1:34:16  lr: 0.000010  loss: 0.4362  time: 0.9900  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1600/7110]  eta: 1:33:17  lr: 0.000010  loss: 0.1833  time: 0.9875  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1650/7110]  eta: 1:32:24  lr: 0.000010  loss: 0.1458  time: 0.9851  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1700/7110]  eta: 1:31:26  lr: 0.000010  loss: 0.1225  time: 0.9748  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1750/7110]  eta: 1:30:30  lr: 0.000010  loss: 0.3347  time: 0.9462  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1800/7110]  eta: 1:29:42  lr: 0.000010  loss: 0.5285  time: 0.9975  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1850/7110]  eta: 1:28:38  lr: 0.000010  loss: 0.3679  time: 0.8992  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1900/7110]  eta: 1:27:46  lr: 0.000010  loss: 0.0971  time: 1.0256  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [1950/7110]  eta: 1:27:02  lr: 0.000010  loss: 0.4501  time: 1.0054  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2000/7110]  eta: 1:26:05  lr: 0.000010  loss: 0.1672  time: 0.9873  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2050/7110]  eta: 1:25:16  lr: 0.000010  loss: 0.4775  time: 1.0189  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2100/7110]  eta: 1:24:27  lr: 0.000010  loss: 0.7253  time: 1.0046  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2150/7110]  eta: 1:23:32  lr: 0.000010  loss: 0.1383  time: 0.9899  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2200/7110]  eta: 1:22:40  lr: 0.000010  loss: 0.3828  time: 0.9833  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2250/7110]  eta: 1:21:47  lr: 0.000010  loss: 0.3000  time: 1.0003  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2300/7110]  eta: 1:20:53  lr: 0.000010  loss: 0.2863  time: 0.9765  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2350/7110]  eta: 1:20:02  lr: 0.000010  loss: 0.1137  time: 1.0194  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2400/7110]  eta: 1:19:08  lr: 0.000010  loss: 0.7812  time: 0.9817  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2450/7110]  eta: 1:18:14  lr: 0.000010  loss: 0.2089  time: 0.9439  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2500/7110]  eta: 1:17:24  lr: 0.000010  loss: 0.5803  time: 1.0042  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2550/7110]  eta: 1:16:34  lr: 0.000010  loss: 0.7080  time: 0.9702  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2600/7110]  eta: 1:15:45  lr: 0.000010  loss: 0.7256  time: 0.9773  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2650/7110]  eta: 1:14:52  lr: 0.000010  loss: 0.1118  time: 1.0363  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2700/7110]  eta: 1:13:59  lr: 0.000010  loss: 0.2389  time: 0.9968  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2750/7110]  eta: 1:13:09  lr: 0.000010  loss: 0.2849  time: 1.0236  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2800/7110]  eta: 1:12:19  lr: 0.000010  loss: 0.0845  time: 0.9962  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2850/7110]  eta: 1:11:30  lr: 0.000010  loss: 0.1490  time: 1.0134  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2900/7110]  eta: 1:10:41  lr: 0.000010  loss: 0.5842  time: 1.0245  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [2950/7110]  eta: 1:09:51  lr: 0.000010  loss: 0.8634  time: 1.0318  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3000/7110]  eta: 1:08:59  lr: 0.000010  loss: 0.1404  time: 0.9703  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3050/7110]  eta: 1:08:10  lr: 0.000010  loss: 0.4085  time: 1.0078  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3100/7110]  eta: 1:07:21  lr: 0.000010  loss: 0.7578  time: 0.9952  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3150/7110]  eta: 1:06:30  lr: 0.000010  loss: 0.5393  time: 0.9863  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3200/7110]  eta: 1:05:38  lr: 0.000010  loss: 0.4773  time: 0.9849  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3250/7110]  eta: 1:04:47  lr: 0.000010  loss: 0.1068  time: 1.0310  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3300/7110]  eta: 1:03:53  lr: 0.000010  loss: 0.3189  time: 0.9692  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3350/7110]  eta: 1:03:03  lr: 0.000010  loss: 1.7109  time: 1.0177  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3400/7110]  eta: 1:02:13  lr: 0.000010  loss: 0.1261  time: 0.9938  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3450/7110]  eta: 1:01:22  lr: 0.000010  loss: 0.1583  time: 1.0397  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3500/7110]  eta: 1:00:32  lr: 0.000010  loss: 0.1558  time: 1.0326  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3550/7110]  eta: 0:59:40  lr: 0.000010  loss: 0.4985  time: 0.9949  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3600/7110]  eta: 0:58:50  lr: 0.000010  loss: 0.7245  time: 1.0394  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3650/7110]  eta: 0:57:57  lr: 0.000010  loss: 0.5896  time: 0.9816  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3700/7110]  eta: 0:57:05  lr: 0.000010  loss: 0.0900  time: 0.9603  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3750/7110]  eta: 0:56:14  lr: 0.000010  loss: 0.2254  time: 0.9342  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3800/7110]  eta: 0:55:25  lr: 0.000010  loss: 0.2078  time: 0.9915  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3850/7110]  eta: 0:54:34  lr: 0.000010  loss: 0.3367  time: 1.0045  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3900/7110]  eta: 0:53:43  lr: 0.000010  loss: 0.4623  time: 0.9813  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [3950/7110]  eta: 0:52:53  lr: 0.000010  loss: 0.1784  time: 1.0072  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4000/7110]  eta: 0:52:03  lr: 0.000010  loss: 0.4286  time: 1.0202  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4050/7110]  eta: 0:51:12  lr: 0.000010  loss: 0.1345  time: 0.9764  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4100/7110]  eta: 0:50:21  lr: 0.000010  loss: 0.1488  time: 0.9821  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4150/7110]  eta: 0:49:31  lr: 0.000010  loss: 0.7760  time: 0.9972  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4200/7110]  eta: 0:48:41  lr: 0.000010  loss: 0.3803  time: 0.9642  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4250/7110]  eta: 0:47:50  lr: 0.000010  loss: 0.4444  time: 0.9941  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4300/7110]  eta: 0:47:01  lr: 0.000010  loss: 0.5400  time: 1.0379  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4350/7110]  eta: 0:46:11  lr: 0.000010  loss: 0.2002  time: 1.0115  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4400/7110]  eta: 0:45:20  lr: 0.000010  loss: 0.6515  time: 0.9491  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4450/7110]  eta: 0:44:29  lr: 0.000010  loss: 0.4089  time: 0.9957  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4500/7110]  eta: 0:43:38  lr: 0.000010  loss: 0.2723  time: 1.0186  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4550/7110]  eta: 0:42:48  lr: 0.000010  loss: 0.3752  time: 0.9676  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4600/7110]  eta: 0:41:57  lr: 0.000010  loss: 0.2963  time: 0.9508  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4650/7110]  eta: 0:41:07  lr: 0.000010  loss: 0.5511  time: 0.9989  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4700/7110]  eta: 0:40:17  lr: 0.000010  loss: 0.1229  time: 0.9600  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4750/7110]  eta: 0:39:26  lr: 0.000010  loss: 0.2866  time: 1.0226  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4800/7110]  eta: 0:38:35  lr: 0.000010  loss: 0.5947  time: 0.9611  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4850/7110]  eta: 0:37:45  lr: 0.000010  loss: 0.1014  time: 0.9936  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4900/7110]  eta: 0:36:55  lr: 0.000010  loss: 0.5235  time: 1.0297  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [4950/7110]  eta: 0:36:05  lr: 0.000010  loss: 0.3021  time: 1.0032  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5000/7110]  eta: 0:35:15  lr: 0.000010  loss: 0.1773  time: 1.0394  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5050/7110]  eta: 0:34:26  lr: 0.000010  loss: 0.1281  time: 1.0600  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5100/7110]  eta: 0:33:36  lr: 0.000010  loss: 0.1497  time: 0.9552  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5150/7110]  eta: 0:32:45  lr: 0.000010  loss: 0.1894  time: 0.9617  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5200/7110]  eta: 0:31:55  lr: 0.000010  loss: 0.1299  time: 1.0013  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5250/7110]  eta: 0:31:04  lr: 0.000010  loss: 0.0699  time: 0.9476  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5300/7110]  eta: 0:30:13  lr: 0.000010  loss: 0.7519  time: 0.9656  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5350/7110]  eta: 0:29:23  lr: 0.000010  loss: 0.2064  time: 0.9594  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5400/7110]  eta: 0:28:32  lr: 0.000010  loss: 0.3921  time: 0.9663  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5450/7110]  eta: 0:27:43  lr: 0.000010  loss: 0.1101  time: 1.0136  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5500/7110]  eta: 0:26:53  lr: 0.000010  loss: 0.2979  time: 1.0391  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5550/7110]  eta: 0:26:03  lr: 0.000010  loss: 0.2803  time: 1.0100  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5600/7110]  eta: 0:25:12  lr: 0.000010  loss: 0.3469  time: 0.9885  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5650/7110]  eta: 0:24:23  lr: 0.000010  loss: 0.0997  time: 1.0184  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5700/7110]  eta: 0:23:33  lr: 0.000010  loss: 0.0787  time: 0.9728  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5750/7110]  eta: 0:22:43  lr: 0.000010  loss: 1.5259  time: 1.0209  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5800/7110]  eta: 0:21:53  lr: 0.000010  loss: 0.1758  time: 0.9831  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.3221  time: 1.0509  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.3229  time: 1.0705  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [5950/7110]  eta: 0:19:22  lr: 0.000010  loss: 0.5108  time: 1.0351  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6000/7110]  eta: 0:18:32  lr: 0.000010  loss: 0.0605  time: 0.9740  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.5910  time: 0.9582  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.3199  time: 0.9791  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.2205  time: 1.0407  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.3105  time: 0.9985  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.6780  time: 1.0277  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.4347  time: 1.0335  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.2305  time: 1.0452  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.1796  time: 0.9280  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 1.3674  time: 1.0438  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.2417  time: 0.9536  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.7994  time: 1.0439  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.3500  time: 1.0178  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.2515  time: 1.0511  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.4467  time: 1.0284  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.3487  time: 0.9550  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.2369  time: 0.9819  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1488  time: 1.0155  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1307  time: 0.9661  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.4487  time: 0.9870  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 1.7275  time: 1.0571  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.2996  time: 1.0252  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.4268  time: 1.0562  data: 0.0000  max mem: 65949
Train: data epoch: [7]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0952  time: 1.1258  data: 0.0000  max mem: 65949
Train: data epoch: [7] Total time: 1:58:52 (1.0032 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:13:13    time: 20.4882  data: 19.2171  max mem: 65949
Evaluation  [  10/1093]  eta: 0:56:35    time: 3.1350  data: 1.7478  max mem: 65949
Evaluation  [  20/1093]  eta: 0:40:33    time: 1.3573  data: 0.0010  max mem: 65949
Evaluation  [  30/1093]  eta: 0:34:15    time: 1.2725  data: 0.0011  max mem: 65949
Evaluation  [  40/1093]  eta: 0:31:40    time: 1.3188  data: 0.0009  max mem: 65949
Evaluation  [  50/1093]  eta: 0:30:04    time: 1.4157  data: 0.0009  max mem: 65949
Evaluation  [  60/1093]  eta: 0:28:05    time: 1.2775  data: 0.0010  max mem: 65949
Evaluation  [  70/1093]  eta: 0:27:27    time: 1.3049  data: 0.0010  max mem: 65949
Evaluation  [  80/1093]  eta: 0:26:37    time: 1.4108  data: 0.0009  max mem: 65949
Evaluation  [  90/1093]  eta: 0:25:56    time: 1.3457  data: 0.0009  max mem: 65949
Evaluation  [ 100/1093]  eta: 0:25:33    time: 1.4086  data: 0.0009  max mem: 65949
Evaluation  [ 110/1093]  eta: 0:25:08    time: 1.4549  data: 0.0009  max mem: 65949
Evaluation  [ 120/1093]  eta: 0:24:36    time: 1.3808  data: 0.0009  max mem: 65949
Evaluation  [ 130/1093]  eta: 0:24:23    time: 1.4387  data: 0.0010  max mem: 65949
Evaluation  [ 140/1093]  eta: 0:23:46    time: 1.3776  data: 0.0010  max mem: 65949
Evaluation  [ 150/1093]  eta: 0:23:21    time: 1.2634  data: 0.0009  max mem: 65949
Evaluation  [ 160/1093]  eta: 0:23:00    time: 1.3585  data: 0.0010  max mem: 65949
Evaluation  [ 170/1093]  eta: 0:22:36    time: 1.3460  data: 0.0010  max mem: 65949
Evaluation  [ 180/1093]  eta: 0:22:17    time: 1.3431  data: 0.0010  max mem: 65949
Evaluation  [ 190/1093]  eta: 0:22:00    time: 1.4031  data: 0.0010  max mem: 65949
Evaluation  [ 200/1093]  eta: 0:21:36    time: 1.3379  data: 0.0010  max mem: 65949
Evaluation  [ 210/1093]  eta: 0:21:30    time: 1.4466  data: 0.0010  max mem: 65949
Evaluation  [ 220/1093]  eta: 0:21:04    time: 1.4098  data: 0.0010  max mem: 65949
Evaluation  [ 230/1093]  eta: 0:20:43    time: 1.2332  data: 0.0010  max mem: 65949
Evaluation  [ 240/1093]  eta: 0:20:26    time: 1.3267  data: 0.0010  max mem: 65949
Evaluation  [ 250/1093]  eta: 0:20:12    time: 1.4068  data: 0.0010  max mem: 65949
Evaluation  [ 260/1093]  eta: 0:20:05    time: 1.5580  data: 0.0010  max mem: 65949
Evaluation  [ 270/1093]  eta: 0:19:44    time: 1.4521  data: 0.0009  max mem: 65949
Evaluation  [ 280/1093]  eta: 0:19:26    time: 1.2628  data: 0.0009  max mem: 65949
Evaluation  [ 290/1093]  eta: 0:19:18    time: 1.4896  data: 0.0010  max mem: 65949
Evaluation  [ 300/1093]  eta: 0:18:56    time: 1.4194  data: 0.0010  max mem: 65949
Evaluation  [ 310/1093]  eta: 0:18:38    time: 1.2180  data: 0.0010  max mem: 65949
Evaluation  [ 320/1093]  eta: 0:18:23    time: 1.3328  data: 0.0009  max mem: 65949
Evaluation  [ 330/1093]  eta: 0:18:05    time: 1.3359  data: 0.0009  max mem: 65949
Evaluation  [ 340/1093]  eta: 0:17:53    time: 1.4049  data: 0.0010  max mem: 65949
Evaluation  [ 350/1093]  eta: 0:17:39    time: 1.4703  data: 0.0010  max mem: 65949
Evaluation  [ 360/1093]  eta: 0:17:19    time: 1.2952  data: 0.0010  max mem: 65949
Evaluation  [ 370/1093]  eta: 0:17:03    time: 1.2277  data: 0.0010  max mem: 65949
Evaluation  [ 380/1093]  eta: 0:16:51    time: 1.4201  data: 0.0010  max mem: 65949
Evaluation  [ 390/1093]  eta: 0:16:36    time: 1.4612  data: 0.0010  max mem: 65949
Evaluation  [ 400/1093]  eta: 0:16:23    time: 1.4303  data: 0.0010  max mem: 65949
Evaluation  [ 410/1093]  eta: 0:16:08    time: 1.4339  data: 0.0009  max mem: 65949
Evaluation  [ 420/1093]  eta: 0:15:55    time: 1.4283  data: 0.0010  max mem: 65949
Evaluation  [ 430/1093]  eta: 0:15:39    time: 1.4087  data: 0.0010  max mem: 65949
Evaluation  [ 440/1093]  eta: 0:15:21    time: 1.2471  data: 0.0010  max mem: 65949
Evaluation  [ 450/1093]  eta: 0:15:07    time: 1.2694  data: 0.0010  max mem: 65949
Evaluation  [ 460/1093]  eta: 0:14:52    time: 1.3540  data: 0.0010  max mem: 65949
Evaluation  [ 470/1093]  eta: 0:14:36    time: 1.2986  data: 0.0010  max mem: 65949
Evaluation  [ 480/1093]  eta: 0:14:22    time: 1.3554  data: 0.0010  max mem: 65949
Evaluation  [ 490/1093]  eta: 0:14:10    time: 1.4913  data: 0.0010  max mem: 65949
Evaluation  [ 500/1093]  eta: 0:13:58    time: 1.5815  data: 0.0010  max mem: 65949
Evaluation  [ 510/1093]  eta: 0:13:45    time: 1.5644  data: 0.0010  max mem: 65949
Evaluation  [ 520/1093]  eta: 0:13:31    time: 1.4593  data: 0.0010  max mem: 65949
Evaluation  [ 530/1093]  eta: 0:13:17    time: 1.4483  data: 0.0010  max mem: 65949
Evaluation  [ 540/1093]  eta: 0:13:03    time: 1.4553  data: 0.0009  max mem: 65949
Evaluation  [ 550/1093]  eta: 0:12:50    time: 1.4490  data: 0.0009  max mem: 65949
Evaluation  [ 560/1093]  eta: 0:12:32    time: 1.2492  data: 0.0009  max mem: 65949
Evaluation  [ 570/1093]  eta: 0:12:17    time: 1.1824  data: 0.0009  max mem: 65949
Evaluation  [ 580/1093]  eta: 0:12:03    time: 1.3605  data: 0.0009  max mem: 65949
Evaluation  [ 590/1093]  eta: 0:11:46    time: 1.2600  data: 0.0010  max mem: 65949
Evaluation  [ 600/1093]  eta: 0:11:33    time: 1.3417  data: 0.0009  max mem: 65949
Evaluation  [ 610/1093]  eta: 0:11:19    time: 1.4892  data: 0.0009  max mem: 65949
Evaluation  [ 620/1093]  eta: 0:11:05    time: 1.4106  data: 0.0010  max mem: 65949
Evaluation  [ 630/1093]  eta: 0:10:52    time: 1.4344  data: 0.0010  max mem: 65949
Evaluation  [ 640/1093]  eta: 0:10:37    time: 1.3882  data: 0.0010  max mem: 65949
Evaluation  [ 650/1093]  eta: 0:10:23    time: 1.3770  data: 0.0010  max mem: 65949
Evaluation  [ 660/1093]  eta: 0:10:08    time: 1.3613  data: 0.0009  max mem: 65949
Evaluation  [ 670/1093]  eta: 0:09:54    time: 1.3380  data: 0.0009  max mem: 65949
Evaluation  [ 680/1093]  eta: 0:09:40    time: 1.4165  data: 0.0009  max mem: 65949
Evaluation  [ 690/1093]  eta: 0:09:27    time: 1.4673  data: 0.0010  max mem: 65949
Evaluation  [ 700/1093]  eta: 0:09:13    time: 1.4895  data: 0.0010  max mem: 65949
Evaluation  [ 710/1093]  eta: 0:08:57    time: 1.3142  data: 0.0009  max mem: 65949
Evaluation  [ 720/1093]  eta: 0:08:42    time: 1.1628  data: 0.0009  max mem: 65949
Evaluation  [ 730/1093]  eta: 0:08:27    time: 1.1312  data: 0.0009  max mem: 65949
Evaluation  [ 740/1093]  eta: 0:08:12    time: 1.1262  data: 0.0009  max mem: 65949
Evaluation  [ 750/1093]  eta: 0:07:57    time: 1.2582  data: 0.0010  max mem: 65949
Evaluation  [ 760/1093]  eta: 0:07:43    time: 1.3690  data: 0.0010  max mem: 65949
Evaluation  [ 770/1093]  eta: 0:07:30    time: 1.4034  data: 0.0009  max mem: 65949
Evaluation  [ 780/1093]  eta: 0:07:16    time: 1.4811  data: 0.0009  max mem: 65949
Evaluation  [ 790/1093]  eta: 0:07:01    time: 1.3234  data: 0.0009  max mem: 65949
Evaluation  [ 800/1093]  eta: 0:06:48    time: 1.3970  data: 0.0009  max mem: 65949
Evaluation  [ 810/1093]  eta: 0:06:35    time: 1.5592  data: 0.0010  max mem: 65949
Evaluation  [ 820/1093]  eta: 0:06:21    time: 1.4615  data: 0.0010  max mem: 65949
Evaluation  [ 830/1093]  eta: 0:06:08    time: 1.5350  data: 0.0010  max mem: 65949
Evaluation  [ 840/1093]  eta: 0:05:53    time: 1.4234  data: 0.0010  max mem: 65949
Evaluation  [ 850/1093]  eta: 0:05:39    time: 1.3247  data: 0.0010  max mem: 65949
Evaluation  [ 860/1093]  eta: 0:05:25    time: 1.3310  data: 0.0010  max mem: 65949
Evaluation  [ 870/1093]  eta: 0:05:11    time: 1.3473  data: 0.0009  max mem: 65949
Evaluation  [ 880/1093]  eta: 0:04:58    time: 1.5250  data: 0.0010  max mem: 65949
Evaluation  [ 890/1093]  eta: 0:04:43    time: 1.3621  data: 0.0010  max mem: 65949
Evaluation  [ 900/1093]  eta: 0:04:29    time: 1.2702  data: 0.0009  max mem: 65949
Evaluation  [ 910/1093]  eta: 0:04:15    time: 1.4397  data: 0.0010  max mem: 65949
Evaluation  [ 920/1093]  eta: 0:04:01    time: 1.4665  data: 0.0010  max mem: 65949
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.4226  data: 0.0009  max mem: 65949
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3911  data: 0.0009  max mem: 65949
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.3769  data: 0.0009  max mem: 65949
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.4123  data: 0.0009  max mem: 65949
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4533  data: 0.0009  max mem: 65949
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.2965  data: 0.0009  max mem: 65949
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.2815  data: 0.0009  max mem: 65949
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4108  data: 0.0010  max mem: 65949
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4269  data: 0.0010  max mem: 65949
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.3018  data: 0.0010  max mem: 65949
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1756  data: 0.0010  max mem: 65949
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2578  data: 0.0009  max mem: 65949
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3434  data: 0.0009  max mem: 65949
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3704  data: 0.0009  max mem: 65949
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4224  data: 0.0009  max mem: 65949
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3744  data: 0.0009  max mem: 65949
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3382  data: 0.0010  max mem: 65949
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3096  data: 0.0393  max mem: 65949
Evaluation Total time: 0:25:20 (1.3908 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_7_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [8]  [   0/7110]  eta: 2 days, 5:10:28  lr: 0.000010  loss: 0.0422  time: 26.9239  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [  50/7110]  eta: 3:00:47  lr: 0.000010  loss: 0.6844  time: 1.0731  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 100/7110]  eta: 2:27:56  lr: 0.000010  loss: 0.8210  time: 0.9642  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 150/7110]  eta: 2:16:39  lr: 0.000010  loss: 0.6905  time: 0.9992  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 200/7110]  eta: 2:10:48  lr: 0.000010  loss: 0.1624  time: 1.0035  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 250/7110]  eta: 2:07:15  lr: 0.000010  loss: 0.7119  time: 1.0375  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 300/7110]  eta: 2:03:37  lr: 0.000010  loss: 0.4962  time: 0.9499  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 350/7110]  eta: 2:00:59  lr: 0.000010  loss: 0.5089  time: 1.0488  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 400/7110]  eta: 1:58:45  lr: 0.000010  loss: 0.3036  time: 0.9850  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 450/7110]  eta: 1:56:43  lr: 0.000010  loss: 0.4474  time: 1.0315  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 500/7110]  eta: 1:54:46  lr: 0.000010  loss: 0.0826  time: 0.9497  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 550/7110]  eta: 1:53:01  lr: 0.000010  loss: 0.0747  time: 0.9769  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 600/7110]  eta: 1:52:24  lr: 0.000010  loss: 0.1702  time: 1.0350  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 650/7110]  eta: 1:51:04  lr: 0.000010  loss: 0.1868  time: 0.9815  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 700/7110]  eta: 1:49:50  lr: 0.000010  loss: 0.3202  time: 0.9835  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 750/7110]  eta: 1:48:53  lr: 0.000010  loss: 0.2504  time: 0.9501  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 800/7110]  eta: 1:47:55  lr: 0.000010  loss: 0.5202  time: 0.9876  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 850/7110]  eta: 1:46:56  lr: 0.000010  loss: 0.3604  time: 1.0059  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 900/7110]  eta: 1:45:52  lr: 0.000010  loss: 0.0212  time: 1.0024  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [ 950/7110]  eta: 1:44:48  lr: 0.000010  loss: 0.0978  time: 0.9838  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1000/7110]  eta: 1:43:46  lr: 0.000010  loss: 0.0862  time: 0.9924  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1050/7110]  eta: 1:42:46  lr: 0.000010  loss: 0.6066  time: 0.9727  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1100/7110]  eta: 1:42:05  lr: 0.000010  loss: 0.1148  time: 1.0717  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1150/7110]  eta: 1:41:08  lr: 0.000010  loss: 0.2491  time: 0.9818  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1200/7110]  eta: 1:40:18  lr: 0.000010  loss: 0.2860  time: 0.9856  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1250/7110]  eta: 1:39:29  lr: 0.000010  loss: 0.8052  time: 1.0224  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1300/7110]  eta: 1:38:32  lr: 0.000010  loss: 0.4119  time: 0.9643  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1350/7110]  eta: 1:37:38  lr: 0.000010  loss: 0.2811  time: 0.9887  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1400/7110]  eta: 1:36:43  lr: 0.000010  loss: 0.2578  time: 1.0142  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1450/7110]  eta: 1:35:50  lr: 0.000010  loss: 1.3808  time: 1.0230  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1500/7110]  eta: 1:34:51  lr: 0.000010  loss: 0.3224  time: 0.9588  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1550/7110]  eta: 1:34:01  lr: 0.000010  loss: 0.0683  time: 1.0086  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1600/7110]  eta: 1:33:11  lr: 0.000010  loss: 0.9898  time: 0.9772  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1650/7110]  eta: 1:32:14  lr: 0.000010  loss: 1.5544  time: 1.0068  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1700/7110]  eta: 1:31:18  lr: 0.000010  loss: 0.2971  time: 1.0180  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1750/7110]  eta: 1:30:33  lr: 0.000010  loss: 0.3873  time: 0.9888  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1800/7110]  eta: 1:29:43  lr: 0.000010  loss: 0.5381  time: 1.0418  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1850/7110]  eta: 1:28:53  lr: 0.000010  loss: 0.2612  time: 1.0513  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1900/7110]  eta: 1:27:59  lr: 0.000010  loss: 0.2255  time: 1.0033  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [1950/7110]  eta: 1:27:07  lr: 0.000010  loss: 0.5832  time: 0.9882  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2000/7110]  eta: 1:26:22  lr: 0.000010  loss: 0.4819  time: 1.0527  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2050/7110]  eta: 1:25:28  lr: 0.000010  loss: 0.1033  time: 0.9734  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2100/7110]  eta: 1:24:32  lr: 0.000010  loss: 0.1733  time: 0.9580  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2150/7110]  eta: 1:23:40  lr: 0.000010  loss: 0.1047  time: 1.0085  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2200/7110]  eta: 1:22:48  lr: 0.000010  loss: 0.4714  time: 1.0010  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2250/7110]  eta: 1:21:54  lr: 0.000010  loss: 0.1778  time: 0.9215  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2300/7110]  eta: 1:21:02  lr: 0.000010  loss: 0.1779  time: 1.0216  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2350/7110]  eta: 1:20:07  lr: 0.000010  loss: 0.1765  time: 0.9434  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2400/7110]  eta: 1:19:15  lr: 0.000010  loss: 0.0797  time: 1.0026  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2450/7110]  eta: 1:18:23  lr: 0.000010  loss: 0.0855  time: 1.0196  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2500/7110]  eta: 1:17:31  lr: 0.000010  loss: 0.3402  time: 1.0464  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2550/7110]  eta: 1:16:45  lr: 0.000010  loss: 0.1905  time: 1.0991  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2600/7110]  eta: 1:15:55  lr: 0.000010  loss: 0.3469  time: 0.9567  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2650/7110]  eta: 1:15:00  lr: 0.000010  loss: 0.3725  time: 0.9275  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2700/7110]  eta: 1:14:05  lr: 0.000010  loss: 0.3504  time: 0.9028  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2750/7110]  eta: 1:13:12  lr: 0.000010  loss: 0.3768  time: 0.9629  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2800/7110]  eta: 1:12:21  lr: 0.000010  loss: 0.1223  time: 0.9798  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2850/7110]  eta: 1:11:27  lr: 0.000010  loss: 0.5950  time: 0.9293  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2900/7110]  eta: 1:10:35  lr: 0.000010  loss: 0.0439  time: 0.9972  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [2950/7110]  eta: 1:09:45  lr: 0.000010  loss: 0.2608  time: 0.9839  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3000/7110]  eta: 1:08:54  lr: 0.000010  loss: 0.1036  time: 0.9792  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3050/7110]  eta: 1:08:04  lr: 0.000010  loss: 0.1386  time: 1.0102  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3100/7110]  eta: 1:07:14  lr: 0.000010  loss: 0.1277  time: 1.0452  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3150/7110]  eta: 1:06:21  lr: 0.000010  loss: 0.3566  time: 0.9933  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3200/7110]  eta: 1:05:30  lr: 0.000010  loss: 0.6000  time: 1.0032  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3250/7110]  eta: 1:04:39  lr: 0.000010  loss: 0.3260  time: 0.9843  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3300/7110]  eta: 1:03:50  lr: 0.000010  loss: 0.2512  time: 1.0449  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3350/7110]  eta: 1:02:57  lr: 0.000010  loss: 0.1985  time: 0.9390  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3400/7110]  eta: 1:02:06  lr: 0.000010  loss: 0.3551  time: 0.9961  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3450/7110]  eta: 1:01:17  lr: 0.000010  loss: 0.1291  time: 1.0808  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3500/7110]  eta: 1:00:28  lr: 0.000010  loss: 0.4864  time: 1.0090  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3550/7110]  eta: 0:59:37  lr: 0.000010  loss: 0.0833  time: 0.9497  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3600/7110]  eta: 0:58:47  lr: 0.000010  loss: 0.1236  time: 0.9843  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3650/7110]  eta: 0:57:55  lr: 0.000010  loss: 0.1627  time: 0.9703  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3700/7110]  eta: 0:57:05  lr: 0.000010  loss: 0.6036  time: 1.0603  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3750/7110]  eta: 0:56:14  lr: 0.000010  loss: 0.0963  time: 0.9212  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3800/7110]  eta: 0:55:23  lr: 0.000010  loss: 0.1398  time: 1.0464  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3850/7110]  eta: 0:54:32  lr: 0.000010  loss: 0.2717  time: 1.0040  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3900/7110]  eta: 0:53:42  lr: 0.000010  loss: 0.1302  time: 0.9853  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [3950/7110]  eta: 0:52:51  lr: 0.000010  loss: 0.8136  time: 1.0320  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4000/7110]  eta: 0:52:02  lr: 0.000010  loss: 0.0922  time: 0.9906  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4050/7110]  eta: 0:51:12  lr: 0.000010  loss: 0.4161  time: 1.0163  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4100/7110]  eta: 0:50:20  lr: 0.000010  loss: 0.2782  time: 0.9682  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4150/7110]  eta: 0:49:31  lr: 0.000010  loss: 0.0713  time: 1.0156  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4200/7110]  eta: 0:48:41  lr: 0.000010  loss: 0.2498  time: 1.0091  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4250/7110]  eta: 0:47:51  lr: 0.000010  loss: 0.3977  time: 1.0589  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4300/7110]  eta: 0:47:01  lr: 0.000010  loss: 0.0524  time: 1.0373  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4350/7110]  eta: 0:46:11  lr: 0.000010  loss: 0.1789  time: 1.0639  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4400/7110]  eta: 0:45:20  lr: 0.000010  loss: 0.4915  time: 0.9963  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4450/7110]  eta: 0:44:29  lr: 0.000010  loss: 0.4959  time: 0.9689  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4500/7110]  eta: 0:43:38  lr: 0.000010  loss: 0.1673  time: 0.9653  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4550/7110]  eta: 0:42:49  lr: 0.000010  loss: 0.5146  time: 0.9729  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4600/7110]  eta: 0:41:58  lr: 0.000010  loss: 0.3981  time: 0.9737  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4650/7110]  eta: 0:41:07  lr: 0.000010  loss: 0.3070  time: 0.9924  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4700/7110]  eta: 0:40:17  lr: 0.000010  loss: 0.0566  time: 1.0063  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4750/7110]  eta: 0:39:25  lr: 0.000010  loss: 0.2121  time: 0.9568  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4800/7110]  eta: 0:38:36  lr: 0.000010  loss: 0.2050  time: 1.0085  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4850/7110]  eta: 0:37:46  lr: 0.000010  loss: 0.1425  time: 0.9797  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4900/7110]  eta: 0:36:56  lr: 0.000010  loss: 0.1867  time: 1.0238  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [4950/7110]  eta: 0:36:07  lr: 0.000010  loss: 0.3206  time: 1.0570  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5000/7110]  eta: 0:35:16  lr: 0.000010  loss: 0.2144  time: 0.9379  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5050/7110]  eta: 0:34:26  lr: 0.000010  loss: 0.4784  time: 0.9960  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5100/7110]  eta: 0:33:36  lr: 0.000010  loss: 0.5117  time: 1.0073  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5150/7110]  eta: 0:32:46  lr: 0.000010  loss: 0.1310  time: 0.9867  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5200/7110]  eta: 0:31:56  lr: 0.000010  loss: 0.8769  time: 0.9982  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5250/7110]  eta: 0:31:06  lr: 0.000010  loss: 0.3924  time: 0.9896  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5300/7110]  eta: 0:30:15  lr: 0.000010  loss: 0.1970  time: 0.9616  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5350/7110]  eta: 0:29:25  lr: 0.000010  loss: 0.0714  time: 0.9658  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5400/7110]  eta: 0:28:35  lr: 0.000010  loss: 0.0381  time: 1.0170  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5450/7110]  eta: 0:27:44  lr: 0.000010  loss: 0.3137  time: 0.9802  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5500/7110]  eta: 0:26:54  lr: 0.000010  loss: 0.5582  time: 1.0182  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5550/7110]  eta: 0:26:04  lr: 0.000010  loss: 0.3448  time: 1.0317  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5600/7110]  eta: 0:25:14  lr: 0.000010  loss: 0.7069  time: 1.0049  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5650/7110]  eta: 0:24:23  lr: 0.000010  loss: 0.6243  time: 0.9951  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5700/7110]  eta: 0:23:33  lr: 0.000010  loss: 0.5229  time: 1.0234  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.1153  time: 1.0249  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.2906  time: 1.0089  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 0.1518  time: 1.0406  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.3178  time: 1.0007  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [5950/7110]  eta: 0:19:24  lr: 0.000010  loss: 0.3795  time: 1.0012  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.1028  time: 0.9597  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.0895  time: 0.9952  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 1.4219  time: 0.9862  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.0133  time: 0.9424  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.0506  time: 0.9763  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 1.4011  time: 1.0047  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.2354  time: 1.0003  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.1328  time: 1.0623  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.0902  time: 1.0246  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.3525  time: 1.0438  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.1167  time: 0.9867  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.0997  time: 1.0497  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.3045  time: 0.9795  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1280  time: 1.0255  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.1076  time: 0.9600  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.3374  time: 0.9734  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.1813  time: 0.9798  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.6003  time: 0.9681  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.5912  time: 1.0072  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.3610  time: 0.9678  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.1044  time: 0.9650  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.0949  time: 1.0449  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.2893  time: 0.9663  data: 0.0000  max mem: 65949
Train: data epoch: [8]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.1489  time: 1.1301  data: 0.0000  max mem: 65949
Train: data epoch: [8] Total time: 1:58:56 (1.0037 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:19:31    time: 20.8336  data: 19.3930  max mem: 65949
Evaluation  [  10/1093]  eta: 0:57:08    time: 3.1655  data: 1.7638  max mem: 65949
Evaluation  [  20/1093]  eta: 0:41:01    time: 1.3668  data: 0.0009  max mem: 65949
Evaluation  [  30/1093]  eta: 0:34:27    time: 1.2732  data: 0.0009  max mem: 65949
Evaluation  [  40/1093]  eta: 0:31:41    time: 1.2924  data: 0.0010  max mem: 65949
Evaluation  [  50/1093]  eta: 0:29:55    time: 1.3760  data: 0.0010  max mem: 65949
Evaluation  [  60/1093]  eta: 0:27:50    time: 1.2326  data: 0.0009  max mem: 65949
Evaluation  [  70/1093]  eta: 0:27:12    time: 1.2754  data: 0.0009  max mem: 65949
Evaluation  [  80/1093]  eta: 0:26:24    time: 1.4009  data: 0.0010  max mem: 65949
Evaluation  [  90/1093]  eta: 0:25:47    time: 1.3542  data: 0.0010  max mem: 65949
Evaluation  [ 100/1093]  eta: 0:25:27    time: 1.4360  data: 0.0010  max mem: 65949
Evaluation  [ 110/1093]  eta: 0:25:05    time: 1.4808  data: 0.0010  max mem: 65949
Evaluation  [ 120/1093]  eta: 0:24:32    time: 1.3843  data: 0.0010  max mem: 65949
Evaluation  [ 130/1093]  eta: 0:24:17    time: 1.4138  data: 0.0010  max mem: 65949
Evaluation  [ 140/1093]  eta: 0:23:41    time: 1.3633  data: 0.0010  max mem: 65949
Evaluation  [ 150/1093]  eta: 0:23:17    time: 1.2717  data: 0.0009  max mem: 65949
Evaluation  [ 160/1093]  eta: 0:22:57    time: 1.3691  data: 0.0009  max mem: 65949
Evaluation  [ 170/1093]  eta: 0:22:32    time: 1.3436  data: 0.0009  max mem: 65949
Evaluation  [ 180/1093]  eta: 0:22:16    time: 1.3560  data: 0.0009  max mem: 65949
Evaluation  [ 190/1093]  eta: 0:21:59    time: 1.4303  data: 0.0009  max mem: 65949
Evaluation  [ 200/1093]  eta: 0:21:36    time: 1.3467  data: 0.0010  max mem: 65949
Evaluation  [ 210/1093]  eta: 0:21:29    time: 1.4527  data: 0.0010  max mem: 65949
Evaluation  [ 220/1093]  eta: 0:21:05    time: 1.4306  data: 0.0009  max mem: 65949
Evaluation  [ 230/1093]  eta: 0:20:44    time: 1.2432  data: 0.0010  max mem: 65949
Evaluation  [ 240/1093]  eta: 0:20:27    time: 1.3207  data: 0.0009  max mem: 65949
Evaluation  [ 250/1093]  eta: 0:20:12    time: 1.3903  data: 0.0009  max mem: 65949
Evaluation  [ 260/1093]  eta: 0:20:05    time: 1.5397  data: 0.0009  max mem: 65949
Evaluation  [ 270/1093]  eta: 0:19:43    time: 1.4428  data: 0.0009  max mem: 65949
Evaluation  [ 280/1093]  eta: 0:19:25    time: 1.2684  data: 0.0009  max mem: 65949
Evaluation  [ 290/1093]  eta: 0:19:18    time: 1.5037  data: 0.0009  max mem: 65949
Evaluation  [ 300/1093]  eta: 0:18:55    time: 1.4080  data: 0.0009  max mem: 65949
Evaluation  [ 310/1093]  eta: 0:18:38    time: 1.2105  data: 0.0009  max mem: 65949
Evaluation  [ 320/1093]  eta: 0:18:22    time: 1.3254  data: 0.0009  max mem: 65949
Evaluation  [ 330/1093]  eta: 0:18:04    time: 1.3264  data: 0.0009  max mem: 65949
Evaluation  [ 340/1093]  eta: 0:17:52    time: 1.4054  data: 0.0010  max mem: 65949
Evaluation  [ 350/1093]  eta: 0:17:38    time: 1.4666  data: 0.0010  max mem: 65949
Evaluation  [ 360/1093]  eta: 0:17:18    time: 1.2922  data: 0.0010  max mem: 65949
Evaluation  [ 370/1093]  eta: 0:17:02    time: 1.2318  data: 0.0010  max mem: 65949
Evaluation  [ 380/1093]  eta: 0:16:51    time: 1.4421  data: 0.0010  max mem: 65949
Evaluation  [ 390/1093]  eta: 0:16:37    time: 1.5141  data: 0.0009  max mem: 65949
Evaluation  [ 400/1093]  eta: 0:16:24    time: 1.4644  data: 0.0009  max mem: 65949
Evaluation  [ 410/1093]  eta: 0:16:07    time: 1.3734  data: 0.0009  max mem: 65949
Evaluation  [ 420/1093]  eta: 0:15:54    time: 1.3730  data: 0.0009  max mem: 65949
Evaluation  [ 430/1093]  eta: 0:15:39    time: 1.4177  data: 0.0009  max mem: 65949
Evaluation  [ 440/1093]  eta: 0:15:21    time: 1.2592  data: 0.0010  max mem: 65949
Evaluation  [ 450/1093]  eta: 0:15:07    time: 1.2833  data: 0.0009  max mem: 65949
Evaluation  [ 460/1093]  eta: 0:14:52    time: 1.3667  data: 0.0009  max mem: 65949
Evaluation  [ 470/1093]  eta: 0:14:36    time: 1.3320  data: 0.0010  max mem: 65949
Evaluation  [ 480/1093]  eta: 0:14:23    time: 1.3837  data: 0.0010  max mem: 65949
Evaluation  [ 490/1093]  eta: 0:14:10    time: 1.4908  data: 0.0010  max mem: 65949
Evaluation  [ 500/1093]  eta: 0:13:58    time: 1.5688  data: 0.0009  max mem: 65949
Evaluation  [ 510/1093]  eta: 0:13:46    time: 1.5724  data: 0.0009  max mem: 65949
Evaluation  [ 520/1093]  eta: 0:13:32    time: 1.4775  data: 0.0009  max mem: 65949
Evaluation  [ 530/1093]  eta: 0:13:18    time: 1.4478  data: 0.0009  max mem: 65949
Evaluation  [ 540/1093]  eta: 0:13:04    time: 1.4640  data: 0.0009  max mem: 65949
Evaluation  [ 550/1093]  eta: 0:12:50    time: 1.4556  data: 0.0009  max mem: 65949
Evaluation  [ 560/1093]  eta: 0:12:33    time: 1.2785  data: 0.0009  max mem: 65949
Evaluation  [ 570/1093]  eta: 0:12:18    time: 1.1994  data: 0.0009  max mem: 65949
Evaluation  [ 580/1093]  eta: 0:12:04    time: 1.3417  data: 0.0009  max mem: 65949
Evaluation  [ 590/1093]  eta: 0:11:47    time: 1.2543  data: 0.0009  max mem: 65949
Evaluation  [ 600/1093]  eta: 0:11:34    time: 1.3434  data: 0.0010  max mem: 65949
Evaluation  [ 610/1093]  eta: 0:11:20    time: 1.4895  data: 0.0010  max mem: 65949
Evaluation  [ 620/1093]  eta: 0:11:06    time: 1.4136  data: 0.0010  max mem: 65949
Evaluation  [ 630/1093]  eta: 0:10:52    time: 1.4306  data: 0.0010  max mem: 65949
Evaluation  [ 640/1093]  eta: 0:10:37    time: 1.3806  data: 0.0010  max mem: 65949
Evaluation  [ 650/1093]  eta: 0:10:24    time: 1.3871  data: 0.0010  max mem: 65949
Evaluation  [ 660/1093]  eta: 0:10:09    time: 1.3695  data: 0.0010  max mem: 65949
Evaluation  [ 670/1093]  eta: 0:09:55    time: 1.3513  data: 0.0010  max mem: 65949
Evaluation  [ 680/1093]  eta: 0:09:41    time: 1.4222  data: 0.0010  max mem: 65949
Evaluation  [ 690/1093]  eta: 0:09:27    time: 1.4604  data: 0.0010  max mem: 65949
Evaluation  [ 700/1093]  eta: 0:09:13    time: 1.4391  data: 0.0010  max mem: 65949
Evaluation  [ 710/1093]  eta: 0:08:58    time: 1.2641  data: 0.0010  max mem: 65949
Evaluation  [ 720/1093]  eta: 0:08:42    time: 1.1517  data: 0.0010  max mem: 65949
Evaluation  [ 730/1093]  eta: 0:08:27    time: 1.1035  data: 0.0010  max mem: 65949
Evaluation  [ 740/1093]  eta: 0:08:11    time: 1.1036  data: 0.0009  max mem: 65949
Evaluation  [ 750/1093]  eta: 0:07:57    time: 1.2423  data: 0.0009  max mem: 65949
Evaluation  [ 760/1093]  eta: 0:07:43    time: 1.3584  data: 0.0009  max mem: 65949
Evaluation  [ 770/1093]  eta: 0:07:29    time: 1.3880  data: 0.0009  max mem: 65949
Evaluation  [ 780/1093]  eta: 0:07:16    time: 1.4438  data: 0.0010  max mem: 65949
Evaluation  [ 790/1093]  eta: 0:07:01    time: 1.2890  data: 0.0010  max mem: 65949
Evaluation  [ 800/1093]  eta: 0:06:48    time: 1.3778  data: 0.0010  max mem: 65949
Evaluation  [ 810/1093]  eta: 0:06:34    time: 1.5489  data: 0.0010  max mem: 65949
Evaluation  [ 820/1093]  eta: 0:06:20    time: 1.4577  data: 0.0009  max mem: 65949
Evaluation  [ 830/1093]  eta: 0:06:07    time: 1.5486  data: 0.0010  max mem: 65949
Evaluation  [ 840/1093]  eta: 0:05:53    time: 1.4511  data: 0.0010  max mem: 65949
Evaluation  [ 850/1093]  eta: 0:05:39    time: 1.3519  data: 0.0009  max mem: 65949
Evaluation  [ 860/1093]  eta: 0:05:24    time: 1.3303  data: 0.0009  max mem: 65949
Evaluation  [ 870/1093]  eta: 0:05:11    time: 1.3287  data: 0.0010  max mem: 65949
Evaluation  [ 880/1093]  eta: 0:04:57    time: 1.4980  data: 0.0010  max mem: 65949
Evaluation  [ 890/1093]  eta: 0:04:42    time: 1.3424  data: 0.0009  max mem: 65949
Evaluation  [ 900/1093]  eta: 0:04:29    time: 1.2614  data: 0.0009  max mem: 65949
Evaluation  [ 910/1093]  eta: 0:04:15    time: 1.4222  data: 0.0009  max mem: 65949
Evaluation  [ 920/1093]  eta: 0:04:01    time: 1.4518  data: 0.0009  max mem: 65949
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.4035  data: 0.0010  max mem: 65949
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3244  data: 0.0010  max mem: 65949
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.3099  data: 0.0009  max mem: 65949
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.3875  data: 0.0009  max mem: 65949
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4478  data: 0.0009  max mem: 65949
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3088  data: 0.0009  max mem: 65949
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3096  data: 0.0009  max mem: 65949
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4544  data: 0.0009  max mem: 65949
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4363  data: 0.0009  max mem: 65949
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2837  data: 0.0009  max mem: 65949
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1471  data: 0.0010  max mem: 65949
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2450  data: 0.0009  max mem: 65949
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3314  data: 0.0009  max mem: 65949
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3783  data: 0.0009  max mem: 65949
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4492  data: 0.0009  max mem: 65949
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3825  data: 0.0009  max mem: 65949
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3455  data: 0.0010  max mem: 65949
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3192  data: 0.0444  max mem: 65949
Evaluation Total time: 0:25:16 (1.3879 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_8_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [9]  [   0/7110]  eta: 2 days, 6:01:35  lr: 0.000010  loss: 0.6275  time: 27.3552  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [  50/7110]  eta: 2:57:14  lr: 0.000010  loss: 0.4960  time: 1.0042  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 100/7110]  eta: 2:26:26  lr: 0.000010  loss: 0.1961  time: 0.9850  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 150/7110]  eta: 2:16:10  lr: 0.000010  loss: 0.2139  time: 0.9977  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 200/7110]  eta: 2:10:09  lr: 0.000010  loss: 0.3505  time: 1.0046  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 250/7110]  eta: 2:06:04  lr: 0.000010  loss: 0.0393  time: 1.0187  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 300/7110]  eta: 2:02:52  lr: 0.000010  loss: 0.4288  time: 0.9595  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 350/7110]  eta: 2:00:51  lr: 0.000010  loss: 0.1161  time: 0.9875  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 400/7110]  eta: 1:59:09  lr: 0.000010  loss: 0.1385  time: 0.9950  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 450/7110]  eta: 1:57:14  lr: 0.000010  loss: 0.4988  time: 0.9660  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 500/7110]  eta: 1:55:55  lr: 0.000010  loss: 0.5070  time: 1.0476  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 550/7110]  eta: 1:54:30  lr: 0.000010  loss: 0.1939  time: 1.0772  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 600/7110]  eta: 1:53:11  lr: 0.000010  loss: 0.4479  time: 1.0016  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 650/7110]  eta: 1:52:13  lr: 0.000010  loss: 0.5519  time: 1.0181  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 700/7110]  eta: 1:51:06  lr: 0.000010  loss: 0.6773  time: 0.9725  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 750/7110]  eta: 1:49:51  lr: 0.000010  loss: 0.1539  time: 0.9595  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 800/7110]  eta: 1:49:02  lr: 0.000010  loss: 0.3291  time: 0.9947  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 850/7110]  eta: 1:47:44  lr: 0.000010  loss: 0.1707  time: 0.9250  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 900/7110]  eta: 1:46:45  lr: 0.000010  loss: 0.1701  time: 1.0248  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [ 950/7110]  eta: 1:45:39  lr: 0.000010  loss: 0.3761  time: 0.9475  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1000/7110]  eta: 1:44:34  lr: 0.000010  loss: 0.1894  time: 1.0096  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1050/7110]  eta: 1:43:37  lr: 0.000010  loss: 0.1465  time: 0.9855  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1100/7110]  eta: 1:42:57  lr: 0.000010  loss: 0.1154  time: 1.1073  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1150/7110]  eta: 1:41:56  lr: 0.000010  loss: 0.1112  time: 1.0019  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1200/7110]  eta: 1:41:03  lr: 0.000010  loss: 0.2193  time: 0.9922  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1250/7110]  eta: 1:40:15  lr: 0.000010  loss: 0.3326  time: 1.0241  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1300/7110]  eta: 1:39:15  lr: 0.000010  loss: 0.3487  time: 0.9714  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1350/7110]  eta: 1:38:14  lr: 0.000010  loss: 0.0709  time: 0.9369  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1400/7110]  eta: 1:37:18  lr: 0.000010  loss: 0.1889  time: 0.9874  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1450/7110]  eta: 1:36:22  lr: 0.000010  loss: 0.2385  time: 0.9692  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1500/7110]  eta: 1:35:31  lr: 0.000010  loss: 0.4311  time: 1.0061  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1550/7110]  eta: 1:34:35  lr: 0.000010  loss: 0.7048  time: 1.0043  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1600/7110]  eta: 1:33:45  lr: 0.000010  loss: 0.1838  time: 1.0326  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1650/7110]  eta: 1:32:46  lr: 0.000010  loss: 0.1272  time: 0.9582  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1700/7110]  eta: 1:31:56  lr: 0.000010  loss: 0.6196  time: 1.0104  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1750/7110]  eta: 1:31:00  lr: 0.000010  loss: 0.1361  time: 0.9747  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1800/7110]  eta: 1:30:08  lr: 0.000010  loss: 0.1871  time: 1.0196  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1850/7110]  eta: 1:29:14  lr: 0.000010  loss: 0.0222  time: 0.9640  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1900/7110]  eta: 1:28:18  lr: 0.000010  loss: 0.1345  time: 0.9556  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [1950/7110]  eta: 1:27:23  lr: 0.000010  loss: 0.0363  time: 0.9124  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2000/7110]  eta: 1:26:37  lr: 0.000010  loss: 0.9400  time: 0.9848  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2050/7110]  eta: 1:25:44  lr: 0.000010  loss: 0.9713  time: 0.9680  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2100/7110]  eta: 1:24:55  lr: 0.000010  loss: 0.1913  time: 1.0249  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2150/7110]  eta: 1:24:03  lr: 0.000010  loss: 0.4945  time: 0.9823  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2200/7110]  eta: 1:23:10  lr: 0.000010  loss: 0.0572  time: 0.9734  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2250/7110]  eta: 1:22:19  lr: 0.000010  loss: 0.1366  time: 1.0078  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2300/7110]  eta: 1:21:31  lr: 0.000010  loss: 0.5612  time: 1.0565  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2350/7110]  eta: 1:20:38  lr: 0.000010  loss: 0.3669  time: 1.0069  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2400/7110]  eta: 1:19:46  lr: 0.000010  loss: 0.4721  time: 1.0139  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2450/7110]  eta: 1:18:53  lr: 0.000010  loss: 0.1117  time: 1.0462  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2500/7110]  eta: 1:18:01  lr: 0.000010  loss: 0.2955  time: 0.9550  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2550/7110]  eta: 1:17:08  lr: 0.000010  loss: 0.2854  time: 1.0003  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2600/7110]  eta: 1:16:16  lr: 0.000010  loss: 0.1227  time: 0.9988  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2650/7110]  eta: 1:15:23  lr: 0.000010  loss: 0.3922  time: 1.0097  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2700/7110]  eta: 1:14:32  lr: 0.000010  loss: 0.2161  time: 0.9871  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2750/7110]  eta: 1:13:42  lr: 0.000010  loss: 0.1278  time: 1.0156  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2800/7110]  eta: 1:12:48  lr: 0.000010  loss: 0.3623  time: 1.0092  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2850/7110]  eta: 1:11:56  lr: 0.000010  loss: 0.3046  time: 1.0108  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2900/7110]  eta: 1:11:06  lr: 0.000010  loss: 0.2979  time: 1.0645  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [2950/7110]  eta: 1:10:12  lr: 0.000010  loss: 0.2596  time: 0.9320  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3000/7110]  eta: 1:09:18  lr: 0.000010  loss: 0.2428  time: 0.9493  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3050/7110]  eta: 1:08:26  lr: 0.000010  loss: 0.2255  time: 0.9628  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3100/7110]  eta: 1:07:35  lr: 0.000010  loss: 0.0911  time: 1.0353  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3150/7110]  eta: 1:06:43  lr: 0.000010  loss: 0.1185  time: 1.0328  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3200/7110]  eta: 1:05:54  lr: 0.000010  loss: 0.1834  time: 1.0938  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3250/7110]  eta: 1:05:00  lr: 0.000010  loss: 0.1836  time: 0.8881  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3300/7110]  eta: 1:04:07  lr: 0.000010  loss: 0.3775  time: 0.9698  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3350/7110]  eta: 1:03:14  lr: 0.000010  loss: 0.2859  time: 0.9680  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3400/7110]  eta: 1:02:24  lr: 0.000010  loss: 0.3288  time: 1.0627  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3450/7110]  eta: 1:01:33  lr: 0.000010  loss: 0.5526  time: 1.0359  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3500/7110]  eta: 1:00:42  lr: 0.000010  loss: 0.0851  time: 0.9539  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3550/7110]  eta: 0:59:52  lr: 0.000010  loss: 0.5194  time: 1.0586  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3600/7110]  eta: 0:59:00  lr: 0.000010  loss: 0.1075  time: 0.9488  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3650/7110]  eta: 0:58:08  lr: 0.000010  loss: 0.5869  time: 0.9914  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3700/7110]  eta: 0:57:18  lr: 0.000010  loss: 0.5274  time: 1.0561  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3750/7110]  eta: 0:56:28  lr: 0.000010  loss: 0.2656  time: 0.9909  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3800/7110]  eta: 0:55:37  lr: 0.000010  loss: 0.5157  time: 1.0067  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3850/7110]  eta: 0:54:46  lr: 0.000010  loss: 0.1924  time: 0.9633  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3900/7110]  eta: 0:53:55  lr: 0.000010  loss: 0.2294  time: 1.0018  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [3950/7110]  eta: 0:53:03  lr: 0.000010  loss: 0.1883  time: 1.0074  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4000/7110]  eta: 0:52:12  lr: 0.000010  loss: 0.6272  time: 1.0191  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4050/7110]  eta: 0:51:22  lr: 0.000010  loss: 0.2819  time: 1.0473  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4100/7110]  eta: 0:50:31  lr: 0.000010  loss: 0.4090  time: 0.9496  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4150/7110]  eta: 0:49:40  lr: 0.000010  loss: 0.1509  time: 0.9408  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4200/7110]  eta: 0:48:49  lr: 0.000010  loss: 0.1799  time: 0.9597  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4250/7110]  eta: 0:47:59  lr: 0.000010  loss: 0.1908  time: 0.9800  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4300/7110]  eta: 0:47:08  lr: 0.000010  loss: 0.1618  time: 0.9763  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4350/7110]  eta: 0:46:17  lr: 0.000010  loss: 0.3438  time: 0.9935  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4400/7110]  eta: 0:45:27  lr: 0.000010  loss: 0.1611  time: 1.0461  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4450/7110]  eta: 0:44:35  lr: 0.000010  loss: 0.1220  time: 0.9731  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4500/7110]  eta: 0:43:44  lr: 0.000010  loss: 0.0273  time: 0.9427  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4550/7110]  eta: 0:42:54  lr: 0.000010  loss: 0.2606  time: 1.0756  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4600/7110]  eta: 0:42:04  lr: 0.000010  loss: 0.0690  time: 1.0169  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4650/7110]  eta: 0:41:14  lr: 0.000010  loss: 0.3491  time: 1.0415  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4700/7110]  eta: 0:40:24  lr: 0.000010  loss: 0.3636  time: 0.9944  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4750/7110]  eta: 0:39:33  lr: 0.000010  loss: 0.1787  time: 0.9512  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4800/7110]  eta: 0:38:42  lr: 0.000010  loss: 0.1619  time: 0.9973  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4850/7110]  eta: 0:37:53  lr: 0.000010  loss: 1.6753  time: 1.0588  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4900/7110]  eta: 0:37:02  lr: 0.000010  loss: 0.2275  time: 0.9864  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [4950/7110]  eta: 0:36:11  lr: 0.000010  loss: 0.0867  time: 1.0364  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5000/7110]  eta: 0:35:22  lr: 0.000010  loss: 0.2382  time: 1.0116  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5050/7110]  eta: 0:34:31  lr: 0.000010  loss: 0.0401  time: 0.9569  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5100/7110]  eta: 0:33:41  lr: 0.000010  loss: 0.1257  time: 1.0689  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5150/7110]  eta: 0:32:50  lr: 0.000010  loss: 0.5641  time: 1.0117  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5200/7110]  eta: 0:32:00  lr: 0.000010  loss: 0.1032  time: 0.9815  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5250/7110]  eta: 0:31:10  lr: 0.000010  loss: 0.3597  time: 0.9847  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5300/7110]  eta: 0:30:20  lr: 0.000010  loss: 0.3894  time: 1.0658  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5350/7110]  eta: 0:29:30  lr: 0.000010  loss: 0.0520  time: 1.0500  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5400/7110]  eta: 0:28:39  lr: 0.000010  loss: 0.2501  time: 0.9984  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5450/7110]  eta: 0:27:49  lr: 0.000010  loss: 0.5525  time: 0.9692  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5500/7110]  eta: 0:26:58  lr: 0.000010  loss: 0.3455  time: 1.0005  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5550/7110]  eta: 0:26:08  lr: 0.000010  loss: 0.4783  time: 1.0385  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5600/7110]  eta: 0:25:18  lr: 0.000010  loss: 0.1389  time: 1.0131  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5650/7110]  eta: 0:24:28  lr: 0.000010  loss: 0.2252  time: 0.9893  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5700/7110]  eta: 0:23:38  lr: 0.000010  loss: 0.3285  time: 1.0810  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5750/7110]  eta: 0:22:48  lr: 0.000010  loss: 0.1355  time: 0.9974  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5800/7110]  eta: 0:21:57  lr: 0.000010  loss: 0.2516  time: 1.0164  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5850/7110]  eta: 0:21:06  lr: 0.000010  loss: 0.1286  time: 0.9720  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5900/7110]  eta: 0:20:16  lr: 0.000010  loss: 0.5108  time: 0.9928  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [5950/7110]  eta: 0:19:26  lr: 0.000010  loss: 0.3612  time: 1.0735  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6000/7110]  eta: 0:18:35  lr: 0.000010  loss: 0.1757  time: 0.9398  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6050/7110]  eta: 0:17:45  lr: 0.000010  loss: 0.0528  time: 0.9679  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6100/7110]  eta: 0:16:55  lr: 0.000010  loss: 0.6935  time: 0.9767  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6150/7110]  eta: 0:16:04  lr: 0.000010  loss: 0.1261  time: 0.9894  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6200/7110]  eta: 0:15:14  lr: 0.000010  loss: 0.0439  time: 0.9945  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6250/7110]  eta: 0:14:24  lr: 0.000010  loss: 0.3800  time: 1.0564  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6300/7110]  eta: 0:13:33  lr: 0.000010  loss: 0.4410  time: 0.9836  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6350/7110]  eta: 0:12:43  lr: 0.000010  loss: 0.2454  time: 1.0084  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6400/7110]  eta: 0:11:53  lr: 0.000010  loss: 0.3705  time: 0.9448  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6450/7110]  eta: 0:11:03  lr: 0.000010  loss: 1.0868  time: 1.0104  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.3185  time: 0.9737  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.2093  time: 0.9557  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6600/7110]  eta: 0:08:32  lr: 0.000010  loss: 0.2728  time: 1.0097  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.4087  time: 1.0208  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.3897  time: 0.9992  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.0270  time: 0.9457  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.7442  time: 0.9688  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6850/7110]  eta: 0:04:21  lr: 0.000010  loss: 0.2696  time: 0.9850  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1545  time: 0.9874  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.3349  time: 1.0797  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.4746  time: 1.0119  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.4343  time: 0.9709  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.7871  time: 0.9690  data: 0.0000  max mem: 65949
Train: data epoch: [9]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.1019  time: 1.0701  data: 0.0000  max mem: 65949
Train: data epoch: [9] Total time: 1:58:56 (1.0037 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:39:04    time: 21.9072  data: 20.6641  max mem: 65949
Evaluation  [  10/1093]  eta: 0:59:06    time: 3.2751  data: 1.8796  max mem: 65949
Evaluation  [  20/1093]  eta: 0:42:00    time: 1.3715  data: 0.0011  max mem: 65949
Evaluation  [  30/1093]  eta: 0:35:05    time: 1.2694  data: 0.0010  max mem: 65949
Evaluation  [  40/1093]  eta: 0:32:21    time: 1.3129  data: 0.0010  max mem: 65949
Evaluation  [  50/1093]  eta: 0:30:28    time: 1.3986  data: 0.0009  max mem: 65949
Evaluation  [  60/1093]  eta: 0:28:34    time: 1.2821  data: 0.0010  max mem: 65949
Evaluation  [  70/1093]  eta: 0:27:51    time: 1.3322  data: 0.0010  max mem: 65949
Evaluation  [  80/1093]  eta: 0:26:59    time: 1.4148  data: 0.0010  max mem: 65949
Evaluation  [  90/1093]  eta: 0:26:17    time: 1.3533  data: 0.0010  max mem: 65949
Evaluation  [ 100/1093]  eta: 0:25:52    time: 1.4176  data: 0.0010  max mem: 65949
Evaluation  [ 110/1093]  eta: 0:25:27    time: 1.4697  data: 0.0010  max mem: 65949
Evaluation  [ 120/1093]  eta: 0:24:51    time: 1.3793  data: 0.0009  max mem: 65949
Evaluation  [ 130/1093]  eta: 0:24:37    time: 1.4252  data: 0.0010  max mem: 65949
Evaluation  [ 140/1093]  eta: 0:24:00    time: 1.3821  data: 0.0010  max mem: 65949
Evaluation  [ 150/1093]  eta: 0:23:32    time: 1.2570  data: 0.0009  max mem: 65949
Evaluation  [ 160/1093]  eta: 0:23:11    time: 1.3480  data: 0.0010  max mem: 65949
Evaluation  [ 170/1093]  eta: 0:22:43    time: 1.3267  data: 0.0010  max mem: 65949
Evaluation  [ 180/1093]  eta: 0:22:27    time: 1.3482  data: 0.0010  max mem: 65949
Evaluation  [ 190/1093]  eta: 0:22:11    time: 1.4479  data: 0.0009  max mem: 65949
Evaluation  [ 200/1093]  eta: 0:21:46    time: 1.3560  data: 0.0009  max mem: 65949
Evaluation  [ 210/1093]  eta: 0:21:43    time: 1.4886  data: 0.0010  max mem: 65949
Evaluation  [ 220/1093]  eta: 0:21:18    time: 1.4781  data: 0.0010  max mem: 65949
Evaluation  [ 230/1093]  eta: 0:20:56    time: 1.2522  data: 0.0010  max mem: 65949
Evaluation  [ 240/1093]  eta: 0:20:37    time: 1.2952  data: 0.0010  max mem: 65949
Evaluation  [ 250/1093]  eta: 0:20:21    time: 1.3669  data: 0.0010  max mem: 65949
Evaluation  [ 260/1093]  eta: 0:20:14    time: 1.5437  data: 0.0010  max mem: 65949
Evaluation  [ 270/1093]  eta: 0:19:52    time: 1.4515  data: 0.0010  max mem: 65949
Evaluation  [ 280/1093]  eta: 0:19:35    time: 1.2870  data: 0.0009  max mem: 65949
Evaluation  [ 290/1093]  eta: 0:19:27    time: 1.5169  data: 0.0009  max mem: 65949
Evaluation  [ 300/1093]  eta: 0:19:05    time: 1.4318  data: 0.0010  max mem: 65949
Evaluation  [ 310/1093]  eta: 0:18:47    time: 1.2298  data: 0.0010  max mem: 65949
Evaluation  [ 320/1093]  eta: 0:18:33    time: 1.3707  data: 0.0010  max mem: 65949
Evaluation  [ 330/1093]  eta: 0:18:16    time: 1.3882  data: 0.0010  max mem: 65949
Evaluation  [ 340/1093]  eta: 0:18:03    time: 1.4194  data: 0.0010  max mem: 65949
Evaluation  [ 350/1093]  eta: 0:17:48    time: 1.4707  data: 0.0011  max mem: 65949
Evaluation  [ 360/1093]  eta: 0:17:29    time: 1.3032  data: 0.0010  max mem: 65949
Evaluation  [ 370/1093]  eta: 0:17:13    time: 1.2660  data: 0.0009  max mem: 65949
Evaluation  [ 380/1093]  eta: 0:17:01    time: 1.4615  data: 0.0009  max mem: 65949
Evaluation  [ 390/1093]  eta: 0:16:47    time: 1.5134  data: 0.0010  max mem: 65949
Evaluation  [ 400/1093]  eta: 0:16:34    time: 1.4733  data: 0.0010  max mem: 65949
Evaluation  [ 410/1093]  eta: 0:16:19    time: 1.4348  data: 0.0010  max mem: 65949
Evaluation  [ 420/1093]  eta: 0:16:05    time: 1.4358  data: 0.0010  max mem: 65949
Evaluation  [ 430/1093]  eta: 0:15:51    time: 1.4547  data: 0.0010  max mem: 65949
Evaluation  [ 440/1093]  eta: 0:15:32    time: 1.2868  data: 0.0010  max mem: 65949
Evaluation  [ 450/1093]  eta: 0:15:18    time: 1.2964  data: 0.0010  max mem: 65949
Evaluation  [ 460/1093]  eta: 0:15:03    time: 1.3958  data: 0.0010  max mem: 65949
Evaluation  [ 470/1093]  eta: 0:14:47    time: 1.3260  data: 0.0010  max mem: 65949
Evaluation  [ 480/1093]  eta: 0:14:33    time: 1.3822  data: 0.0010  max mem: 65949
Evaluation  [ 490/1093]  eta: 0:14:20    time: 1.5063  data: 0.0011  max mem: 65949
Evaluation  [ 500/1093]  eta: 0:14:08    time: 1.5714  data: 0.0011  max mem: 65949
Evaluation  [ 510/1093]  eta: 0:13:55    time: 1.5831  data: 0.0010  max mem: 65949
Evaluation  [ 520/1093]  eta: 0:13:41    time: 1.4897  data: 0.0010  max mem: 65949
Evaluation  [ 530/1093]  eta: 0:13:27    time: 1.4523  data: 0.0010  max mem: 65949
Evaluation  [ 540/1093]  eta: 0:13:12    time: 1.4556  data: 0.0010  max mem: 65949
Evaluation  [ 550/1093]  eta: 0:12:58    time: 1.4223  data: 0.0010  max mem: 65949
Evaluation  [ 560/1093]  eta: 0:12:41    time: 1.2647  data: 0.0010  max mem: 65949
Evaluation  [ 570/1093]  eta: 0:12:26    time: 1.2294  data: 0.0010  max mem: 65949
Evaluation  [ 580/1093]  eta: 0:12:11    time: 1.3808  data: 0.0010  max mem: 65949
Evaluation  [ 590/1093]  eta: 0:11:55    time: 1.2864  data: 0.0009  max mem: 65949
Evaluation  [ 600/1093]  eta: 0:11:42    time: 1.3703  data: 0.0009  max mem: 65949
Evaluation  [ 610/1093]  eta: 0:11:28    time: 1.5078  data: 0.0009  max mem: 65949
Evaluation  [ 620/1093]  eta: 0:11:13    time: 1.4309  data: 0.0010  max mem: 65949
Evaluation  [ 630/1093]  eta: 0:10:59    time: 1.4410  data: 0.0010  max mem: 65949
Evaluation  [ 640/1093]  eta: 0:10:44    time: 1.3785  data: 0.0010  max mem: 65949
Evaluation  [ 650/1093]  eta: 0:10:31    time: 1.4083  data: 0.0010  max mem: 65949
Evaluation  [ 660/1093]  eta: 0:10:15    time: 1.3818  data: 0.0009  max mem: 65949
Evaluation  [ 670/1093]  eta: 0:10:01    time: 1.3545  data: 0.0009  max mem: 65949
Evaluation  [ 680/1093]  eta: 0:09:47    time: 1.4315  data: 0.0010  max mem: 65949
Evaluation  [ 690/1093]  eta: 0:09:34    time: 1.4833  data: 0.0010  max mem: 65949
Evaluation  [ 700/1093]  eta: 0:09:19    time: 1.4782  data: 0.0009  max mem: 65949
Evaluation  [ 710/1093]  eta: 0:09:04    time: 1.2827  data: 0.0010  max mem: 65949
Evaluation  [ 720/1093]  eta: 0:08:48    time: 1.1664  data: 0.0010  max mem: 65949
Evaluation  [ 730/1093]  eta: 0:08:32    time: 1.1117  data: 0.0010  max mem: 65949
Evaluation  [ 740/1093]  eta: 0:08:17    time: 1.1664  data: 0.0010  max mem: 65949
Evaluation  [ 750/1093]  eta: 0:08:03    time: 1.3320  data: 0.0012  max mem: 65949
Evaluation  [ 760/1093]  eta: 0:07:49    time: 1.3878  data: 0.0013  max mem: 65949
Evaluation  [ 770/1093]  eta: 0:07:35    time: 1.3944  data: 0.0011  max mem: 65949
Evaluation  [ 780/1093]  eta: 0:07:21    time: 1.4496  data: 0.0009  max mem: 65949
Evaluation  [ 790/1093]  eta: 0:07:05    time: 1.2647  data: 0.0010  max mem: 65949
Evaluation  [ 800/1093]  eta: 0:06:52    time: 1.3224  data: 0.0010  max mem: 65949
Evaluation  [ 810/1093]  eta: 0:06:38    time: 1.5695  data: 0.0010  max mem: 65949
Evaluation  [ 820/1093]  eta: 0:06:25    time: 1.5068  data: 0.0011  max mem: 65949
Evaluation  [ 830/1093]  eta: 0:06:11    time: 1.5466  data: 0.0011  max mem: 65949
Evaluation  [ 840/1093]  eta: 0:05:57    time: 1.4421  data: 0.0010  max mem: 65949
Evaluation  [ 850/1093]  eta: 0:05:42    time: 1.3438  data: 0.0011  max mem: 65949
Evaluation  [ 860/1093]  eta: 0:05:28    time: 1.3238  data: 0.0012  max mem: 65949
Evaluation  [ 870/1093]  eta: 0:05:14    time: 1.3273  data: 0.0011  max mem: 65949
Evaluation  [ 880/1093]  eta: 0:05:00    time: 1.5009  data: 0.0010  max mem: 65949
Evaluation  [ 890/1093]  eta: 0:04:45    time: 1.3464  data: 0.0010  max mem: 65949
Evaluation  [ 900/1093]  eta: 0:04:31    time: 1.2785  data: 0.0010  max mem: 65949
Evaluation  [ 910/1093]  eta: 0:04:17    time: 1.4441  data: 0.0010  max mem: 65949
Evaluation  [ 920/1093]  eta: 0:04:03    time: 1.4663  data: 0.0010  max mem: 65949
Evaluation  [ 930/1093]  eta: 0:03:49    time: 1.4264  data: 0.0010  max mem: 65949
Evaluation  [ 940/1093]  eta: 0:03:35    time: 1.3613  data: 0.0010  max mem: 65949
Evaluation  [ 950/1093]  eta: 0:03:21    time: 1.3355  data: 0.0010  max mem: 65949
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.4030  data: 0.0010  max mem: 65949
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4583  data: 0.0010  max mem: 65949
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3561  data: 0.0010  max mem: 65949
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.3568  data: 0.0010  max mem: 65949
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4637  data: 0.0010  max mem: 65949
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4939  data: 0.0010  max mem: 65949
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3440  data: 0.0010  max mem: 65949
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.2012  data: 0.0009  max mem: 65949
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2739  data: 0.0010  max mem: 65949
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3304  data: 0.0010  max mem: 65949
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3871  data: 0.0010  max mem: 65949
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4507  data: 0.0010  max mem: 65949
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.4007  data: 0.0010  max mem: 65949
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3496  data: 0.0010  max mem: 65949
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3239  data: 0.0445  max mem: 65949
Evaluation Total time: 0:25:34 (1.4040 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_9_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [10]  [   0/7110]  eta: 2 days, 6:11:03  lr: 0.000010  loss: 0.2383  time: 27.4351  data: 0.0001  max mem: 65949
Train: data epoch: [10]  [  50/7110]  eta: 2:57:33  lr: 0.000010  loss: 0.0385  time: 1.0430  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 100/7110]  eta: 2:28:31  lr: 0.000010  loss: 0.5402  time: 1.0764  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 150/7110]  eta: 2:17:02  lr: 0.000010  loss: 0.3563  time: 1.0114  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 200/7110]  eta: 2:09:28  lr: 0.000010  loss: 0.0412  time: 0.9775  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 250/7110]  eta: 2:06:00  lr: 0.000010  loss: 0.5652  time: 1.0297  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 300/7110]  eta: 2:02:36  lr: 0.000010  loss: 0.3849  time: 1.0003  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 350/7110]  eta: 1:59:54  lr: 0.000010  loss: 0.2906  time: 0.9933  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 400/7110]  eta: 1:58:25  lr: 0.000010  loss: 0.0806  time: 1.0391  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 450/7110]  eta: 1:56:19  lr: 0.000010  loss: 0.1787  time: 0.9586  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 500/7110]  eta: 1:54:37  lr: 0.000010  loss: 0.0983  time: 0.9580  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 550/7110]  eta: 1:52:53  lr: 0.000010  loss: 0.6739  time: 0.9470  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 600/7110]  eta: 1:51:39  lr: 0.000010  loss: 0.0691  time: 0.9726  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 650/7110]  eta: 1:50:25  lr: 0.000010  loss: 0.0894  time: 0.9989  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 700/7110]  eta: 1:49:32  lr: 0.000010  loss: 0.0973  time: 1.0494  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 750/7110]  eta: 1:48:45  lr: 0.000010  loss: 0.1432  time: 1.0591  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 800/7110]  eta: 1:47:42  lr: 0.000010  loss: 0.0945  time: 1.0475  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 850/7110]  eta: 1:46:36  lr: 0.000010  loss: 0.4905  time: 1.0049  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 900/7110]  eta: 1:45:27  lr: 0.000010  loss: 0.0949  time: 0.9469  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [ 950/7110]  eta: 1:44:21  lr: 0.000010  loss: 0.1432  time: 0.9379  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1000/7110]  eta: 1:43:32  lr: 0.000010  loss: 0.0799  time: 1.0611  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1050/7110]  eta: 1:42:38  lr: 0.000010  loss: 0.1143  time: 1.0331  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1100/7110]  eta: 1:41:32  lr: 0.000010  loss: 0.5285  time: 0.9601  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1150/7110]  eta: 1:40:36  lr: 0.000010  loss: 0.1192  time: 1.0177  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1200/7110]  eta: 1:39:40  lr: 0.000010  loss: 0.3193  time: 0.9988  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1250/7110]  eta: 1:38:34  lr: 0.000010  loss: 0.3348  time: 0.9310  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1300/7110]  eta: 1:37:46  lr: 0.000010  loss: 0.7332  time: 1.0910  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1350/7110]  eta: 1:37:00  lr: 0.000010  loss: 0.1447  time: 1.0252  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1400/7110]  eta: 1:36:05  lr: 0.000010  loss: 0.3087  time: 0.9929  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1450/7110]  eta: 1:35:20  lr: 0.000010  loss: 0.5034  time: 1.0241  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1500/7110]  eta: 1:34:27  lr: 0.000010  loss: 0.1633  time: 1.0162  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1550/7110]  eta: 1:33:27  lr: 0.000010  loss: 0.2588  time: 0.9625  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1600/7110]  eta: 1:32:28  lr: 0.000010  loss: 0.1982  time: 0.9485  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1650/7110]  eta: 1:31:40  lr: 0.000010  loss: 0.0585  time: 1.0367  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1700/7110]  eta: 1:30:51  lr: 0.000010  loss: 0.6905  time: 0.9712  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1750/7110]  eta: 1:30:00  lr: 0.000010  loss: 0.1257  time: 0.9976  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1800/7110]  eta: 1:29:07  lr: 0.000010  loss: 0.1814  time: 0.9651  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1850/7110]  eta: 1:28:16  lr: 0.000010  loss: 0.6494  time: 1.0119  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1900/7110]  eta: 1:27:21  lr: 0.000010  loss: 0.2700  time: 0.9782  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [1950/7110]  eta: 1:26:28  lr: 0.000010  loss: 0.1344  time: 0.9281  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2000/7110]  eta: 1:25:37  lr: 0.000010  loss: 0.0725  time: 0.9871  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2050/7110]  eta: 1:24:49  lr: 0.000010  loss: 0.1399  time: 0.9812  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2100/7110]  eta: 1:23:57  lr: 0.000010  loss: 0.2780  time: 1.0180  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2150/7110]  eta: 1:23:10  lr: 0.000010  loss: 0.0665  time: 1.0230  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2200/7110]  eta: 1:22:20  lr: 0.000010  loss: 0.3110  time: 1.0358  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2250/7110]  eta: 1:21:28  lr: 0.000010  loss: 0.5776  time: 0.9726  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2300/7110]  eta: 1:20:34  lr: 0.000010  loss: 0.0364  time: 1.0071  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2350/7110]  eta: 1:19:46  lr: 0.000010  loss: 0.3725  time: 0.9879  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2400/7110]  eta: 1:18:55  lr: 0.000010  loss: 0.3035  time: 0.9729  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2450/7110]  eta: 1:18:02  lr: 0.000010  loss: 0.5622  time: 0.9551  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2500/7110]  eta: 1:17:16  lr: 0.000010  loss: 0.0871  time: 1.0765  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2550/7110]  eta: 1:16:26  lr: 0.000010  loss: 0.7466  time: 1.0717  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2600/7110]  eta: 1:15:36  lr: 0.000010  loss: 0.3309  time: 0.9968  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2650/7110]  eta: 1:14:46  lr: 0.000010  loss: 0.3709  time: 0.9807  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2700/7110]  eta: 1:13:56  lr: 0.000010  loss: 0.6613  time: 1.0050  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2750/7110]  eta: 1:13:03  lr: 0.000010  loss: 0.1454  time: 0.9730  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2800/7110]  eta: 1:12:11  lr: 0.000010  loss: 0.5672  time: 0.9901  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2850/7110]  eta: 1:11:21  lr: 0.000010  loss: 0.0781  time: 1.0150  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2900/7110]  eta: 1:10:33  lr: 0.000010  loss: 0.2546  time: 1.0309  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [2950/7110]  eta: 1:09:42  lr: 0.000010  loss: 0.3221  time: 0.9530  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3000/7110]  eta: 1:08:50  lr: 0.000010  loss: 0.1325  time: 1.0035  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3050/7110]  eta: 1:07:59  lr: 0.000010  loss: 0.1966  time: 1.0089  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3100/7110]  eta: 1:07:08  lr: 0.000010  loss: 0.6917  time: 0.9714  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3150/7110]  eta: 1:06:16  lr: 0.000010  loss: 0.1116  time: 0.9699  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3200/7110]  eta: 1:05:27  lr: 0.000010  loss: 0.4745  time: 1.0649  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3250/7110]  eta: 1:04:35  lr: 0.000010  loss: 0.0435  time: 0.9819  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3300/7110]  eta: 1:03:42  lr: 0.000010  loss: 0.3891  time: 0.9787  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3350/7110]  eta: 1:02:52  lr: 0.000010  loss: 0.7066  time: 1.0226  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3400/7110]  eta: 1:02:01  lr: 0.000010  loss: 0.2218  time: 0.9846  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3450/7110]  eta: 1:01:12  lr: 0.000010  loss: 0.1157  time: 1.0293  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3500/7110]  eta: 1:00:21  lr: 0.000010  loss: 0.1295  time: 0.9700  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3550/7110]  eta: 0:59:30  lr: 0.000010  loss: 0.1535  time: 1.0071  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3600/7110]  eta: 0:58:39  lr: 0.000010  loss: 0.4479  time: 0.9278  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3650/7110]  eta: 0:57:49  lr: 0.000010  loss: 0.1078  time: 0.9874  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3700/7110]  eta: 0:56:58  lr: 0.000010  loss: 0.0671  time: 0.9912  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3750/7110]  eta: 0:56:08  lr: 0.000010  loss: 0.5611  time: 1.0163  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3800/7110]  eta: 0:55:17  lr: 0.000010  loss: 0.1281  time: 0.9988  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3850/7110]  eta: 0:54:27  lr: 0.000010  loss: 0.1924  time: 1.0235  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3900/7110]  eta: 0:53:35  lr: 0.000010  loss: 0.6495  time: 0.9615  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [3950/7110]  eta: 0:52:43  lr: 0.000010  loss: 1.7871  time: 0.9552  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4000/7110]  eta: 0:51:52  lr: 0.000010  loss: 0.2557  time: 0.9657  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4050/7110]  eta: 0:51:02  lr: 0.000010  loss: 0.0861  time: 1.0097  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4100/7110]  eta: 0:50:13  lr: 0.000010  loss: 0.1400  time: 1.0126  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4150/7110]  eta: 0:49:25  lr: 0.000010  loss: 0.3822  time: 1.0790  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4200/7110]  eta: 0:48:36  lr: 0.000010  loss: 0.3296  time: 1.0389  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4250/7110]  eta: 0:47:46  lr: 0.000010  loss: 0.2573  time: 1.0242  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4300/7110]  eta: 0:46:56  lr: 0.000010  loss: 0.1743  time: 0.9831  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4350/7110]  eta: 0:46:04  lr: 0.000010  loss: 0.1328  time: 0.9153  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4400/7110]  eta: 0:45:13  lr: 0.000010  loss: 0.4842  time: 0.9768  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4450/7110]  eta: 0:44:23  lr: 0.000010  loss: 0.1475  time: 1.0155  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4500/7110]  eta: 0:43:32  lr: 0.000010  loss: 0.3686  time: 0.9610  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4550/7110]  eta: 0:42:42  lr: 0.000010  loss: 0.4358  time: 0.9559  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4600/7110]  eta: 0:41:51  lr: 0.000010  loss: 0.2852  time: 0.9633  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4650/7110]  eta: 0:41:00  lr: 0.000010  loss: 0.3016  time: 0.9656  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4700/7110]  eta: 0:40:09  lr: 0.000010  loss: 0.1996  time: 0.9167  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4750/7110]  eta: 0:39:18  lr: 0.000010  loss: 0.3192  time: 1.0251  data: 0.0000  max mem: 65949
Train: data epoch: [10]  [4800/7110]  eta: 0:38:29  lr: 0.000010  loss: 0.1900  time: 1.0616  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [4850/7110]  eta: 0:37:39  lr: 0.000010  loss: 0.0601  time: 1.0261  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [4900/7110]  eta: 0:36:50  lr: 0.000010  loss: 0.1591  time: 1.0059  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [4950/7110]  eta: 0:35:59  lr: 0.000010  loss: 0.4085  time: 0.9999  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5000/7110]  eta: 0:35:10  lr: 0.000010  loss: 0.1099  time: 1.0698  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5050/7110]  eta: 0:34:20  lr: 0.000010  loss: 0.3705  time: 0.9535  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5100/7110]  eta: 0:33:30  lr: 0.000010  loss: 0.0639  time: 1.0039  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5150/7110]  eta: 0:32:40  lr: 0.000010  loss: 0.0429  time: 0.9747  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5200/7110]  eta: 0:31:49  lr: 0.000010  loss: 0.0564  time: 1.0001  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5250/7110]  eta: 0:30:59  lr: 0.000010  loss: 0.5582  time: 1.0101  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5300/7110]  eta: 0:30:09  lr: 0.000010  loss: 0.1847  time: 1.0437  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5350/7110]  eta: 0:29:20  lr: 0.000010  loss: 0.8409  time: 1.0607  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5400/7110]  eta: 0:28:30  lr: 0.000010  loss: 0.3260  time: 1.0031  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5450/7110]  eta: 0:27:40  lr: 0.000010  loss: 0.1200  time: 1.0349  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5500/7110]  eta: 0:26:50  lr: 0.000010  loss: 0.0670  time: 1.0246  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5550/7110]  eta: 0:26:00  lr: 0.000010  loss: 0.3352  time: 0.9798  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5600/7110]  eta: 0:25:10  lr: 0.000010  loss: 0.1208  time: 1.0299  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5650/7110]  eta: 0:24:20  lr: 0.000010  loss: 0.1708  time: 1.0345  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5700/7110]  eta: 0:23:31  lr: 0.000010  loss: 1.2231  time: 1.0339  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5750/7110]  eta: 0:22:41  lr: 0.000010  loss: 0.6003  time: 1.0193  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5800/7110]  eta: 0:21:51  lr: 0.000010  loss: 0.3985  time: 1.0189  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5850/7110]  eta: 0:21:01  lr: 0.000010  loss: 0.2466  time: 1.0040  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5900/7110]  eta: 0:20:10  lr: 0.000010  loss: 0.1508  time: 0.9572  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [5950/7110]  eta: 0:19:20  lr: 0.000010  loss: 0.4220  time: 0.9383  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6000/7110]  eta: 0:18:30  lr: 0.000010  loss: 0.6334  time: 0.9891  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6050/7110]  eta: 0:17:40  lr: 0.000010  loss: 0.1874  time: 1.0089  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6100/7110]  eta: 0:16:50  lr: 0.000010  loss: 0.2432  time: 0.9894  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6150/7110]  eta: 0:16:00  lr: 0.000010  loss: 0.3992  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6200/7110]  eta: 0:15:10  lr: 0.000010  loss: 0.3753  time: 0.9769  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6250/7110]  eta: 0:14:20  lr: 0.000010  loss: 0.1439  time: 0.9815  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6300/7110]  eta: 0:13:30  lr: 0.000010  loss: 0.0360  time: 0.9769  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6350/7110]  eta: 0:12:40  lr: 0.000010  loss: 0.3221  time: 1.0157  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6400/7110]  eta: 0:11:50  lr: 0.000010  loss: 0.3057  time: 1.0493  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6450/7110]  eta: 0:11:00  lr: 0.000010  loss: 0.1389  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6500/7110]  eta: 0:10:10  lr: 0.000010  loss: 0.5213  time: 1.0350  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6550/7110]  eta: 0:09:20  lr: 0.000010  loss: 0.6195  time: 1.0007  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.7547  time: 1.0496  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.4610  time: 1.0296  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.0674  time: 0.9345  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.1771  time: 0.9577  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.4409  time: 1.0303  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.5447  time: 1.0202  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1360  time: 1.0135  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.2387  time: 1.0077  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.2677  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.8330  time: 0.9992  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.0804  time: 0.9724  data: 0.0000  max mem: 66110
Train: data epoch: [10]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.2085  time: 1.1563  data: 0.0000  max mem: 66110
Train: data epoch: [10] Total time: 1:58:43 (1.0020 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:25:45    time: 21.1763  data: 19.9119  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:50    time: 3.2046  data: 1.8111  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:23    time: 1.3710  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:43    time: 1.2755  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:04    time: 1.3173  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:15    time: 1.4012  data: 0.0011  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:18    time: 1.2692  data: 0.0011  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:34    time: 1.3016  data: 0.0011  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:43    time: 1.3951  data: 0.0011  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:02    time: 1.3465  data: 0.0011  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:52    time: 1.4825  data: 0.0011  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:27    time: 1.5346  data: 0.0011  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:50    time: 1.3743  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:34    time: 1.4067  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:57    time: 1.3634  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:28    time: 1.2512  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:08    time: 1.3495  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:40    time: 1.3239  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:22    time: 1.3226  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:03    time: 1.3974  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:41    time: 1.3430  data: 0.0009  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:34    time: 1.4685  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:09    time: 1.4247  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:48    time: 1.2451  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:29    time: 1.3023  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:14    time: 1.3649  data: 0.0009  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:07    time: 1.5457  data: 0.0009  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:44    time: 1.4289  data: 0.0009  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:28    time: 1.2693  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:20    time: 1.5254  data: 0.0012  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:59    time: 1.4287  data: 0.0012  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:40    time: 1.2209  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:25    time: 1.3279  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:06    time: 1.3219  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:54    time: 1.3923  data: 0.0012  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:40    time: 1.4739  data: 0.0012  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:21    time: 1.3052  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:05    time: 1.2514  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:54    time: 1.4547  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:38    time: 1.4538  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:25    time: 1.4039  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:09    time: 1.4136  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:56    time: 1.4127  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:41    time: 1.4261  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:23    time: 1.2546  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:08    time: 1.2578  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:53    time: 1.3502  data: 0.0011  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:37    time: 1.3042  data: 0.0012  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:23    time: 1.3615  data: 0.0013  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:10    time: 1.4849  data: 0.0011  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:58    time: 1.5588  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:45    time: 1.5469  data: 0.0011  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:31    time: 1.4760  data: 0.0011  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:18    time: 1.4710  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:04    time: 1.4793  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:50    time: 1.4340  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:33    time: 1.2579  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:18    time: 1.2235  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:04    time: 1.3792  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:47    time: 1.2678  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:35    time: 1.3435  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:21    time: 1.5062  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:07    time: 1.4104  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:52    time: 1.3870  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:37    time: 1.3440  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:24    time: 1.3746  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:09    time: 1.3619  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:55    time: 1.3929  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:41    time: 1.4434  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:27    time: 1.4469  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:14    time: 1.4759  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:58    time: 1.3092  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:43    time: 1.1704  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:27    time: 1.1139  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:12    time: 1.1309  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:58    time: 1.3208  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:44    time: 1.4160  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:30    time: 1.3737  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:17    time: 1.4303  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:01    time: 1.2528  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:48    time: 1.3263  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:35    time: 1.5442  data: 0.0011  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:21    time: 1.4155  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:07    time: 1.4938  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:53    time: 1.4292  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:39    time: 1.3068  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:24    time: 1.2888  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:10    time: 1.3254  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:57    time: 1.5019  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:42    time: 1.3477  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:29    time: 1.2702  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:15    time: 1.4287  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:01    time: 1.4610  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.4065  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3113  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.2997  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.3981  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4376  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.2940  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3046  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4463  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4768  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.3288  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1580  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2495  data: 0.0009  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3386  data: 0.0009  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3759  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4427  data: 0.0011  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3775  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3385  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3124  data: 0.0420  max mem: 66110
Evaluation Total time: 0:25:17 (1.3882 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_10_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [11]  [   0/7110]  eta: 2 days, 8:32:48  lr: 0.000010  loss: 0.1935  time: 28.6313  data: 0.0001  max mem: 66110
Train: data epoch: [11]  [  50/7110]  eta: 3:00:14  lr: 0.000010  loss: 0.1299  time: 0.9245  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 100/7110]  eta: 2:29:02  lr: 0.000010  loss: 0.1577  time: 0.9975  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 150/7110]  eta: 2:17:10  lr: 0.000010  loss: 0.2906  time: 0.9847  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 200/7110]  eta: 2:10:42  lr: 0.000010  loss: 0.1943  time: 1.0225  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 250/7110]  eta: 2:06:25  lr: 0.000010  loss: 0.1793  time: 0.9926  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 300/7110]  eta: 2:03:15  lr: 0.000010  loss: 0.1372  time: 0.9696  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 350/7110]  eta: 2:01:41  lr: 0.000010  loss: 0.0810  time: 1.0417  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 400/7110]  eta: 1:59:34  lr: 0.000010  loss: 0.1621  time: 0.9938  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 450/7110]  eta: 1:57:55  lr: 0.000010  loss: 0.4219  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 500/7110]  eta: 1:56:13  lr: 0.000010  loss: 0.0378  time: 0.9624  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 550/7110]  eta: 1:54:26  lr: 0.000010  loss: 0.4246  time: 0.9670  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 600/7110]  eta: 1:53:00  lr: 0.000010  loss: 0.1353  time: 0.9731  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 650/7110]  eta: 1:51:30  lr: 0.000010  loss: 0.5926  time: 0.9722  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 700/7110]  eta: 1:50:30  lr: 0.000010  loss: 0.4392  time: 1.0843  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 750/7110]  eta: 1:49:17  lr: 0.000010  loss: 0.9652  time: 0.9859  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 800/7110]  eta: 1:47:57  lr: 0.000010  loss: 0.9630  time: 0.9514  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 850/7110]  eta: 1:46:50  lr: 0.000010  loss: 0.1341  time: 0.9714  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 900/7110]  eta: 1:45:47  lr: 0.000010  loss: 0.3857  time: 1.0234  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [ 950/7110]  eta: 1:44:41  lr: 0.000010  loss: 0.1248  time: 0.9776  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1000/7110]  eta: 1:43:42  lr: 0.000010  loss: 0.3694  time: 0.9759  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1050/7110]  eta: 1:42:39  lr: 0.000010  loss: 0.0776  time: 1.0016  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1100/7110]  eta: 1:41:38  lr: 0.000010  loss: 0.4314  time: 1.0209  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1150/7110]  eta: 1:40:43  lr: 0.000010  loss: 0.2633  time: 0.9719  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1200/7110]  eta: 1:39:56  lr: 0.000010  loss: 0.0741  time: 0.9945  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1250/7110]  eta: 1:38:57  lr: 0.000010  loss: 0.2022  time: 1.0084  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1300/7110]  eta: 1:38:03  lr: 0.000010  loss: 0.1644  time: 1.0201  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1350/7110]  eta: 1:37:13  lr: 0.000010  loss: 0.3994  time: 1.0193  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1400/7110]  eta: 1:36:18  lr: 0.000010  loss: 0.5830  time: 0.9820  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1450/7110]  eta: 1:35:25  lr: 0.000010  loss: 0.4456  time: 1.0271  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1500/7110]  eta: 1:34:42  lr: 0.000010  loss: 0.6093  time: 1.0555  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1550/7110]  eta: 1:33:49  lr: 0.000010  loss: 0.1348  time: 0.9891  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1600/7110]  eta: 1:32:51  lr: 0.000010  loss: 0.3069  time: 0.9762  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1650/7110]  eta: 1:31:59  lr: 0.000010  loss: 0.3212  time: 1.0115  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1700/7110]  eta: 1:31:08  lr: 0.000010  loss: 0.1446  time: 1.0083  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1750/7110]  eta: 1:30:17  lr: 0.000010  loss: 0.4296  time: 1.0318  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1800/7110]  eta: 1:29:23  lr: 0.000010  loss: 0.1126  time: 1.0398  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1850/7110]  eta: 1:28:31  lr: 0.000010  loss: 0.6018  time: 1.0214  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1900/7110]  eta: 1:27:40  lr: 0.000010  loss: 0.0969  time: 0.9888  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [1950/7110]  eta: 1:26:49  lr: 0.000010  loss: 0.3360  time: 1.0199  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2000/7110]  eta: 1:26:03  lr: 0.000010  loss: 0.3625  time: 1.0529  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2050/7110]  eta: 1:25:12  lr: 0.000010  loss: 0.5022  time: 0.9571  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2100/7110]  eta: 1:24:19  lr: 0.000010  loss: 1.5553  time: 0.9780  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2150/7110]  eta: 1:23:24  lr: 0.000010  loss: 0.1320  time: 1.0324  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2200/7110]  eta: 1:22:26  lr: 0.000010  loss: 0.2792  time: 0.9565  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2250/7110]  eta: 1:21:41  lr: 0.000010  loss: 0.1322  time: 1.0648  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2300/7110]  eta: 1:20:51  lr: 0.000010  loss: 0.4039  time: 0.9838  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2350/7110]  eta: 1:20:03  lr: 0.000010  loss: 0.4639  time: 1.0586  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2400/7110]  eta: 1:19:09  lr: 0.000010  loss: 0.4086  time: 0.9520  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2450/7110]  eta: 1:18:17  lr: 0.000010  loss: 0.6069  time: 0.9829  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2500/7110]  eta: 1:17:27  lr: 0.000010  loss: 0.4200  time: 0.9715  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2550/7110]  eta: 1:16:35  lr: 0.000010  loss: 0.3451  time: 1.0014  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2600/7110]  eta: 1:15:42  lr: 0.000010  loss: 0.0736  time: 0.9529  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2650/7110]  eta: 1:14:51  lr: 0.000010  loss: 0.4128  time: 0.9999  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2700/7110]  eta: 1:14:01  lr: 0.000010  loss: 0.1599  time: 1.0347  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2750/7110]  eta: 1:13:13  lr: 0.000010  loss: 0.3575  time: 1.0346  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2800/7110]  eta: 1:12:22  lr: 0.000010  loss: 0.2696  time: 1.0096  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2850/7110]  eta: 1:11:31  lr: 0.000010  loss: 0.5287  time: 1.0186  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2900/7110]  eta: 1:10:41  lr: 0.000010  loss: 0.3750  time: 0.9761  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [2950/7110]  eta: 1:09:50  lr: 0.000010  loss: 0.0591  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3000/7110]  eta: 1:08:59  lr: 0.000010  loss: 0.3011  time: 0.9169  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3050/7110]  eta: 1:08:10  lr: 0.000010  loss: 0.2753  time: 1.0479  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3100/7110]  eta: 1:07:19  lr: 0.000010  loss: 0.0761  time: 1.0159  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3150/7110]  eta: 1:06:27  lr: 0.000010  loss: 0.0252  time: 0.9492  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3200/7110]  eta: 1:05:35  lr: 0.000010  loss: 0.1256  time: 0.9636  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3250/7110]  eta: 1:04:45  lr: 0.000010  loss: 0.1816  time: 0.9700  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3300/7110]  eta: 1:03:54  lr: 0.000010  loss: 0.1404  time: 1.0183  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3350/7110]  eta: 1:03:06  lr: 0.000010  loss: 0.1073  time: 1.0332  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3400/7110]  eta: 1:02:14  lr: 0.000010  loss: 0.0734  time: 0.9720  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3450/7110]  eta: 1:01:22  lr: 0.000010  loss: 0.0512  time: 0.9585  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3500/7110]  eta: 1:00:30  lr: 0.000010  loss: 0.0586  time: 1.0351  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3550/7110]  eta: 0:59:41  lr: 0.000010  loss: 0.1090  time: 1.1130  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3600/7110]  eta: 0:58:50  lr: 0.000010  loss: 0.3758  time: 1.0064  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3650/7110]  eta: 0:58:01  lr: 0.000010  loss: 0.4225  time: 1.0189  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3700/7110]  eta: 0:57:11  lr: 0.000010  loss: 0.2089  time: 1.0445  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3750/7110]  eta: 0:56:21  lr: 0.000010  loss: 0.0968  time: 1.0629  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3800/7110]  eta: 0:55:30  lr: 0.000010  loss: 0.3715  time: 1.0421  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3850/7110]  eta: 0:54:38  lr: 0.000010  loss: 0.7813  time: 0.9335  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3900/7110]  eta: 0:53:47  lr: 0.000010  loss: 0.3477  time: 0.9639  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [3950/7110]  eta: 0:52:58  lr: 0.000010  loss: 1.7763  time: 1.0130  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4000/7110]  eta: 0:52:07  lr: 0.000010  loss: 0.7240  time: 0.9957  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4050/7110]  eta: 0:51:16  lr: 0.000010  loss: 0.1676  time: 1.0234  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4100/7110]  eta: 0:50:25  lr: 0.000010  loss: 0.4937  time: 0.9922  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4150/7110]  eta: 0:49:35  lr: 0.000010  loss: 0.5202  time: 1.0146  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4200/7110]  eta: 0:48:43  lr: 0.000010  loss: 0.3474  time: 0.9975  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4250/7110]  eta: 0:47:53  lr: 0.000010  loss: 0.1063  time: 1.0347  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4300/7110]  eta: 0:47:02  lr: 0.000010  loss: 1.0251  time: 0.9814  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4350/7110]  eta: 0:46:12  lr: 0.000010  loss: 0.4139  time: 0.9926  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4400/7110]  eta: 0:45:23  lr: 0.000010  loss: 0.5552  time: 1.1089  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4450/7110]  eta: 0:44:31  lr: 0.000010  loss: 0.3277  time: 0.9381  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4500/7110]  eta: 0:43:40  lr: 0.000010  loss: 0.3494  time: 0.9819  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4550/7110]  eta: 0:42:49  lr: 0.000010  loss: 0.1074  time: 1.0560  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4600/7110]  eta: 0:41:59  lr: 0.000010  loss: 0.5202  time: 1.0716  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4650/7110]  eta: 0:41:09  lr: 0.000010  loss: 0.4388  time: 0.9634  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4700/7110]  eta: 0:40:18  lr: 0.000010  loss: 0.0495  time: 0.9869  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4750/7110]  eta: 0:39:28  lr: 0.000010  loss: 0.5108  time: 1.0078  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4800/7110]  eta: 0:38:39  lr: 0.000010  loss: 0.5319  time: 1.0292  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4850/7110]  eta: 0:37:48  lr: 0.000010  loss: 0.0821  time: 1.0448  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4900/7110]  eta: 0:36:59  lr: 0.000010  loss: 0.1376  time: 1.0126  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [4950/7110]  eta: 0:36:08  lr: 0.000010  loss: 0.6583  time: 1.0095  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5000/7110]  eta: 0:35:18  lr: 0.000010  loss: 0.5519  time: 0.9964  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5050/7110]  eta: 0:34:27  lr: 0.000010  loss: 0.2717  time: 1.0169  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5100/7110]  eta: 0:33:36  lr: 0.000010  loss: 0.1205  time: 1.0128  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5150/7110]  eta: 0:32:46  lr: 0.000010  loss: 0.2276  time: 1.0145  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5200/7110]  eta: 0:31:55  lr: 0.000010  loss: 0.3613  time: 0.9976  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5250/7110]  eta: 0:31:06  lr: 0.000010  loss: 0.1702  time: 1.0144  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5300/7110]  eta: 0:30:16  lr: 0.000010  loss: 0.2014  time: 0.9721  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5350/7110]  eta: 0:29:25  lr: 0.000010  loss: 0.3795  time: 0.9725  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5400/7110]  eta: 0:28:34  lr: 0.000010  loss: 0.0364  time: 0.9554  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5450/7110]  eta: 0:27:44  lr: 0.000010  loss: 0.0732  time: 0.9605  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5500/7110]  eta: 0:26:54  lr: 0.000010  loss: 0.1585  time: 1.0363  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5550/7110]  eta: 0:26:04  lr: 0.000010  loss: 0.1001  time: 0.9761  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5600/7110]  eta: 0:25:13  lr: 0.000010  loss: 0.0394  time: 0.9510  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5650/7110]  eta: 0:24:23  lr: 0.000010  loss: 0.0777  time: 1.0069  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5700/7110]  eta: 0:23:33  lr: 0.000010  loss: 0.0427  time: 1.0526  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5750/7110]  eta: 0:22:43  lr: 0.000010  loss: 0.1897  time: 0.9667  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5800/7110]  eta: 0:21:53  lr: 0.000010  loss: 0.2476  time: 1.0562  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.1664  time: 1.0031  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5900/7110]  eta: 0:20:12  lr: 0.000010  loss: 0.2137  time: 0.9359  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.1342  time: 1.0071  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.5161  time: 0.9755  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.2367  time: 1.0034  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.3718  time: 0.9766  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.3250  time: 1.0161  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.4940  time: 1.0309  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.2525  time: 1.0015  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.0966  time: 0.9824  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 1.2641  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.4910  time: 0.9963  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.3083  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.3661  time: 0.9439  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.2633  time: 0.9897  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.3832  time: 1.0367  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.4019  time: 0.9795  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.3511  time: 1.0181  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.4071  time: 0.9823  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.1336  time: 0.9825  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.3033  time: 0.9829  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1769  time: 0.9916  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1221  time: 0.9498  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.2796  time: 1.0085  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.2265  time: 0.9851  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.0560  time: 1.0627  data: 0.0000  max mem: 66110
Train: data epoch: [11]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0571  time: 1.1417  data: 0.0000  max mem: 66110
Train: data epoch: [11] Total time: 1:58:41 (1.0016 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:54:01    time: 22.7274  data: 21.4748  max mem: 66110
Evaluation  [  10/1093]  eta: 1:00:29    time: 3.3514  data: 1.9533  max mem: 66110
Evaluation  [  20/1093]  eta: 0:42:39    time: 1.3686  data: 0.0011  max mem: 66110
Evaluation  [  30/1093]  eta: 0:35:42    time: 1.2811  data: 0.0011  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:47    time: 1.3250  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:55    time: 1.4114  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:48    time: 1.2733  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:28:01    time: 1.3010  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:27:08    time: 1.4057  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:27    time: 1.3634  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:57    time: 1.4134  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:32    time: 1.4522  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:57    time: 1.3868  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:46    time: 1.4558  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:07    time: 1.4022  data: 0.0011  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:38    time: 1.2508  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:18    time: 1.3528  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:49    time: 1.3267  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:31    time: 1.3313  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:12    time: 1.4112  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:49    time: 1.3420  data: 0.0011  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:45    time: 1.5015  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:20    time: 1.4691  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:58    time: 1.2439  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:38    time: 1.2912  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:22    time: 1.3601  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:15    time: 1.5519  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:53    time: 1.4543  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:36    time: 1.2794  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:28    time: 1.5085  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:05    time: 1.4140  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:47    time: 1.2218  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:33    time: 1.3769  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:16    time: 1.3890  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:03    time: 1.4156  data: 0.0011  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:48    time: 1.4721  data: 0.0011  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:29    time: 1.2977  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:13    time: 1.2666  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:01    time: 1.4724  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:46    time: 1.4839  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:33    time: 1.4398  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:19    time: 1.4729  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:06    time: 1.4762  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:50    time: 1.4260  data: 0.0009  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:32    time: 1.2622  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:17    time: 1.2910  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:03    time: 1.4234  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:48    time: 1.3790  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:34    time: 1.3857  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:21    time: 1.5034  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:09    time: 1.5747  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:55    time: 1.5637  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:41    time: 1.4862  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:27    time: 1.4643  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:13    time: 1.4815  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:59    time: 1.4481  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:41    time: 1.2598  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:26    time: 1.2012  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:12    time: 1.3576  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:55    time: 1.2609  data: 0.0011  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:42    time: 1.3476  data: 0.0011  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:27    time: 1.5065  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:13    time: 1.4180  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:59    time: 1.4337  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:44    time: 1.3901  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:31    time: 1.4393  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:16    time: 1.4179  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:02    time: 1.3703  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:47    time: 1.4232  data: 0.0012  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:34    time: 1.4518  data: 0.0012  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:19    time: 1.4799  data: 0.0012  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:04    time: 1.3080  data: 0.0011  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:48    time: 1.1763  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:32    time: 1.1321  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:18    time: 1.1769  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:03    time: 1.3347  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:49    time: 1.3934  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:35    time: 1.3998  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:21    time: 1.4560  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:06    time: 1.2831  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:53    time: 1.3260  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:38    time: 1.4795  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:24    time: 1.4173  data: 0.0012  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:11    time: 1.5391  data: 0.0012  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:56    time: 1.4472  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:42    time: 1.3505  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:28    time: 1.3413  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:14    time: 1.3432  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:00    time: 1.5097  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:45    time: 1.3477  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:31    time: 1.2626  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:17    time: 1.4233  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:03    time: 1.4526  data: 0.0009  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:49    time: 1.3977  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:35    time: 1.3323  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:21    time: 1.3174  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.3966  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4605  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:38    time: 1.3640  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.3724  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:10    time: 1.4689  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4887  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3326  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1839  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2760  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3609  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.4060  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4456  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3617  data: 0.0009  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3185  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2953  data: 0.0476  max mem: 66110
Evaluation Total time: 0:25:33 (1.4026 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_11_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [12]  [   0/7110]  eta: 2 days, 5:44:17  lr: 0.000010  loss: 0.1743  time: 27.2093  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [  50/7110]  eta: 2:58:01  lr: 0.000010  loss: 0.3630  time: 1.0137  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 100/7110]  eta: 2:27:28  lr: 0.000010  loss: 0.0587  time: 0.9937  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 150/7110]  eta: 2:16:26  lr: 0.000010  loss: 0.6258  time: 1.0100  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 200/7110]  eta: 2:09:08  lr: 0.000010  loss: 0.1235  time: 0.9685  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 250/7110]  eta: 2:05:42  lr: 0.000010  loss: 0.2854  time: 1.0715  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 300/7110]  eta: 2:02:25  lr: 0.000010  loss: 0.7329  time: 0.9578  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 350/7110]  eta: 2:00:48  lr: 0.000010  loss: 0.8854  time: 0.9686  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 400/7110]  eta: 1:59:10  lr: 0.000010  loss: 0.2854  time: 1.0723  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 450/7110]  eta: 1:57:49  lr: 0.000010  loss: 0.5730  time: 1.0088  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 500/7110]  eta: 1:56:07  lr: 0.000010  loss: 0.1076  time: 1.0105  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 550/7110]  eta: 1:55:00  lr: 0.000010  loss: 0.1805  time: 1.0081  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 600/7110]  eta: 1:53:38  lr: 0.000010  loss: 0.1174  time: 1.0087  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 650/7110]  eta: 1:52:34  lr: 0.000010  loss: 0.3023  time: 1.0440  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 700/7110]  eta: 1:51:01  lr: 0.000010  loss: 0.1065  time: 0.9653  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 750/7110]  eta: 1:49:44  lr: 0.000010  loss: 0.7125  time: 1.0019  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 800/7110]  eta: 1:48:41  lr: 0.000010  loss: 0.2681  time: 1.0294  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 850/7110]  eta: 1:47:37  lr: 0.000010  loss: 0.3456  time: 1.0044  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 900/7110]  eta: 1:46:38  lr: 0.000010  loss: 0.2432  time: 0.9982  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [ 950/7110]  eta: 1:45:39  lr: 0.000010  loss: 0.5332  time: 1.0468  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1000/7110]  eta: 1:44:43  lr: 0.000010  loss: 0.4456  time: 0.9801  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1050/7110]  eta: 1:43:59  lr: 0.000010  loss: 0.2539  time: 1.0522  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1100/7110]  eta: 1:42:59  lr: 0.000010  loss: 0.1588  time: 1.0152  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1150/7110]  eta: 1:41:59  lr: 0.000010  loss: 0.8881  time: 0.9589  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1200/7110]  eta: 1:41:01  lr: 0.000010  loss: 0.1863  time: 1.0177  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1250/7110]  eta: 1:40:05  lr: 0.000010  loss: 0.2463  time: 1.0040  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1300/7110]  eta: 1:39:09  lr: 0.000010  loss: 0.7233  time: 1.0566  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1350/7110]  eta: 1:38:15  lr: 0.000010  loss: 0.0676  time: 0.9998  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1400/7110]  eta: 1:37:14  lr: 0.000010  loss: 0.3626  time: 0.9543  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1450/7110]  eta: 1:36:17  lr: 0.000010  loss: 0.3349  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1500/7110]  eta: 1:35:22  lr: 0.000010  loss: 0.4001  time: 0.9995  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1550/7110]  eta: 1:34:29  lr: 0.000010  loss: 0.5316  time: 0.9945  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1600/7110]  eta: 1:33:40  lr: 0.000010  loss: 0.0448  time: 1.0001  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1650/7110]  eta: 1:32:46  lr: 0.000010  loss: 0.5687  time: 1.0076  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1700/7110]  eta: 1:31:52  lr: 0.000010  loss: 0.3681  time: 1.0147  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1750/7110]  eta: 1:30:57  lr: 0.000010  loss: 0.1337  time: 0.9430  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1800/7110]  eta: 1:30:11  lr: 0.000010  loss: 0.2193  time: 1.0172  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1850/7110]  eta: 1:29:17  lr: 0.000010  loss: 0.0493  time: 0.9967  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1900/7110]  eta: 1:28:26  lr: 0.000010  loss: 0.8748  time: 0.9922  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [1950/7110]  eta: 1:27:33  lr: 0.000010  loss: 0.3259  time: 1.0785  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2000/7110]  eta: 1:26:42  lr: 0.000010  loss: 0.0507  time: 1.0535  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2050/7110]  eta: 1:25:49  lr: 0.000010  loss: 0.4278  time: 0.9939  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2100/7110]  eta: 1:24:54  lr: 0.000010  loss: 0.0575  time: 1.0027  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2150/7110]  eta: 1:24:02  lr: 0.000010  loss: 0.0734  time: 1.0022  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2200/7110]  eta: 1:23:10  lr: 0.000010  loss: 0.3744  time: 0.9792  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2250/7110]  eta: 1:22:18  lr: 0.000010  loss: 0.0778  time: 1.0058  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2300/7110]  eta: 1:21:25  lr: 0.000010  loss: 0.1645  time: 1.0118  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2350/7110]  eta: 1:20:31  lr: 0.000010  loss: 0.2344  time: 0.9618  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2400/7110]  eta: 1:19:38  lr: 0.000010  loss: 0.0925  time: 0.9538  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2450/7110]  eta: 1:18:48  lr: 0.000010  loss: 0.4848  time: 1.0112  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2500/7110]  eta: 1:17:54  lr: 0.000010  loss: 0.3100  time: 1.0010  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2550/7110]  eta: 1:17:03  lr: 0.000010  loss: 0.1640  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2600/7110]  eta: 1:16:09  lr: 0.000010  loss: 0.1029  time: 0.9402  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2650/7110]  eta: 1:15:14  lr: 0.000010  loss: 0.1627  time: 0.9705  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2700/7110]  eta: 1:14:22  lr: 0.000010  loss: 0.2610  time: 0.9697  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2750/7110]  eta: 1:13:30  lr: 0.000010  loss: 0.1233  time: 0.9824  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2800/7110]  eta: 1:12:37  lr: 0.000010  loss: 0.3956  time: 0.9863  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2850/7110]  eta: 1:11:47  lr: 0.000010  loss: 0.1926  time: 1.0123  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2900/7110]  eta: 1:10:58  lr: 0.000010  loss: 0.3593  time: 1.0007  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [2950/7110]  eta: 1:10:07  lr: 0.000010  loss: 1.7420  time: 1.0257  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3000/7110]  eta: 1:09:16  lr: 0.000010  loss: 1.1103  time: 0.9678  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3050/7110]  eta: 1:08:25  lr: 0.000010  loss: 0.1512  time: 1.0474  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3100/7110]  eta: 1:07:34  lr: 0.000010  loss: 0.2084  time: 1.0299  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3150/7110]  eta: 1:06:41  lr: 0.000010  loss: 0.1859  time: 0.9838  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3200/7110]  eta: 1:05:48  lr: 0.000010  loss: 0.0998  time: 1.0074  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3250/7110]  eta: 1:04:59  lr: 0.000010  loss: 0.1064  time: 1.0399  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3300/7110]  eta: 1:04:09  lr: 0.000010  loss: 0.1013  time: 0.9784  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3350/7110]  eta: 1:03:18  lr: 0.000010  loss: 0.1405  time: 0.9869  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3400/7110]  eta: 1:02:26  lr: 0.000010  loss: 0.2487  time: 0.9740  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3450/7110]  eta: 1:01:36  lr: 0.000010  loss: 0.4173  time: 1.0436  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3500/7110]  eta: 1:00:44  lr: 0.000010  loss: 0.3533  time: 0.9896  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3550/7110]  eta: 0:59:53  lr: 0.000010  loss: 0.2065  time: 0.9607  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3600/7110]  eta: 0:59:01  lr: 0.000010  loss: 0.0529  time: 0.9678  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3650/7110]  eta: 0:58:09  lr: 0.000010  loss: 0.2685  time: 0.9661  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3700/7110]  eta: 0:57:19  lr: 0.000010  loss: 0.1755  time: 0.9926  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3750/7110]  eta: 0:56:27  lr: 0.000010  loss: 0.3826  time: 0.9850  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3800/7110]  eta: 0:55:36  lr: 0.000010  loss: 0.3740  time: 1.0349  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3850/7110]  eta: 0:54:43  lr: 0.000010  loss: 0.8009  time: 0.9220  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3900/7110]  eta: 0:53:53  lr: 0.000010  loss: 0.1657  time: 0.9906  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [3950/7110]  eta: 0:53:03  lr: 0.000010  loss: 0.2669  time: 1.0431  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4000/7110]  eta: 0:52:13  lr: 0.000010  loss: 0.0891  time: 1.0314  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4050/7110]  eta: 0:51:23  lr: 0.000010  loss: 0.5818  time: 1.0036  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4100/7110]  eta: 0:50:34  lr: 0.000010  loss: 0.4891  time: 1.0637  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4150/7110]  eta: 0:49:43  lr: 0.000010  loss: 0.1378  time: 1.0309  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4200/7110]  eta: 0:48:53  lr: 0.000010  loss: 0.3465  time: 1.0312  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4250/7110]  eta: 0:48:02  lr: 0.000010  loss: 0.2066  time: 0.9696  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4300/7110]  eta: 0:47:11  lr: 0.000010  loss: 0.1945  time: 0.9953  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4350/7110]  eta: 0:46:20  lr: 0.000010  loss: 0.1136  time: 0.9827  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4400/7110]  eta: 0:45:29  lr: 0.000010  loss: 0.1217  time: 0.9244  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4450/7110]  eta: 0:44:39  lr: 0.000010  loss: 1.5069  time: 0.9985  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4500/7110]  eta: 0:43:48  lr: 0.000010  loss: 0.3831  time: 0.9172  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4550/7110]  eta: 0:42:58  lr: 0.000010  loss: 0.2467  time: 1.0458  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4600/7110]  eta: 0:42:07  lr: 0.000010  loss: 0.2102  time: 0.9769  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4650/7110]  eta: 0:41:17  lr: 0.000010  loss: 0.3397  time: 1.0527  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4700/7110]  eta: 0:40:26  lr: 0.000010  loss: 0.1150  time: 0.9733  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4750/7110]  eta: 0:39:36  lr: 0.000010  loss: 0.1370  time: 1.0245  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4800/7110]  eta: 0:38:46  lr: 0.000010  loss: 0.4525  time: 1.0408  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4850/7110]  eta: 0:37:56  lr: 0.000010  loss: 0.3265  time: 1.0106  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4900/7110]  eta: 0:37:05  lr: 0.000010  loss: 0.4626  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [4950/7110]  eta: 0:36:14  lr: 0.000010  loss: 0.1580  time: 0.9484  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5000/7110]  eta: 0:35:24  lr: 0.000010  loss: 0.3406  time: 1.0170  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5050/7110]  eta: 0:34:33  lr: 0.000010  loss: 0.2791  time: 1.0040  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5100/7110]  eta: 0:33:43  lr: 0.000010  loss: 0.0808  time: 0.9993  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5150/7110]  eta: 0:32:52  lr: 0.000010  loss: 0.1261  time: 0.9991  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5200/7110]  eta: 0:32:02  lr: 0.000010  loss: 0.4292  time: 0.9304  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5250/7110]  eta: 0:31:11  lr: 0.000010  loss: 0.0669  time: 0.9967  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5300/7110]  eta: 0:30:21  lr: 0.000010  loss: 0.8256  time: 0.9610  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5350/7110]  eta: 0:29:31  lr: 0.000010  loss: 0.4180  time: 0.9453  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5400/7110]  eta: 0:28:40  lr: 0.000010  loss: 0.5948  time: 0.9486  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5450/7110]  eta: 0:27:49  lr: 0.000010  loss: 0.4908  time: 1.0689  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5500/7110]  eta: 0:26:59  lr: 0.000010  loss: 0.1581  time: 1.0479  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5550/7110]  eta: 0:26:09  lr: 0.000010  loss: 0.1894  time: 1.0004  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5600/7110]  eta: 0:25:18  lr: 0.000010  loss: 0.2384  time: 1.0172  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5650/7110]  eta: 0:24:28  lr: 0.000010  loss: 1.6581  time: 1.0254  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5700/7110]  eta: 0:23:38  lr: 0.000010  loss: 0.1541  time: 1.0109  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5750/7110]  eta: 0:22:48  lr: 0.000010  loss: 0.1907  time: 1.0481  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5800/7110]  eta: 0:21:57  lr: 0.000010  loss: 0.0828  time: 0.9643  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5850/7110]  eta: 0:21:07  lr: 0.000010  loss: 0.2156  time: 0.9786  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5900/7110]  eta: 0:20:17  lr: 0.000010  loss: 0.1258  time: 0.9975  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [5950/7110]  eta: 0:19:26  lr: 0.000010  loss: 0.2484  time: 0.9863  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6000/7110]  eta: 0:18:36  lr: 0.000010  loss: 0.2937  time: 1.0069  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6050/7110]  eta: 0:17:45  lr: 0.000010  loss: 0.9476  time: 0.9842  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6100/7110]  eta: 0:16:55  lr: 0.000010  loss: 0.7314  time: 1.0155  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6150/7110]  eta: 0:16:05  lr: 0.000010  loss: 0.1424  time: 1.0337  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6200/7110]  eta: 0:15:15  lr: 0.000010  loss: 0.1590  time: 1.0124  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6250/7110]  eta: 0:14:25  lr: 0.000010  loss: 0.1369  time: 1.0325  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6300/7110]  eta: 0:13:34  lr: 0.000010  loss: 0.3036  time: 1.0612  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6350/7110]  eta: 0:12:44  lr: 0.000010  loss: 0.3109  time: 0.9504  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6400/7110]  eta: 0:11:54  lr: 0.000010  loss: 0.5345  time: 0.9805  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6450/7110]  eta: 0:11:03  lr: 0.000010  loss: 0.1217  time: 0.9540  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6500/7110]  eta: 0:10:13  lr: 0.000010  loss: 0.0134  time: 1.0127  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6550/7110]  eta: 0:09:23  lr: 0.000010  loss: 0.1946  time: 1.0133  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6600/7110]  eta: 0:08:32  lr: 0.000010  loss: 0.8997  time: 1.0898  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6650/7110]  eta: 0:07:42  lr: 0.000010  loss: 0.5724  time: 1.0379  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6700/7110]  eta: 0:06:52  lr: 0.000010  loss: 0.2645  time: 0.9468  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6750/7110]  eta: 0:06:02  lr: 0.000010  loss: 0.1058  time: 1.0911  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.1847  time: 0.9395  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6850/7110]  eta: 0:04:21  lr: 0.000010  loss: 0.1763  time: 1.0334  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6900/7110]  eta: 0:03:31  lr: 0.000010  loss: 0.3070  time: 0.9365  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.0877  time: 0.9601  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.1778  time: 0.9542  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.4062  time: 0.9971  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1646  time: 0.9485  data: 0.0000  max mem: 66110
Train: data epoch: [12]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.5493  time: 1.0516  data: 0.0000  max mem: 66110
Train: data epoch: [12] Total time: 1:59:07 (1.0052 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:13:13    time: 20.4881  data: 19.2497  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:51    time: 3.1505  data: 1.7509  max mem: 66110
Evaluation  [  20/1093]  eta: 0:40:43    time: 1.3665  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:33:59    time: 1.2414  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:25    time: 1.2802  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:29:54    time: 1.4120  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:27:55    time: 1.2774  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:15    time: 1.2903  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:27    time: 1.3979  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:25:49    time: 1.3510  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:27    time: 1.4195  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:08    time: 1.4912  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:32    time: 1.3928  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:19    time: 1.4060  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:42    time: 1.3638  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:14    time: 1.2377  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:55    time: 1.3473  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:29    time: 1.3402  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:12    time: 1.3385  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:21:57    time: 1.4278  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:35    time: 1.3686  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:31    time: 1.5042  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:06    time: 1.4566  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:46    time: 1.2468  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:29    time: 1.3402  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:15    time: 1.4101  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:09    time: 1.5681  data: 0.0009  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:47    time: 1.4615  data: 0.0009  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:30    time: 1.2827  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:22    time: 1.5147  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:00    time: 1.4206  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:42    time: 1.2201  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:28    time: 1.3684  data: 0.0009  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:10    time: 1.3646  data: 0.0009  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:58    time: 1.4031  data: 0.0009  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:43    time: 1.4769  data: 0.0011  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:25    time: 1.3180  data: 0.0011  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:08    time: 1.2533  data: 0.0011  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:57    time: 1.4552  data: 0.0011  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:43    time: 1.5112  data: 0.0011  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:29    time: 1.4504  data: 0.0011  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:14    time: 1.4287  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:01    time: 1.4303  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:45    time: 1.4249  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:27    time: 1.2591  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:13    time: 1.2753  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:57    time: 1.3692  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:41    time: 1.3150  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:28    time: 1.3671  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:16    time: 1.5182  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:04    time: 1.6076  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:51    time: 1.5711  data: 0.0011  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:37    time: 1.4794  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:23    time: 1.4701  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:09    time: 1.4811  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:55    time: 1.4546  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:38    time: 1.2591  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:22    time: 1.2036  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:08    time: 1.3467  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:51    time: 1.2675  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:39    time: 1.3807  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:25    time: 1.5133  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:10    time: 1.4013  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:56    time: 1.3963  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:41    time: 1.3658  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:28    time: 1.4406  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:13    time: 1.4169  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:59    time: 1.3403  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:44    time: 1.4092  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:31    time: 1.4733  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:16    time: 1.4382  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:01    time: 1.2454  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:45    time: 1.1768  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:30    time: 1.1409  data: 0.0011  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:15    time: 1.1393  data: 0.0011  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:00    time: 1.2790  data: 0.0011  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:46    time: 1.3729  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:32    time: 1.4016  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:19    time: 1.4609  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:03    time: 1.2890  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:50    time: 1.3659  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:37    time: 1.5446  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:23    time: 1.4516  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:09    time: 1.5271  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:55    time: 1.4206  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:41    time: 1.3245  data: 0.0011  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:26    time: 1.3481  data: 0.0012  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:12    time: 1.3647  data: 0.0012  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:59    time: 1.5025  data: 0.0011  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:44    time: 1.3525  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:30    time: 1.2824  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:16    time: 1.4318  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:02    time: 1.4433  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:48    time: 1.3875  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:34    time: 1.3199  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:20    time: 1.3147  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:06    time: 1.3937  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:52    time: 1.4448  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:38    time: 1.3573  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.3719  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:10    time: 1.4627  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4852  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3363  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1663  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2587  data: 0.0009  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3534  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3943  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4518  data: 0.0011  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3851  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3394  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3129  data: 0.0419  max mem: 66110
Evaluation Total time: 0:25:27 (1.3971 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_12_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [13]  [   0/7110]  eta: 2 days, 6:40:57  lr: 0.000010  loss: 0.0822  time: 27.6875  data: 0.0001  max mem: 66110
Train: data epoch: [13]  [  50/7110]  eta: 3:00:18  lr: 0.000010  loss: 0.5364  time: 0.9900  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 100/7110]  eta: 2:25:28  lr: 0.000010  loss: 0.4250  time: 0.9726  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 150/7110]  eta: 2:15:16  lr: 0.000010  loss: 0.5265  time: 0.9607  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 200/7110]  eta: 2:10:24  lr: 0.000010  loss: 0.5051  time: 1.0022  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 250/7110]  eta: 2:06:37  lr: 0.000010  loss: 0.7035  time: 0.9529  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 300/7110]  eta: 2:03:46  lr: 0.000010  loss: 1.0205  time: 1.0207  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 350/7110]  eta: 2:01:39  lr: 0.000010  loss: 0.5056  time: 0.9917  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 400/7110]  eta: 1:59:30  lr: 0.000010  loss: 0.3492  time: 1.0047  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 450/7110]  eta: 1:57:44  lr: 0.000010  loss: 0.5933  time: 1.0145  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 500/7110]  eta: 1:56:39  lr: 0.000010  loss: 0.2358  time: 1.0914  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 550/7110]  eta: 1:54:58  lr: 0.000010  loss: 0.1620  time: 0.9466  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 600/7110]  eta: 1:53:28  lr: 0.000010  loss: 0.4374  time: 1.0237  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 650/7110]  eta: 1:52:02  lr: 0.000010  loss: 0.6073  time: 0.9792  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 700/7110]  eta: 1:50:37  lr: 0.000010  loss: 0.2398  time: 0.9709  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 750/7110]  eta: 1:49:32  lr: 0.000010  loss: 0.1604  time: 1.0439  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 800/7110]  eta: 1:48:26  lr: 0.000010  loss: 0.4563  time: 0.9688  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 850/7110]  eta: 1:47:24  lr: 0.000010  loss: 1.3859  time: 1.0656  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 900/7110]  eta: 1:46:24  lr: 0.000010  loss: 0.0405  time: 0.9973  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [ 950/7110]  eta: 1:45:15  lr: 0.000010  loss: 0.8736  time: 1.0057  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1000/7110]  eta: 1:44:18  lr: 0.000010  loss: 0.1152  time: 1.0318  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1050/7110]  eta: 1:43:19  lr: 0.000010  loss: 0.5685  time: 1.0329  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1100/7110]  eta: 1:42:13  lr: 0.000010  loss: 0.4703  time: 1.0063  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1150/7110]  eta: 1:41:22  lr: 0.000010  loss: 1.7556  time: 1.0447  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1200/7110]  eta: 1:40:24  lr: 0.000010  loss: 0.4743  time: 0.9650  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1250/7110]  eta: 1:39:23  lr: 0.000010  loss: 0.1442  time: 0.9483  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1300/7110]  eta: 1:38:36  lr: 0.000010  loss: 0.0936  time: 1.0661  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1350/7110]  eta: 1:37:46  lr: 0.000010  loss: 0.3968  time: 0.9954  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1400/7110]  eta: 1:36:48  lr: 0.000010  loss: 1.1898  time: 0.9737  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1450/7110]  eta: 1:35:52  lr: 0.000010  loss: 0.1122  time: 1.0446  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1500/7110]  eta: 1:35:06  lr: 0.000010  loss: 0.1526  time: 1.0315  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1550/7110]  eta: 1:34:07  lr: 0.000010  loss: 0.3766  time: 0.9751  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1600/7110]  eta: 1:33:14  lr: 0.000010  loss: 0.0800  time: 0.9949  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1650/7110]  eta: 1:32:16  lr: 0.000010  loss: 0.4351  time: 1.0182  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1700/7110]  eta: 1:31:18  lr: 0.000010  loss: 0.0364  time: 0.9967  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1750/7110]  eta: 1:30:27  lr: 0.000010  loss: 0.0690  time: 1.0135  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1800/7110]  eta: 1:29:37  lr: 0.000010  loss: 0.2760  time: 1.0264  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1850/7110]  eta: 1:28:37  lr: 0.000010  loss: 0.1416  time: 0.9640  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1900/7110]  eta: 1:27:43  lr: 0.000010  loss: 0.0103  time: 0.9877  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [1950/7110]  eta: 1:26:51  lr: 0.000010  loss: 0.3668  time: 1.0053  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2000/7110]  eta: 1:25:56  lr: 0.000010  loss: 0.0532  time: 0.9698  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2050/7110]  eta: 1:25:04  lr: 0.000010  loss: 0.4015  time: 1.0069  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2100/7110]  eta: 1:24:09  lr: 0.000010  loss: 0.1714  time: 0.9522  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2150/7110]  eta: 1:23:21  lr: 0.000010  loss: 0.3004  time: 1.0367  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2200/7110]  eta: 1:22:29  lr: 0.000010  loss: 0.1615  time: 0.9560  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2250/7110]  eta: 1:21:38  lr: 0.000010  loss: 0.6673  time: 1.0071  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2300/7110]  eta: 1:20:45  lr: 0.000010  loss: 0.3726  time: 1.0007  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2350/7110]  eta: 1:19:54  lr: 0.000010  loss: 1.7758  time: 0.9986  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2400/7110]  eta: 1:19:10  lr: 0.000010  loss: 0.2310  time: 1.0807  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2450/7110]  eta: 1:18:20  lr: 0.000010  loss: 0.5102  time: 0.9908  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2500/7110]  eta: 1:17:29  lr: 0.000010  loss: 0.2146  time: 1.0183  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2550/7110]  eta: 1:16:36  lr: 0.000010  loss: 0.8310  time: 0.9678  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2600/7110]  eta: 1:15:47  lr: 0.000010  loss: 0.1346  time: 0.9913  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2650/7110]  eta: 1:14:55  lr: 0.000010  loss: 0.0650  time: 1.0098  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2700/7110]  eta: 1:14:03  lr: 0.000010  loss: 0.1488  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2750/7110]  eta: 1:13:11  lr: 0.000010  loss: 0.1067  time: 0.9709  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2800/7110]  eta: 1:12:20  lr: 0.000010  loss: 0.4325  time: 0.9661  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2850/7110]  eta: 1:11:32  lr: 0.000010  loss: 0.4231  time: 0.9930  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2900/7110]  eta: 1:10:44  lr: 0.000010  loss: 0.1021  time: 1.0899  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [2950/7110]  eta: 1:09:52  lr: 0.000010  loss: 0.3396  time: 0.9721  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3000/7110]  eta: 1:09:02  lr: 0.000010  loss: 0.4643  time: 1.0082  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3050/7110]  eta: 1:08:09  lr: 0.000010  loss: 0.3289  time: 0.9203  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3100/7110]  eta: 1:07:19  lr: 0.000010  loss: 0.1845  time: 0.9791  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3150/7110]  eta: 1:06:29  lr: 0.000010  loss: 0.3881  time: 1.0096  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3200/7110]  eta: 1:05:38  lr: 0.000010  loss: 0.4134  time: 1.0257  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3250/7110]  eta: 1:04:46  lr: 0.000010  loss: 0.2658  time: 0.9539  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3300/7110]  eta: 1:03:55  lr: 0.000010  loss: 0.1109  time: 0.9666  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3350/7110]  eta: 1:03:02  lr: 0.000010  loss: 0.3790  time: 0.9910  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3400/7110]  eta: 1:02:11  lr: 0.000010  loss: 0.1963  time: 0.9531  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3450/7110]  eta: 1:01:19  lr: 0.000010  loss: 0.1730  time: 0.9942  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3500/7110]  eta: 1:00:28  lr: 0.000010  loss: 0.2293  time: 1.0499  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3550/7110]  eta: 0:59:35  lr: 0.000010  loss: 0.1912  time: 1.0188  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3600/7110]  eta: 0:58:46  lr: 0.000010  loss: 0.1016  time: 1.0073  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3650/7110]  eta: 0:57:55  lr: 0.000010  loss: 0.2194  time: 0.9890  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3700/7110]  eta: 0:57:05  lr: 0.000010  loss: 0.0853  time: 1.0436  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3750/7110]  eta: 0:56:14  lr: 0.000010  loss: 0.0542  time: 0.9599  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3800/7110]  eta: 0:55:24  lr: 0.000010  loss: 0.3393  time: 1.0365  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3850/7110]  eta: 0:54:34  lr: 0.000010  loss: 0.0474  time: 1.0220  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3900/7110]  eta: 0:53:43  lr: 0.000010  loss: 0.5499  time: 0.9974  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [3950/7110]  eta: 0:52:52  lr: 0.000010  loss: 0.2668  time: 0.9490  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4000/7110]  eta: 0:52:02  lr: 0.000010  loss: 0.0941  time: 0.9433  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4050/7110]  eta: 0:51:12  lr: 0.000010  loss: 0.5180  time: 0.9775  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4100/7110]  eta: 0:50:20  lr: 0.000010  loss: 0.2297  time: 0.9819  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4150/7110]  eta: 0:49:30  lr: 0.000010  loss: 0.0675  time: 1.0264  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4200/7110]  eta: 0:48:40  lr: 0.000010  loss: 0.1822  time: 0.9576  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4250/7110]  eta: 0:47:49  lr: 0.000010  loss: 0.4060  time: 0.9908  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4300/7110]  eta: 0:47:01  lr: 0.000010  loss: 0.3550  time: 1.0368  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4350/7110]  eta: 0:46:11  lr: 0.000010  loss: 0.6442  time: 1.0182  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4400/7110]  eta: 0:45:22  lr: 0.000010  loss: 0.6325  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4450/7110]  eta: 0:44:32  lr: 0.000010  loss: 1.2571  time: 1.0636  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4500/7110]  eta: 0:43:42  lr: 0.000010  loss: 0.6080  time: 0.9685  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4550/7110]  eta: 0:42:51  lr: 0.000010  loss: 0.0995  time: 0.9598  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4600/7110]  eta: 0:42:00  lr: 0.000010  loss: 0.2995  time: 1.0008  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4650/7110]  eta: 0:41:08  lr: 0.000010  loss: 0.3593  time: 0.9722  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4700/7110]  eta: 0:40:18  lr: 0.000010  loss: 0.4448  time: 0.9942  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4750/7110]  eta: 0:39:28  lr: 0.000010  loss: 0.1384  time: 1.0830  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4800/7110]  eta: 0:38:38  lr: 0.000010  loss: 0.1051  time: 1.0317  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4850/7110]  eta: 0:37:48  lr: 0.000010  loss: 0.2655  time: 1.0011  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4900/7110]  eta: 0:36:58  lr: 0.000010  loss: 0.1342  time: 1.0278  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [4950/7110]  eta: 0:36:09  lr: 0.000010  loss: 0.5274  time: 1.0782  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5000/7110]  eta: 0:35:18  lr: 0.000010  loss: 0.1288  time: 0.9694  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5050/7110]  eta: 0:34:28  lr: 0.000010  loss: 0.0756  time: 1.0053  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5100/7110]  eta: 0:33:37  lr: 0.000010  loss: 1.8833  time: 1.0196  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5150/7110]  eta: 0:32:46  lr: 0.000010  loss: 0.3776  time: 0.9804  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5200/7110]  eta: 0:31:56  lr: 0.000010  loss: 0.1719  time: 1.0163  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5250/7110]  eta: 0:31:06  lr: 0.000010  loss: 0.5437  time: 1.0190  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5300/7110]  eta: 0:30:15  lr: 0.000010  loss: 0.4437  time: 0.9599  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5350/7110]  eta: 0:29:25  lr: 0.000010  loss: 0.0575  time: 1.0461  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5400/7110]  eta: 0:28:34  lr: 0.000010  loss: 0.2125  time: 1.0069  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5450/7110]  eta: 0:27:45  lr: 0.000010  loss: 0.2446  time: 1.0652  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5500/7110]  eta: 0:26:54  lr: 0.000010  loss: 0.1014  time: 1.0082  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5550/7110]  eta: 0:26:04  lr: 0.000010  loss: 0.3193  time: 1.0110  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5600/7110]  eta: 0:25:14  lr: 0.000010  loss: 0.5746  time: 1.0558  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5650/7110]  eta: 0:24:24  lr: 0.000010  loss: 0.0934  time: 0.9863  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.1149  time: 0.9837  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5750/7110]  eta: 0:22:43  lr: 0.000010  loss: 0.3181  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5800/7110]  eta: 0:21:53  lr: 0.000010  loss: 0.4144  time: 0.9844  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.1280  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.4230  time: 1.0213  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.3205  time: 1.0560  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.2627  time: 1.0186  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.3372  time: 1.0749  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.4259  time: 0.9946  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.2623  time: 0.9955  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.2068  time: 1.0007  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.4007  time: 1.0241  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.1881  time: 1.0023  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.4292  time: 1.0051  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.3060  time: 1.0128  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.1120  time: 1.0785  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.7100  time: 1.0756  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.2944  time: 1.0256  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.3616  time: 1.0544  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.3099  time: 0.9711  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.9082  time: 0.9770  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.4648  time: 0.9858  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.2335  time: 0.9646  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.0595  time: 1.0086  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.4951  time: 1.0213  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.4226  time: 1.0571  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.1724  time: 0.9654  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1622  time: 0.9743  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.0632  time: 0.9663  data: 0.0000  max mem: 66110
Train: data epoch: [13]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.1798  time: 1.0895  data: 0.0000  max mem: 66110
Train: data epoch: [13] Total time: 1:58:53 (1.0034 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:15:12    time: 20.5974  data: 19.3414  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:55    time: 3.1538  data: 1.7594  max mem: 66110
Evaluation  [  20/1093]  eta: 0:40:54    time: 1.3719  data: 0.0012  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:30    time: 1.2838  data: 0.0012  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:45    time: 1.3087  data: 0.0011  max mem: 66110
Evaluation  [  50/1093]  eta: 0:29:58    time: 1.3796  data: 0.0011  max mem: 66110
Evaluation  [  60/1093]  eta: 0:27:46    time: 1.2090  data: 0.0011  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:06    time: 1.2446  data: 0.0011  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:18    time: 1.3930  data: 0.0011  max mem: 66110
Evaluation  [  90/1093]  eta: 0:25:42    time: 1.3568  data: 0.0011  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:18    time: 1.4084  data: 0.0012  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:24:57    time: 1.4545  data: 0.0011  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:22    time: 1.3749  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:08    time: 1.3991  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:33    time: 1.3597  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:08    time: 1.2672  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:50    time: 1.3667  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:24    time: 1.3347  data: 0.0011  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:07    time: 1.3405  data: 0.0011  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:21:51    time: 1.4202  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:28    time: 1.3398  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:22    time: 1.4491  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:20:57    time: 1.4160  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:37    time: 1.2314  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:20    time: 1.3161  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:05    time: 1.3905  data: 0.0011  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:19:58    time: 1.5473  data: 0.0011  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:37    time: 1.4420  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:18    time: 1.2341  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:11    time: 1.4735  data: 0.0011  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:49    time: 1.4237  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:31    time: 1.2220  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:15    time: 1.3119  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:17:59    time: 1.3284  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:47    time: 1.4201  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:33    time: 1.4720  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:15    time: 1.3183  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:16:59    time: 1.2693  data: 0.0011  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:49    time: 1.4725  data: 0.0011  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:33    time: 1.4819  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:20    time: 1.4188  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:06    time: 1.4296  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:53    time: 1.4325  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:37    time: 1.4209  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:19    time: 1.2511  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:06    time: 1.2881  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:51    time: 1.3814  data: 0.0011  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:35    time: 1.3226  data: 0.0011  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:21    time: 1.3691  data: 0.0011  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:09    time: 1.5073  data: 0.0012  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:58    time: 1.5937  data: 0.0011  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:45    time: 1.5588  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:30    time: 1.4590  data: 0.0011  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:17    time: 1.4505  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:03    time: 1.4705  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:49    time: 1.4317  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:32    time: 1.2550  data: 0.0012  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:17    time: 1.2092  data: 0.0011  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:02    time: 1.3426  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:46    time: 1.2610  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:33    time: 1.3525  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:19    time: 1.4821  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:05    time: 1.3979  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:51    time: 1.3979  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:36    time: 1.3689  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:23    time: 1.3954  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:08    time: 1.3595  data: 0.0011  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:54    time: 1.3395  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:40    time: 1.4189  data: 0.0009  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:26    time: 1.4665  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:12    time: 1.4259  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:56    time: 1.2422  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:41    time: 1.1847  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:26    time: 1.1649  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:11    time: 1.1478  data: 0.0011  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:57    time: 1.2687  data: 0.0012  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:43    time: 1.3691  data: 0.0011  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:29    time: 1.3973  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:16    time: 1.4570  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:00    time: 1.2668  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:47    time: 1.3378  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:33    time: 1.5125  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:20    time: 1.4231  data: 0.0009  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:06    time: 1.5169  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:52    time: 1.3817  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:38    time: 1.3084  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:24    time: 1.3496  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:10    time: 1.3528  data: 0.0011  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:56    time: 1.5103  data: 0.0011  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:42    time: 1.3437  data: 0.0011  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:28    time: 1.2615  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:14    time: 1.4320  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:00    time: 1.4436  data: 0.0011  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:46    time: 1.3984  data: 0.0012  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:32    time: 1.3429  data: 0.0012  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:18    time: 1.3170  data: 0.0011  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.3883  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4461  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3497  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3618  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4658  data: 0.0011  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4424  data: 0.0012  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.3278  data: 0.0011  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.2277  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2829  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3493  data: 0.0011  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3889  data: 0.0012  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4409  data: 0.0012  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3768  data: 0.0011  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3455  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3187  data: 0.0447  max mem: 66110
Evaluation Total time: 0:25:17 (1.3885 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_13_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [14]  [   0/7110]  eta: 2 days, 5:47:38  lr: 0.000010  loss: 0.3802  time: 27.2375  data: 0.0004  max mem: 66110
Train: data epoch: [14]  [  50/7110]  eta: 2:54:21  lr: 0.000010  loss: 0.0300  time: 0.9361  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 100/7110]  eta: 2:26:11  lr: 0.000010  loss: 0.3364  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 150/7110]  eta: 2:15:59  lr: 0.000010  loss: 0.3071  time: 1.0411  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 200/7110]  eta: 2:09:18  lr: 0.000010  loss: 0.1408  time: 1.0032  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 250/7110]  eta: 2:05:21  lr: 0.000010  loss: 0.5032  time: 1.0104  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 300/7110]  eta: 2:02:41  lr: 0.000010  loss: 0.1153  time: 1.0174  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 350/7110]  eta: 2:01:19  lr: 0.000010  loss: 0.0729  time: 1.0564  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 400/7110]  eta: 1:59:01  lr: 0.000010  loss: 0.2745  time: 0.9741  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 450/7110]  eta: 1:57:41  lr: 0.000010  loss: 0.6564  time: 1.0337  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 500/7110]  eta: 1:55:32  lr: 0.000010  loss: 0.2837  time: 0.9600  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 550/7110]  eta: 1:54:01  lr: 0.000010  loss: 0.4191  time: 1.0203  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 600/7110]  eta: 1:52:39  lr: 0.000010  loss: 0.0634  time: 1.0034  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 650/7110]  eta: 1:51:33  lr: 0.000010  loss: 0.1709  time: 0.9781  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 700/7110]  eta: 1:50:41  lr: 0.000010  loss: 0.2621  time: 1.0163  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 750/7110]  eta: 1:49:27  lr: 0.000010  loss: 0.7964  time: 0.9889  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 800/7110]  eta: 1:48:37  lr: 0.000010  loss: 0.1631  time: 1.0326  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 850/7110]  eta: 1:47:30  lr: 0.000010  loss: 0.2299  time: 1.0122  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 900/7110]  eta: 1:46:19  lr: 0.000010  loss: 0.7810  time: 0.9721  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [ 950/7110]  eta: 1:45:16  lr: 0.000010  loss: 0.0810  time: 1.0066  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1000/7110]  eta: 1:44:11  lr: 0.000010  loss: 0.2694  time: 0.9712  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1050/7110]  eta: 1:43:14  lr: 0.000010  loss: 1.6667  time: 1.0010  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1100/7110]  eta: 1:42:14  lr: 0.000010  loss: 0.0653  time: 1.0178  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1150/7110]  eta: 1:41:25  lr: 0.000010  loss: 1.7079  time: 1.0169  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1200/7110]  eta: 1:40:26  lr: 0.000010  loss: 0.5435  time: 0.9564  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1250/7110]  eta: 1:39:30  lr: 0.000010  loss: 1.3835  time: 1.0241  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1300/7110]  eta: 1:38:30  lr: 0.000010  loss: 0.2179  time: 1.0080  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1350/7110]  eta: 1:37:35  lr: 0.000010  loss: 0.3479  time: 0.9948  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1400/7110]  eta: 1:36:43  lr: 0.000010  loss: 0.6773  time: 0.9944  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1450/7110]  eta: 1:35:47  lr: 0.000010  loss: 0.2183  time: 1.0242  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1500/7110]  eta: 1:34:59  lr: 0.000010  loss: 0.0800  time: 0.9974  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1550/7110]  eta: 1:34:09  lr: 0.000010  loss: 0.2814  time: 1.0417  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1600/7110]  eta: 1:33:16  lr: 0.000010  loss: 0.5893  time: 1.0231  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1650/7110]  eta: 1:32:27  lr: 0.000010  loss: 0.0757  time: 1.0257  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1700/7110]  eta: 1:31:37  lr: 0.000010  loss: 0.3714  time: 1.0044  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1750/7110]  eta: 1:30:44  lr: 0.000010  loss: 0.0633  time: 0.9995  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1800/7110]  eta: 1:29:47  lr: 0.000010  loss: 0.2803  time: 0.9538  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1850/7110]  eta: 1:28:55  lr: 0.000010  loss: 0.0539  time: 0.9745  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1900/7110]  eta: 1:28:03  lr: 0.000010  loss: 0.4009  time: 0.9808  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [1950/7110]  eta: 1:27:08  lr: 0.000010  loss: 0.1103  time: 1.0078  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2000/7110]  eta: 1:26:18  lr: 0.000010  loss: 0.1964  time: 1.0051  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2050/7110]  eta: 1:25:27  lr: 0.000010  loss: 0.1495  time: 0.9657  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2100/7110]  eta: 1:24:33  lr: 0.000010  loss: 0.0343  time: 0.9733  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2150/7110]  eta: 1:23:39  lr: 0.000010  loss: 0.3946  time: 0.9744  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2200/7110]  eta: 1:22:50  lr: 0.000010  loss: 0.3642  time: 0.9815  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2250/7110]  eta: 1:22:00  lr: 0.000010  loss: 0.1412  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2300/7110]  eta: 1:21:11  lr: 0.000010  loss: 0.1110  time: 1.0023  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2350/7110]  eta: 1:20:17  lr: 0.000010  loss: 0.0762  time: 0.9783  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2400/7110]  eta: 1:19:24  lr: 0.000010  loss: 0.3253  time: 0.9792  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2450/7110]  eta: 1:18:32  lr: 0.000010  loss: 0.2170  time: 0.9469  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2500/7110]  eta: 1:17:40  lr: 0.000010  loss: 0.1349  time: 0.9766  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2550/7110]  eta: 1:16:48  lr: 0.000010  loss: 0.0709  time: 0.9683  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2600/7110]  eta: 1:15:54  lr: 0.000010  loss: 0.3956  time: 0.9937  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2650/7110]  eta: 1:15:05  lr: 0.000010  loss: 0.1434  time: 1.0875  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2700/7110]  eta: 1:14:13  lr: 0.000010  loss: 0.1584  time: 1.0386  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2750/7110]  eta: 1:13:23  lr: 0.000010  loss: 0.4712  time: 0.9822  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2800/7110]  eta: 1:12:32  lr: 0.000010  loss: 0.5335  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2850/7110]  eta: 1:11:39  lr: 0.000010  loss: 0.1596  time: 1.0043  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2900/7110]  eta: 1:10:50  lr: 0.000010  loss: 0.5562  time: 0.9827  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [2950/7110]  eta: 1:09:58  lr: 0.000010  loss: 0.5544  time: 0.9964  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3000/7110]  eta: 1:09:09  lr: 0.000010  loss: 0.0943  time: 1.0186  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3050/7110]  eta: 1:08:17  lr: 0.000010  loss: 0.5098  time: 1.0332  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3100/7110]  eta: 1:07:25  lr: 0.000010  loss: 0.1497  time: 0.9578  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3150/7110]  eta: 1:06:35  lr: 0.000010  loss: 0.2034  time: 1.0398  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3200/7110]  eta: 1:05:43  lr: 0.000010  loss: 0.0740  time: 0.9277  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3250/7110]  eta: 1:04:52  lr: 0.000010  loss: 0.2834  time: 1.0038  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3300/7110]  eta: 1:03:57  lr: 0.000010  loss: 0.2225  time: 0.9386  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3350/7110]  eta: 1:03:07  lr: 0.000010  loss: 0.1406  time: 1.0728  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3400/7110]  eta: 1:02:16  lr: 0.000010  loss: 0.4555  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3450/7110]  eta: 1:01:27  lr: 0.000010  loss: 0.0489  time: 0.9918  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3500/7110]  eta: 1:00:38  lr: 0.000010  loss: 0.1557  time: 1.0787  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3550/7110]  eta: 0:59:47  lr: 0.000010  loss: 0.5310  time: 0.9806  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3600/7110]  eta: 0:58:57  lr: 0.000010  loss: 0.2765  time: 1.0051  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3650/7110]  eta: 0:58:05  lr: 0.000010  loss: 0.6054  time: 0.9797  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3700/7110]  eta: 0:57:15  lr: 0.000010  loss: 0.0532  time: 1.0320  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3750/7110]  eta: 0:56:23  lr: 0.000010  loss: 0.4103  time: 0.9346  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3800/7110]  eta: 0:55:31  lr: 0.000010  loss: 0.1327  time: 1.0369  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3850/7110]  eta: 0:54:41  lr: 0.000010  loss: 0.1509  time: 0.9783  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3900/7110]  eta: 0:53:51  lr: 0.000010  loss: 0.0959  time: 0.9720  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [3950/7110]  eta: 0:52:59  lr: 0.000010  loss: 0.7735  time: 0.9857  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4000/7110]  eta: 0:52:08  lr: 0.000010  loss: 0.2226  time: 1.0371  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4050/7110]  eta: 0:51:18  lr: 0.000010  loss: 0.4292  time: 0.9566  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4100/7110]  eta: 0:50:27  lr: 0.000010  loss: 0.0655  time: 1.0346  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4150/7110]  eta: 0:49:37  lr: 0.000010  loss: 0.3758  time: 1.0134  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4200/7110]  eta: 0:48:46  lr: 0.000010  loss: 0.1939  time: 0.9747  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4250/7110]  eta: 0:47:56  lr: 0.000010  loss: 0.3121  time: 0.9907  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4300/7110]  eta: 0:47:05  lr: 0.000010  loss: 0.1016  time: 1.0071  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4350/7110]  eta: 0:46:14  lr: 0.000010  loss: 0.2126  time: 0.9718  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4400/7110]  eta: 0:45:24  lr: 0.000010  loss: 0.5862  time: 1.0659  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4450/7110]  eta: 0:44:34  lr: 0.000010  loss: 0.0942  time: 0.9884  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4500/7110]  eta: 0:43:44  lr: 0.000010  loss: 0.0809  time: 1.0014  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4550/7110]  eta: 0:42:53  lr: 0.000010  loss: 0.5280  time: 0.9300  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4600/7110]  eta: 0:42:03  lr: 0.000010  loss: 0.0769  time: 0.9874  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4650/7110]  eta: 0:41:13  lr: 0.000010  loss: 0.1230  time: 1.0574  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4700/7110]  eta: 0:40:23  lr: 0.000010  loss: 0.6045  time: 1.0008  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4750/7110]  eta: 0:39:32  lr: 0.000010  loss: 0.7306  time: 0.9609  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4800/7110]  eta: 0:38:41  lr: 0.000010  loss: 0.0807  time: 0.9481  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4850/7110]  eta: 0:37:50  lr: 0.000010  loss: 0.1361  time: 0.9499  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4900/7110]  eta: 0:37:00  lr: 0.000010  loss: 0.3213  time: 0.9799  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [4950/7110]  eta: 0:36:10  lr: 0.000010  loss: 0.2969  time: 1.0019  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5000/7110]  eta: 0:35:20  lr: 0.000010  loss: 0.4630  time: 1.0497  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5050/7110]  eta: 0:34:30  lr: 0.000010  loss: 0.6282  time: 0.9775  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5100/7110]  eta: 0:33:39  lr: 0.000010  loss: 0.1451  time: 0.9615  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5150/7110]  eta: 0:32:49  lr: 0.000010  loss: 0.2646  time: 0.9596  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5200/7110]  eta: 0:31:58  lr: 0.000010  loss: 0.3969  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5250/7110]  eta: 0:31:08  lr: 0.000010  loss: 0.2129  time: 0.9998  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5300/7110]  eta: 0:30:17  lr: 0.000010  loss: 0.0511  time: 1.0182  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5350/7110]  eta: 0:29:27  lr: 0.000010  loss: 0.1683  time: 0.9930  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5400/7110]  eta: 0:28:37  lr: 0.000010  loss: 0.2967  time: 1.0003  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5450/7110]  eta: 0:27:47  lr: 0.000010  loss: 0.3996  time: 1.0587  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5500/7110]  eta: 0:26:57  lr: 0.000010  loss: 0.1148  time: 0.9991  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5550/7110]  eta: 0:26:07  lr: 0.000010  loss: 0.1023  time: 1.0135  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5600/7110]  eta: 0:25:16  lr: 0.000010  loss: 0.4868  time: 1.0213  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5650/7110]  eta: 0:24:26  lr: 0.000010  loss: 0.6472  time: 0.9830  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5700/7110]  eta: 0:23:36  lr: 0.000010  loss: 0.0865  time: 0.9886  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5750/7110]  eta: 0:22:45  lr: 0.000010  loss: 0.0887  time: 0.9918  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5800/7110]  eta: 0:21:55  lr: 0.000010  loss: 0.0883  time: 0.9876  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 0.8813  time: 0.9795  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.4065  time: 1.0201  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [5950/7110]  eta: 0:19:24  lr: 0.000010  loss: 0.0882  time: 1.0789  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6000/7110]  eta: 0:18:34  lr: 0.000010  loss: 0.4770  time: 1.0301  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6050/7110]  eta: 0:17:44  lr: 0.000010  loss: 0.5489  time: 0.9451  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.3472  time: 0.9325  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.0977  time: 0.9358  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.1927  time: 1.0365  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.2445  time: 0.9742  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.2330  time: 1.0360  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.1723  time: 0.9802  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.0894  time: 1.0091  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.0814  time: 0.9420  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.1199  time: 0.9988  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.3000  time: 0.9623  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 1.5527  time: 0.9668  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1125  time: 1.0298  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 1.8227  time: 1.0097  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.0839  time: 0.9928  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.2197  time: 1.0024  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1444  time: 1.0272  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.0404  time: 0.9459  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1837  time: 0.9705  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.1596  time: 1.0652  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1867  time: 1.0169  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.6468  time: 1.0191  data: 0.0000  max mem: 66110
Train: data epoch: [14]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0871  time: 1.1093  data: 0.0000  max mem: 66110
Train: data epoch: [14] Total time: 1:58:48 (1.0026 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:34:15    time: 21.6431  data: 20.3896  max mem: 66110
Evaluation  [  10/1093]  eta: 0:58:45    time: 3.2557  data: 1.8546  max mem: 66110
Evaluation  [  20/1093]  eta: 0:42:02    time: 1.3861  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:35:00    time: 1.2724  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:10    time: 1.2909  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:26    time: 1.4020  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:24    time: 1.2727  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:43    time: 1.3073  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:51    time: 1.4114  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:08    time: 1.3448  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:38    time: 1.3832  data: 0.0011  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:16    time: 1.4426  data: 0.0012  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:40    time: 1.3784  data: 0.0011  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:24    time: 1.4023  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:48    time: 1.3606  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:23    time: 1.2743  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:03    time: 1.3736  data: 0.0011  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:34    time: 1.3151  data: 0.0011  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:17    time: 1.3180  data: 0.0011  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:01    time: 1.4202  data: 0.0011  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:38    time: 1.3616  data: 0.0012  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:35    time: 1.5093  data: 0.0012  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:11    time: 1.4736  data: 0.0011  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:48    time: 1.2339  data: 0.0011  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:30    time: 1.2917  data: 0.0011  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:15    time: 1.3778  data: 0.0012  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:08    time: 1.5436  data: 0.0011  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:46    time: 1.4420  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:27    time: 1.2458  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:19    time: 1.4755  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:57    time: 1.4230  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:39    time: 1.2321  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:26    time: 1.3725  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:08    time: 1.3708  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:56    time: 1.4037  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:42    time: 1.4733  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:22    time: 1.2976  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:06    time: 1.2582  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:55    time: 1.4740  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:41    time: 1.5000  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:28    time: 1.4458  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:11    time: 1.3892  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:58    time: 1.3937  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:44    time: 1.4698  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:25    time: 1.2608  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:10    time: 1.2311  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:55    time: 1.3549  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:39    time: 1.3085  data: 0.0011  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:25    time: 1.3662  data: 0.0011  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:13    time: 1.4932  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:01    time: 1.5692  data: 0.0011  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:47    time: 1.5395  data: 0.0011  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:33    time: 1.4550  data: 0.0011  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:20    time: 1.4753  data: 0.0011  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:06    time: 1.4991  data: 0.0011  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:52    time: 1.4382  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:34    time: 1.2419  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:19    time: 1.2016  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:05    time: 1.3609  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:48    time: 1.2577  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:36    time: 1.3480  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:22    time: 1.5118  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:07    time: 1.4016  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:53    time: 1.3874  data: 0.0011  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:38    time: 1.3570  data: 0.0013  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:24    time: 1.3722  data: 0.0012  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:09    time: 1.3522  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:55    time: 1.3408  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:41    time: 1.4208  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:28    time: 1.4847  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:13    time: 1.4333  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:58    time: 1.2466  data: 0.0011  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:43    time: 1.1611  data: 0.0012  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:27    time: 1.1176  data: 0.0012  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:12    time: 1.1578  data: 0.0011  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:59    time: 1.3374  data: 0.0011  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:45    time: 1.4105  data: 0.0011  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:31    time: 1.3945  data: 0.0011  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:17    time: 1.4515  data: 0.0011  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:02    time: 1.2470  data: 0.0011  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:48    time: 1.3091  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:35    time: 1.5516  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:21    time: 1.4727  data: 0.0011  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:08    time: 1.5268  data: 0.0011  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:53    time: 1.4239  data: 0.0011  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:39    time: 1.3401  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:25    time: 1.3485  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:11    time: 1.3482  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:58    time: 1.5063  data: 0.0011  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:43    time: 1.3509  data: 0.0011  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:29    time: 1.2726  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:15    time: 1.4283  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:01    time: 1.4400  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.3817  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3323  data: 0.0011  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.3341  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.4038  data: 0.0011  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4544  data: 0.0011  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3015  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3095  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4470  data: 0.0011  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4328  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2923  data: 0.0009  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1538  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2510  data: 0.0009  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3604  data: 0.0009  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3990  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4505  data: 0.0011  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3976  data: 0.0011  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3439  data: 0.0011  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3164  data: 0.0416  max mem: 66110
Evaluation Total time: 0:25:20 (1.3908 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_14_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [15]  [   0/7110]  eta: 2 days, 5:31:18  lr: 0.000010  loss: 0.3351  time: 27.0997  data: 0.0001  max mem: 66110
Train: data epoch: [15]  [  50/7110]  eta: 2:54:34  lr: 0.000010  loss: 0.2349  time: 1.0073  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 100/7110]  eta: 2:25:39  lr: 0.000010  loss: 0.3698  time: 0.9637  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 150/7110]  eta: 2:14:41  lr: 0.000010  loss: 0.2630  time: 0.9815  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 200/7110]  eta: 2:09:03  lr: 0.000010  loss: 0.3136  time: 1.0088  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 250/7110]  eta: 2:05:29  lr: 0.000010  loss: 0.5172  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 300/7110]  eta: 2:02:56  lr: 0.000010  loss: 0.4033  time: 0.9979  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 350/7110]  eta: 2:00:21  lr: 0.000010  loss: 0.1650  time: 0.9847  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 400/7110]  eta: 1:59:03  lr: 0.000010  loss: 0.1163  time: 1.0468  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 450/7110]  eta: 1:57:18  lr: 0.000010  loss: 0.2575  time: 0.9908  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 500/7110]  eta: 1:55:49  lr: 0.000010  loss: 0.3481  time: 1.0467  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 550/7110]  eta: 1:54:06  lr: 0.000010  loss: 0.1700  time: 0.9752  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 600/7110]  eta: 1:52:37  lr: 0.000010  loss: 0.2371  time: 0.9232  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 650/7110]  eta: 1:51:18  lr: 0.000010  loss: 0.2128  time: 0.9613  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 700/7110]  eta: 1:50:06  lr: 0.000010  loss: 0.2115  time: 1.0349  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 750/7110]  eta: 1:49:18  lr: 0.000010  loss: 0.2553  time: 1.0353  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 800/7110]  eta: 1:48:20  lr: 0.000010  loss: 0.1389  time: 1.0580  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 850/7110]  eta: 1:46:59  lr: 0.000010  loss: 0.0214  time: 0.9577  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 900/7110]  eta: 1:46:05  lr: 0.000010  loss: 0.0806  time: 1.0102  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [ 950/7110]  eta: 1:45:06  lr: 0.000010  loss: 0.0799  time: 1.0021  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1000/7110]  eta: 1:44:08  lr: 0.000010  loss: 0.2557  time: 1.0219  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1050/7110]  eta: 1:43:16  lr: 0.000010  loss: 1.8567  time: 1.0109  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1100/7110]  eta: 1:42:24  lr: 0.000010  loss: 0.3699  time: 0.9677  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1150/7110]  eta: 1:41:26  lr: 0.000010  loss: 0.0722  time: 0.9793  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1200/7110]  eta: 1:40:31  lr: 0.000010  loss: 1.5959  time: 1.0123  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1250/7110]  eta: 1:39:37  lr: 0.000010  loss: 0.1192  time: 0.9600  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1300/7110]  eta: 1:38:48  lr: 0.000010  loss: 0.5083  time: 0.9901  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1350/7110]  eta: 1:37:45  lr: 0.000010  loss: 0.2324  time: 0.9446  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1400/7110]  eta: 1:36:49  lr: 0.000010  loss: 0.1574  time: 1.0033  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1450/7110]  eta: 1:35:52  lr: 0.000010  loss: 0.0641  time: 0.9242  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1500/7110]  eta: 1:34:55  lr: 0.000010  loss: 0.6598  time: 0.9815  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1550/7110]  eta: 1:34:03  lr: 0.000010  loss: 0.3422  time: 0.9841  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1600/7110]  eta: 1:33:11  lr: 0.000010  loss: 0.5523  time: 1.0003  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1650/7110]  eta: 1:32:14  lr: 0.000010  loss: 0.4492  time: 1.0014  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1700/7110]  eta: 1:31:20  lr: 0.000010  loss: 0.3635  time: 1.0018  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1750/7110]  eta: 1:30:27  lr: 0.000010  loss: 0.4471  time: 0.9701  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1800/7110]  eta: 1:29:37  lr: 0.000010  loss: 0.3552  time: 0.9839  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1850/7110]  eta: 1:28:51  lr: 0.000010  loss: 0.3194  time: 1.0817  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1900/7110]  eta: 1:28:01  lr: 0.000010  loss: 0.0952  time: 1.0628  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [1950/7110]  eta: 1:27:13  lr: 0.000010  loss: 0.1277  time: 1.0378  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2000/7110]  eta: 1:26:23  lr: 0.000010  loss: 0.4078  time: 1.0115  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2050/7110]  eta: 1:25:27  lr: 0.000010  loss: 0.0562  time: 0.9441  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2100/7110]  eta: 1:24:35  lr: 0.000010  loss: 0.4125  time: 1.0033  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2150/7110]  eta: 1:23:42  lr: 0.000010  loss: 0.3066  time: 0.9495  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2200/7110]  eta: 1:22:50  lr: 0.000010  loss: 0.2276  time: 1.0090  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2250/7110]  eta: 1:21:54  lr: 0.000010  loss: 0.4618  time: 0.9656  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2300/7110]  eta: 1:21:03  lr: 0.000010  loss: 0.3965  time: 0.9852  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2350/7110]  eta: 1:20:13  lr: 0.000010  loss: 0.3778  time: 1.0705  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2400/7110]  eta: 1:19:24  lr: 0.000010  loss: 0.2140  time: 1.0270  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2450/7110]  eta: 1:18:37  lr: 0.000010  loss: 0.1065  time: 1.0996  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2500/7110]  eta: 1:17:52  lr: 0.000010  loss: 0.1216  time: 1.0227  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2550/7110]  eta: 1:17:03  lr: 0.000010  loss: 0.1459  time: 1.0236  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2600/7110]  eta: 1:16:10  lr: 0.000010  loss: 0.4959  time: 0.9915  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2650/7110]  eta: 1:15:20  lr: 0.000010  loss: 0.1428  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2700/7110]  eta: 1:14:27  lr: 0.000010  loss: 0.0686  time: 0.9839  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2750/7110]  eta: 1:13:35  lr: 0.000010  loss: 1.5442  time: 1.0610  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2800/7110]  eta: 1:12:41  lr: 0.000010  loss: 0.5762  time: 0.9682  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2850/7110]  eta: 1:11:49  lr: 0.000010  loss: 0.1035  time: 0.9886  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2900/7110]  eta: 1:10:56  lr: 0.000010  loss: 0.4100  time: 0.9842  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [2950/7110]  eta: 1:10:03  lr: 0.000010  loss: 1.4143  time: 1.0097  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3000/7110]  eta: 1:09:12  lr: 0.000010  loss: 0.0738  time: 0.9429  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3050/7110]  eta: 1:08:21  lr: 0.000010  loss: 0.1090  time: 0.9965  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3100/7110]  eta: 1:07:29  lr: 0.000010  loss: 0.1733  time: 1.0018  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3150/7110]  eta: 1:06:37  lr: 0.000010  loss: 0.3056  time: 0.9232  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3200/7110]  eta: 1:05:48  lr: 0.000010  loss: 0.5362  time: 1.0371  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3250/7110]  eta: 1:04:57  lr: 0.000010  loss: 0.7216  time: 0.9807  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3300/7110]  eta: 1:04:05  lr: 0.000010  loss: 0.2310  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3350/7110]  eta: 1:03:15  lr: 0.000010  loss: 0.1301  time: 1.0111  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3400/7110]  eta: 1:02:23  lr: 0.000010  loss: 0.0394  time: 0.9414  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3450/7110]  eta: 1:01:32  lr: 0.000010  loss: 0.0493  time: 1.0152  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3500/7110]  eta: 1:00:41  lr: 0.000010  loss: 0.4407  time: 0.9371  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3550/7110]  eta: 0:59:49  lr: 0.000010  loss: 0.1261  time: 0.9773  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3600/7110]  eta: 0:58:57  lr: 0.000010  loss: 0.1375  time: 0.9362  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3650/7110]  eta: 0:58:06  lr: 0.000010  loss: 0.3302  time: 1.0206  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3700/7110]  eta: 0:57:16  lr: 0.000010  loss: 0.3391  time: 1.0228  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3750/7110]  eta: 0:56:24  lr: 0.000010  loss: 0.5249  time: 1.0476  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3800/7110]  eta: 0:55:33  lr: 0.000010  loss: 0.3352  time: 0.9724  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3850/7110]  eta: 0:54:43  lr: 0.000010  loss: 0.0768  time: 1.0312  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3900/7110]  eta: 0:53:51  lr: 0.000010  loss: 0.1241  time: 0.9825  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [3950/7110]  eta: 0:53:01  lr: 0.000010  loss: 0.6555  time: 1.0392  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4000/7110]  eta: 0:52:11  lr: 0.000010  loss: 0.0730  time: 1.0055  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4050/7110]  eta: 0:51:22  lr: 0.000010  loss: 0.1115  time: 1.0762  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4100/7110]  eta: 0:50:32  lr: 0.000010  loss: 0.5452  time: 1.0691  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4150/7110]  eta: 0:49:41  lr: 0.000010  loss: 0.1688  time: 1.0232  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4200/7110]  eta: 0:48:51  lr: 0.000010  loss: 0.0413  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4250/7110]  eta: 0:47:59  lr: 0.000010  loss: 0.4171  time: 0.9489  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4300/7110]  eta: 0:47:09  lr: 0.000010  loss: 0.4330  time: 0.9964  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4350/7110]  eta: 0:46:18  lr: 0.000010  loss: 1.5995  time: 0.9892  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4400/7110]  eta: 0:45:27  lr: 0.000010  loss: 0.1793  time: 1.0139  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4450/7110]  eta: 0:44:37  lr: 0.000010  loss: 0.1000  time: 0.9921  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4500/7110]  eta: 0:43:46  lr: 0.000010  loss: 1.4877  time: 0.9775  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4550/7110]  eta: 0:42:56  lr: 0.000010  loss: 0.6398  time: 1.0100  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4600/7110]  eta: 0:42:06  lr: 0.000010  loss: 0.0298  time: 1.0274  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4650/7110]  eta: 0:41:15  lr: 0.000010  loss: 0.0378  time: 0.9964  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4700/7110]  eta: 0:40:24  lr: 0.000010  loss: 0.2018  time: 0.9610  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4750/7110]  eta: 0:39:36  lr: 0.000010  loss: 1.5571  time: 1.0998  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4800/7110]  eta: 0:38:45  lr: 0.000010  loss: 0.4198  time: 1.0256  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4850/7110]  eta: 0:37:54  lr: 0.000010  loss: 0.3699  time: 0.9312  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4900/7110]  eta: 0:37:03  lr: 0.000010  loss: 0.1069  time: 0.9590  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [4950/7110]  eta: 0:36:13  lr: 0.000010  loss: 0.1693  time: 1.0271  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5000/7110]  eta: 0:35:22  lr: 0.000010  loss: 0.5526  time: 0.9784  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5050/7110]  eta: 0:34:32  lr: 0.000010  loss: 0.5152  time: 0.9863  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5100/7110]  eta: 0:33:41  lr: 0.000010  loss: 0.9338  time: 0.9649  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5150/7110]  eta: 0:32:50  lr: 0.000010  loss: 0.0258  time: 0.9853  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5200/7110]  eta: 0:32:00  lr: 0.000010  loss: 0.7570  time: 1.0675  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5250/7110]  eta: 0:31:09  lr: 0.000010  loss: 0.3071  time: 0.9713  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5300/7110]  eta: 0:30:19  lr: 0.000010  loss: 0.1813  time: 0.9650  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5350/7110]  eta: 0:29:28  lr: 0.000010  loss: 0.0691  time: 1.0378  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5400/7110]  eta: 0:28:38  lr: 0.000010  loss: 0.2207  time: 1.0205  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5450/7110]  eta: 0:27:48  lr: 0.000010  loss: 0.2157  time: 0.9646  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5500/7110]  eta: 0:26:57  lr: 0.000010  loss: 0.1827  time: 0.9262  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5550/7110]  eta: 0:26:07  lr: 0.000010  loss: 0.5585  time: 0.9672  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5600/7110]  eta: 0:25:16  lr: 0.000010  loss: 0.3488  time: 0.9695  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5650/7110]  eta: 0:24:25  lr: 0.000010  loss: 0.1011  time: 0.9925  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5700/7110]  eta: 0:23:36  lr: 0.000010  loss: 0.2536  time: 1.0276  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5750/7110]  eta: 0:22:45  lr: 0.000010  loss: 0.2110  time: 0.9448  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5800/7110]  eta: 0:21:55  lr: 0.000010  loss: 0.0639  time: 0.9708  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5850/7110]  eta: 0:21:05  lr: 0.000010  loss: 0.1759  time: 1.0040  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5900/7110]  eta: 0:20:15  lr: 0.000010  loss: 0.0947  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [5950/7110]  eta: 0:19:25  lr: 0.000010  loss: 0.1278  time: 1.0298  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6000/7110]  eta: 0:18:35  lr: 0.000010  loss: 0.4498  time: 0.9518  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6050/7110]  eta: 0:17:44  lr: 0.000010  loss: 0.1797  time: 0.9959  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6100/7110]  eta: 0:16:55  lr: 0.000010  loss: 0.8542  time: 1.0689  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6150/7110]  eta: 0:16:04  lr: 0.000010  loss: 0.2515  time: 0.9919  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6200/7110]  eta: 0:15:14  lr: 0.000010  loss: 0.2466  time: 1.0262  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6250/7110]  eta: 0:14:24  lr: 0.000010  loss: 0.3864  time: 1.0353  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6300/7110]  eta: 0:13:33  lr: 0.000010  loss: 0.1242  time: 0.9792  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6350/7110]  eta: 0:12:43  lr: 0.000010  loss: 0.4424  time: 1.0064  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6400/7110]  eta: 0:11:53  lr: 0.000010  loss: 0.1648  time: 0.9859  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6450/7110]  eta: 0:11:03  lr: 0.000010  loss: 0.0455  time: 1.0508  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.4356  time: 0.9740  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.3839  time: 0.9464  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6600/7110]  eta: 0:08:32  lr: 0.000010  loss: 0.7269  time: 0.9584  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1821  time: 0.9451  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.2057  time: 1.0052  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.0677  time: 0.9698  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.4250  time: 0.9969  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.3116  time: 0.9723  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.2857  time: 1.0178  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.2220  time: 0.9686  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.8741  time: 0.9981  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.4879  time: 0.9319  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.0959  time: 0.9853  data: 0.0000  max mem: 66110
Train: data epoch: [15]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.2194  time: 1.0865  data: 0.0000  max mem: 66110
Train: data epoch: [15] Total time: 1:58:53 (1.0034 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:08:03    time: 20.2048  data: 18.9575  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:19    time: 3.1207  data: 1.7244  max mem: 66110
Evaluation  [  20/1093]  eta: 0:40:41    time: 1.3788  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:08    time: 1.2704  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:43    time: 1.3159  data: 0.0011  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:09    time: 1.4372  data: 0.0012  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:02    time: 1.2613  data: 0.0011  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:18    time: 1.2624  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:29    time: 1.3895  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:25:55    time: 1.3681  data: 0.0009  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:30    time: 1.4259  data: 0.0009  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:07    time: 1.4558  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:32    time: 1.3717  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:18    time: 1.4121  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:42    time: 1.3686  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:15    time: 1.2521  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:54    time: 1.3355  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:26    time: 1.3010  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:09    time: 1.3237  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:21:56    time: 1.4470  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:34    time: 1.3865  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:31    time: 1.5047  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:07    time: 1.4770  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:47    time: 1.2717  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:28    time: 1.3135  data: 0.0009  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:13    time: 1.3652  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:07    time: 1.5569  data: 0.0011  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:45    time: 1.4574  data: 0.0011  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:26    time: 1.2489  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:18    time: 1.4748  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:56    time: 1.4094  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:38    time: 1.2209  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:26    time: 1.4072  data: 0.0012  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:09    time: 1.4174  data: 0.0011  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:57    time: 1.4094  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:42    time: 1.4675  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:23    time: 1.2963  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:07    time: 1.2724  data: 0.0011  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:57    time: 1.4983  data: 0.0011  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:42    time: 1.5138  data: 0.0011  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:29    time: 1.4508  data: 0.0011  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:15    time: 1.4489  data: 0.0011  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:01    time: 1.4510  data: 0.0011  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:47    time: 1.4697  data: 0.0011  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:28    time: 1.2707  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:13    time: 1.2364  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:58    time: 1.3639  data: 0.0009  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:42    time: 1.3003  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:28    time: 1.3457  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:15    time: 1.4808  data: 0.0011  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:03    time: 1.5593  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:49    time: 1.5411  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:35    time: 1.4456  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:22    time: 1.4564  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:08    time: 1.4873  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:54    time: 1.4636  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:37    time: 1.2812  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:22    time: 1.2246  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:07    time: 1.3694  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:51    time: 1.2822  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:38    time: 1.3777  data: 0.0011  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:24    time: 1.5103  data: 0.0011  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:10    time: 1.4002  data: 0.0011  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:55    time: 1.3743  data: 0.0011  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:40    time: 1.3398  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:27    time: 1.4015  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:11    time: 1.3777  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:58    time: 1.3626  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:44    time: 1.4648  data: 0.0011  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:30    time: 1.5065  data: 0.0011  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:16    time: 1.4388  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:00    time: 1.2455  data: 0.0009  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:45    time: 1.1747  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:29    time: 1.1463  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:15    time: 1.1715  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:00    time: 1.3127  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:46    time: 1.3866  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:32    time: 1.3863  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:19    time: 1.4451  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:03    time: 1.2636  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:50    time: 1.2882  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:36    time: 1.5013  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:22    time: 1.4590  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:09    time: 1.5180  data: 0.0011  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:54    time: 1.4205  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:40    time: 1.3490  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:26    time: 1.3542  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:12    time: 1.3502  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:58    time: 1.4984  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:44    time: 1.3658  data: 0.0009  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:30    time: 1.2884  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:16    time: 1.4250  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:02    time: 1.4257  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:48    time: 1.3664  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:34    time: 1.3471  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:20    time: 1.3447  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:06    time: 1.4109  data: 0.0009  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:52    time: 1.4802  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:38    time: 1.3689  data: 0.0011  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.3593  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:10    time: 1.4596  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4876  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3411  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1913  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2815  data: 0.0009  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3571  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3911  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4454  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3706  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3196  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2889  data: 0.0404  max mem: 66110
Evaluation Total time: 0:25:26 (1.3963 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_15_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [16]  [   0/7110]  eta: 2 days, 6:02:32  lr: 0.000010  loss: 0.2240  time: 27.3632  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [  50/7110]  eta: 2:59:17  lr: 0.000010  loss: 0.4470  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 100/7110]  eta: 2:29:38  lr: 0.000010  loss: 0.2175  time: 0.9690  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 150/7110]  eta: 2:19:28  lr: 0.000010  loss: 0.0588  time: 1.0319  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 200/7110]  eta: 2:13:21  lr: 0.000010  loss: 0.0461  time: 0.9846  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 250/7110]  eta: 2:07:47  lr: 0.000010  loss: 0.2405  time: 0.9866  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 300/7110]  eta: 2:04:24  lr: 0.000010  loss: 0.1837  time: 0.9692  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 350/7110]  eta: 2:01:29  lr: 0.000010  loss: 0.1844  time: 0.9739  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 400/7110]  eta: 1:59:37  lr: 0.000010  loss: 0.1692  time: 0.9992  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 450/7110]  eta: 1:57:50  lr: 0.000010  loss: 0.6325  time: 0.9774  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 500/7110]  eta: 1:56:08  lr: 0.000010  loss: 0.4499  time: 0.9509  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 550/7110]  eta: 1:54:32  lr: 0.000010  loss: 0.3514  time: 0.9607  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 600/7110]  eta: 1:53:19  lr: 0.000010  loss: 0.1554  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 650/7110]  eta: 1:51:58  lr: 0.000010  loss: 0.1909  time: 0.9695  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 700/7110]  eta: 1:50:35  lr: 0.000010  loss: 0.1386  time: 1.0018  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 750/7110]  eta: 1:49:35  lr: 0.000010  loss: 0.4920  time: 1.0022  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 800/7110]  eta: 1:48:22  lr: 0.000010  loss: 0.1503  time: 0.9700  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 850/7110]  eta: 1:47:16  lr: 0.000010  loss: 0.2914  time: 0.9590  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 900/7110]  eta: 1:46:02  lr: 0.000010  loss: 0.0550  time: 1.0178  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [ 950/7110]  eta: 1:44:59  lr: 0.000010  loss: 0.1836  time: 0.9946  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1000/7110]  eta: 1:44:01  lr: 0.000010  loss: 0.1428  time: 1.0089  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1050/7110]  eta: 1:43:10  lr: 0.000010  loss: 0.1958  time: 1.0165  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1100/7110]  eta: 1:42:13  lr: 0.000010  loss: 0.3591  time: 1.0170  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1150/7110]  eta: 1:41:22  lr: 0.000010  loss: 0.8658  time: 1.0228  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1200/7110]  eta: 1:40:21  lr: 0.000010  loss: 0.4068  time: 0.9303  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1250/7110]  eta: 1:39:22  lr: 0.000010  loss: 1.2105  time: 1.0382  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1300/7110]  eta: 1:38:30  lr: 0.000010  loss: 0.0910  time: 0.9585  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1350/7110]  eta: 1:37:26  lr: 0.000010  loss: 0.5678  time: 0.9432  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1400/7110]  eta: 1:36:37  lr: 0.000010  loss: 0.3740  time: 1.0790  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1450/7110]  eta: 1:35:43  lr: 0.000010  loss: 0.3550  time: 1.0473  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1500/7110]  eta: 1:34:49  lr: 0.000010  loss: 0.3054  time: 1.0121  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1550/7110]  eta: 1:33:54  lr: 0.000010  loss: 0.3173  time: 0.9459  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1600/7110]  eta: 1:33:00  lr: 0.000010  loss: 0.2003  time: 0.9613  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1650/7110]  eta: 1:32:08  lr: 0.000010  loss: 0.5265  time: 0.9850  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1700/7110]  eta: 1:31:16  lr: 0.000010  loss: 0.1762  time: 0.9636  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1750/7110]  eta: 1:30:21  lr: 0.000010  loss: 0.0716  time: 1.0049  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1800/7110]  eta: 1:29:30  lr: 0.000010  loss: 0.0107  time: 1.0102  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1850/7110]  eta: 1:28:39  lr: 0.000010  loss: 0.2268  time: 0.9898  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1900/7110]  eta: 1:27:49  lr: 0.000010  loss: 0.4852  time: 1.0294  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [1950/7110]  eta: 1:27:03  lr: 0.000010  loss: 0.3516  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2000/7110]  eta: 1:26:08  lr: 0.000010  loss: 0.0687  time: 0.9649  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2050/7110]  eta: 1:25:26  lr: 0.000010  loss: 0.0549  time: 1.0704  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2100/7110]  eta: 1:24:37  lr: 0.000010  loss: 0.1528  time: 1.0716  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2150/7110]  eta: 1:23:44  lr: 0.000010  loss: 0.0414  time: 0.9775  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2200/7110]  eta: 1:22:54  lr: 0.000010  loss: 0.2683  time: 1.0310  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2250/7110]  eta: 1:22:08  lr: 0.000010  loss: 0.0494  time: 1.0409  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2300/7110]  eta: 1:21:19  lr: 0.000010  loss: 0.2560  time: 1.0270  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2350/7110]  eta: 1:20:28  lr: 0.000010  loss: 0.4299  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2400/7110]  eta: 1:19:34  lr: 0.000010  loss: 0.1571  time: 0.9480  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2450/7110]  eta: 1:18:43  lr: 0.000010  loss: 0.1746  time: 1.0226  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2500/7110]  eta: 1:17:53  lr: 0.000010  loss: 0.1080  time: 0.9955  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2550/7110]  eta: 1:17:03  lr: 0.000010  loss: 0.2970  time: 1.0646  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2600/7110]  eta: 1:16:07  lr: 0.000010  loss: 0.6897  time: 0.9555  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2650/7110]  eta: 1:15:12  lr: 0.000010  loss: 0.4909  time: 0.9580  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2700/7110]  eta: 1:14:22  lr: 0.000010  loss: 0.1978  time: 1.0177  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2750/7110]  eta: 1:13:32  lr: 0.000010  loss: 0.1564  time: 1.0230  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2800/7110]  eta: 1:12:37  lr: 0.000010  loss: 0.0839  time: 0.9614  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2850/7110]  eta: 1:11:45  lr: 0.000010  loss: 0.2404  time: 1.0062  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2900/7110]  eta: 1:10:55  lr: 0.000010  loss: 0.1057  time: 1.0060  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [2950/7110]  eta: 1:10:05  lr: 0.000010  loss: 1.6671  time: 1.0453  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3000/7110]  eta: 1:09:10  lr: 0.000010  loss: 0.6057  time: 0.9709  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3050/7110]  eta: 1:08:17  lr: 0.000010  loss: 0.1363  time: 0.9463  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3100/7110]  eta: 1:07:26  lr: 0.000010  loss: 0.1276  time: 0.9859  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3150/7110]  eta: 1:06:38  lr: 0.000010  loss: 0.8812  time: 1.0664  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3200/7110]  eta: 1:05:47  lr: 0.000010  loss: 0.3119  time: 0.9947  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3250/7110]  eta: 1:04:56  lr: 0.000010  loss: 0.1089  time: 1.0012  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3300/7110]  eta: 1:04:05  lr: 0.000010  loss: 0.6513  time: 0.9993  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3350/7110]  eta: 1:03:15  lr: 0.000010  loss: 0.4190  time: 1.0289  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3400/7110]  eta: 1:02:25  lr: 0.000010  loss: 0.2798  time: 1.0614  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3450/7110]  eta: 1:01:34  lr: 0.000010  loss: 0.3302  time: 0.9721  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3500/7110]  eta: 1:00:41  lr: 0.000010  loss: 0.3937  time: 0.9636  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3550/7110]  eta: 0:59:52  lr: 0.000010  loss: 0.0359  time: 1.0789  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3600/7110]  eta: 0:59:01  lr: 0.000010  loss: 0.3878  time: 0.9823  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3650/7110]  eta: 0:58:08  lr: 0.000010  loss: 0.5048  time: 0.9620  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3700/7110]  eta: 0:57:17  lr: 0.000010  loss: 0.3091  time: 1.0028  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3750/7110]  eta: 0:56:26  lr: 0.000010  loss: 1.5726  time: 0.9861  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3800/7110]  eta: 0:55:34  lr: 0.000010  loss: 0.1125  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3850/7110]  eta: 0:54:42  lr: 0.000010  loss: 0.1313  time: 0.9737  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3900/7110]  eta: 0:53:51  lr: 0.000010  loss: 1.5925  time: 0.9918  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [3950/7110]  eta: 0:53:00  lr: 0.000010  loss: 0.1771  time: 0.9916  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4000/7110]  eta: 0:52:10  lr: 0.000010  loss: 0.2390  time: 1.0286  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4050/7110]  eta: 0:51:19  lr: 0.000010  loss: 0.4455  time: 0.9801  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4100/7110]  eta: 0:50:28  lr: 0.000010  loss: 0.1148  time: 0.9837  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4150/7110]  eta: 0:49:37  lr: 0.000010  loss: 0.5663  time: 0.9475  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4200/7110]  eta: 0:48:45  lr: 0.000010  loss: 0.8040  time: 0.9562  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4250/7110]  eta: 0:47:55  lr: 0.000010  loss: 0.1739  time: 0.9676  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4300/7110]  eta: 0:47:05  lr: 0.000010  loss: 0.5757  time: 0.9571  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4350/7110]  eta: 0:46:14  lr: 0.000010  loss: 0.0396  time: 0.9978  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4400/7110]  eta: 0:45:24  lr: 0.000010  loss: 0.2096  time: 1.0140  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4450/7110]  eta: 0:44:34  lr: 0.000010  loss: 0.3427  time: 1.0303  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4500/7110]  eta: 0:43:43  lr: 0.000010  loss: 0.0409  time: 0.9744  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4550/7110]  eta: 0:42:53  lr: 0.000010  loss: 0.0288  time: 1.0400  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4600/7110]  eta: 0:42:03  lr: 0.000010  loss: 0.3042  time: 0.9936  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4650/7110]  eta: 0:41:12  lr: 0.000010  loss: 0.1513  time: 1.0077  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4700/7110]  eta: 0:40:22  lr: 0.000010  loss: 0.1264  time: 1.0133  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4750/7110]  eta: 0:39:31  lr: 0.000010  loss: 0.3127  time: 0.9437  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4800/7110]  eta: 0:38:41  lr: 0.000010  loss: 0.2097  time: 0.9931  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4850/7110]  eta: 0:37:50  lr: 0.000010  loss: 0.2477  time: 1.0081  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4900/7110]  eta: 0:36:59  lr: 0.000010  loss: 0.1067  time: 0.9459  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [4950/7110]  eta: 0:36:08  lr: 0.000010  loss: 0.3422  time: 0.9088  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5000/7110]  eta: 0:35:18  lr: 0.000010  loss: 0.2337  time: 1.0608  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5050/7110]  eta: 0:34:28  lr: 0.000010  loss: 0.3099  time: 1.0530  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5100/7110]  eta: 0:33:38  lr: 0.000010  loss: 0.2410  time: 1.0255  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5150/7110]  eta: 0:32:48  lr: 0.000010  loss: 0.1243  time: 0.9816  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5200/7110]  eta: 0:31:58  lr: 0.000010  loss: 0.2802  time: 1.0302  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5250/7110]  eta: 0:31:08  lr: 0.000010  loss: 0.2192  time: 1.0024  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5300/7110]  eta: 0:30:17  lr: 0.000010  loss: 0.6371  time: 0.9821  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5350/7110]  eta: 0:29:27  lr: 0.000010  loss: 0.2287  time: 1.0101  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5400/7110]  eta: 0:28:36  lr: 0.000010  loss: 0.0930  time: 0.9687  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5450/7110]  eta: 0:27:46  lr: 0.000010  loss: 0.1403  time: 1.0050  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5500/7110]  eta: 0:26:56  lr: 0.000010  loss: 0.0683  time: 1.0281  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5550/7110]  eta: 0:26:05  lr: 0.000010  loss: 0.2220  time: 0.9533  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5600/7110]  eta: 0:25:15  lr: 0.000010  loss: 0.0968  time: 1.0001  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5650/7110]  eta: 0:24:25  lr: 0.000010  loss: 0.1766  time: 1.0324  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.3849  time: 0.9460  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.7887  time: 0.9556  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5800/7110]  eta: 0:21:53  lr: 0.000010  loss: 0.4341  time: 0.9371  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.1705  time: 1.0798  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.0858  time: 0.9910  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.1940  time: 0.9838  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.0563  time: 1.0050  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.3946  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.2776  time: 0.9741  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.1938  time: 1.0057  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.2462  time: 0.9880  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.2671  time: 1.0361  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.3432  time: 0.9639  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.3073  time: 1.0357  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.0649  time: 0.9631  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.1810  time: 0.9884  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.2786  time: 1.0411  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.6258  time: 1.0274  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.3059  time: 0.9905  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1696  time: 0.9750  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.1509  time: 0.9670  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.3976  time: 0.9228  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.1111  time: 1.0305  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1215  time: 0.9734  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.3161  time: 1.0522  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1390  time: 0.9513  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.3067  time: 0.9760  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.0558  time: 0.9969  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.0872  time: 1.0513  data: 0.0000  max mem: 66110
Train: data epoch: [16]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.3771  time: 1.0941  data: 0.0000  max mem: 66110
Train: data epoch: [16] Total time: 1:58:50 (1.0029 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:08:19    time: 20.2190  data: 18.9690  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:31    time: 3.1315  data: 1.7255  max mem: 66110
Evaluation  [  20/1093]  eta: 0:40:45    time: 1.3817  data: 0.0012  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:17    time: 1.2774  data: 0.0012  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:43    time: 1.3134  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:02    time: 1.4077  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:27:56    time: 1.2437  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:14    time: 1.2657  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:33    time: 1.4194  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:25:55    time: 1.3850  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:36    time: 1.4432  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:14    time: 1.4925  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:37    time: 1.3750  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:27    time: 1.4289  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:50    time: 1.3921  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:27    time: 1.2876  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:10    time: 1.4200  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:45    time: 1.3773  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:28    time: 1.3625  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:12    time: 1.4436  data: 0.0011  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:49    time: 1.3728  data: 0.0011  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:45    time: 1.5060  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:19    time: 1.4600  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:57    time: 1.2411  data: 0.0011  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:40    time: 1.3274  data: 0.0011  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:25    time: 1.4032  data: 0.0011  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:17    time: 1.5564  data: 0.0012  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:53    time: 1.4091  data: 0.0011  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:35    time: 1.2429  data: 0.0011  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:28    time: 1.5208  data: 0.0011  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:06    time: 1.4402  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:47    time: 1.2281  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:31    time: 1.3257  data: 0.0011  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:14    time: 1.3509  data: 0.0012  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:02    time: 1.4203  data: 0.0011  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:47    time: 1.4733  data: 0.0011  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:27    time: 1.2977  data: 0.0012  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:10    time: 1.2271  data: 0.0011  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:59    time: 1.4384  data: 0.0011  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:45    time: 1.5265  data: 0.0012  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:32    time: 1.4711  data: 0.0012  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:18    time: 1.4816  data: 0.0011  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:05    time: 1.4886  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:51    time: 1.4699  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:31    time: 1.2622  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:16    time: 1.2317  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:01    time: 1.3774  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:46    time: 1.3485  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:32    time: 1.3939  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:19    time: 1.4860  data: 0.0011  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:07    time: 1.5576  data: 0.0011  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:54    time: 1.5830  data: 0.0011  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:40    time: 1.5123  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:27    time: 1.4886  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:13    time: 1.4936  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:59    time: 1.4682  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:41    time: 1.2977  data: 0.0011  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:27    time: 1.2481  data: 0.0011  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:12    time: 1.3677  data: 0.0011  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:55    time: 1.2602  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:42    time: 1.3725  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:28    time: 1.5174  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:14    time: 1.4232  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:00    time: 1.4215  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:44    time: 1.3668  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:31    time: 1.3998  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:15    time: 1.3814  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:02    time: 1.3709  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:47    time: 1.4471  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:34    time: 1.4932  data: 0.0011  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:19    time: 1.4718  data: 0.0012  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:04    time: 1.2566  data: 0.0011  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:48    time: 1.1801  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:33    time: 1.1639  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:18    time: 1.1640  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:03    time: 1.3130  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:49    time: 1.3950  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:35    time: 1.3945  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:21    time: 1.4524  data: 0.0011  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:06    time: 1.2958  data: 0.0011  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:52    time: 1.3289  data: 0.0011  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:39    time: 1.5219  data: 0.0011  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:25    time: 1.4751  data: 0.0011  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:11    time: 1.5235  data: 0.0012  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:57    time: 1.4253  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:43    time: 1.3522  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:28    time: 1.3685  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:14    time: 1.3674  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:00    time: 1.5080  data: 0.0011  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:46    time: 1.3561  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:32    time: 1.2795  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:17    time: 1.4295  data: 0.0011  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:03    time: 1.4347  data: 0.0011  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:49    time: 1.4122  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:35    time: 1.3843  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:21    time: 1.3476  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.3723  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4398  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3754  data: 0.0011  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3712  data: 0.0012  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4623  data: 0.0011  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4605  data: 0.0011  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.2951  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1734  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2792  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3555  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3894  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4517  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3887  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3364  data: 0.0009  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3138  data: 0.0451  max mem: 66110
Evaluation Total time: 0:25:34 (1.4038 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_16_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [17]  [   0/7110]  eta: 2 days, 5:22:42  lr: 0.000010  loss: 0.5287  time: 27.0271  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [  50/7110]  eta: 2:54:35  lr: 0.000010  loss: 0.1264  time: 1.0111  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 100/7110]  eta: 2:25:41  lr: 0.000010  loss: 0.0771  time: 0.9692  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 150/7110]  eta: 2:15:01  lr: 0.000010  loss: 0.2386  time: 0.9964  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 200/7110]  eta: 2:08:24  lr: 0.000010  loss: 1.1009  time: 1.0096  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 250/7110]  eta: 2:04:58  lr: 0.000010  loss: 0.1551  time: 1.0173  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 300/7110]  eta: 2:02:08  lr: 0.000010  loss: 0.4472  time: 0.9736  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 350/7110]  eta: 2:00:20  lr: 0.000010  loss: 0.0586  time: 0.9640  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 400/7110]  eta: 1:58:45  lr: 0.000010  loss: 0.1402  time: 0.9993  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 450/7110]  eta: 1:57:13  lr: 0.000010  loss: 0.0299  time: 0.9682  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 500/7110]  eta: 1:55:25  lr: 0.000010  loss: 0.1712  time: 0.9796  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 550/7110]  eta: 1:54:20  lr: 0.000010  loss: 0.1166  time: 1.0571  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 600/7110]  eta: 1:52:46  lr: 0.000010  loss: 0.3058  time: 0.9603  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 650/7110]  eta: 1:51:31  lr: 0.000010  loss: 0.1707  time: 1.0015  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 700/7110]  eta: 1:50:13  lr: 0.000010  loss: 0.5475  time: 1.0157  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 750/7110]  eta: 1:49:01  lr: 0.000010  loss: 0.7332  time: 0.9893  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 800/7110]  eta: 1:47:55  lr: 0.000010  loss: 0.3945  time: 0.9780  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 850/7110]  eta: 1:46:51  lr: 0.000010  loss: 0.1656  time: 1.0031  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 900/7110]  eta: 1:45:53  lr: 0.000010  loss: 0.4026  time: 1.0121  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [ 950/7110]  eta: 1:44:55  lr: 0.000010  loss: 0.2310  time: 0.9529  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1000/7110]  eta: 1:43:54  lr: 0.000010  loss: 0.4890  time: 0.9788  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1050/7110]  eta: 1:43:00  lr: 0.000010  loss: 0.4495  time: 0.9981  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1100/7110]  eta: 1:41:59  lr: 0.000010  loss: 0.1053  time: 0.9609  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1150/7110]  eta: 1:41:05  lr: 0.000010  loss: 0.1758  time: 1.0155  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1200/7110]  eta: 1:40:09  lr: 0.000010  loss: 0.0642  time: 0.9804  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1250/7110]  eta: 1:39:05  lr: 0.000010  loss: 0.5026  time: 0.9585  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1300/7110]  eta: 1:38:07  lr: 0.000010  loss: 0.8713  time: 0.9564  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1350/7110]  eta: 1:37:07  lr: 0.000010  loss: 0.0952  time: 0.9834  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1400/7110]  eta: 1:36:08  lr: 0.000010  loss: 0.1354  time: 0.9774  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1450/7110]  eta: 1:35:13  lr: 0.000010  loss: 0.5891  time: 0.9797  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1500/7110]  eta: 1:34:20  lr: 0.000010  loss: 0.0430  time: 1.0265  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1550/7110]  eta: 1:33:24  lr: 0.000010  loss: 0.1368  time: 0.9790  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1600/7110]  eta: 1:32:34  lr: 0.000010  loss: 0.4800  time: 0.9948  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1650/7110]  eta: 1:31:43  lr: 0.000010  loss: 0.8260  time: 0.9430  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1700/7110]  eta: 1:30:52  lr: 0.000010  loss: 0.6839  time: 1.0183  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1750/7110]  eta: 1:29:58  lr: 0.000010  loss: 0.3044  time: 0.9713  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1800/7110]  eta: 1:29:05  lr: 0.000010  loss: 0.2449  time: 1.0127  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1850/7110]  eta: 1:28:10  lr: 0.000010  loss: 0.2371  time: 0.9790  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1900/7110]  eta: 1:27:20  lr: 0.000010  loss: 1.4644  time: 1.0103  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [1950/7110]  eta: 1:26:31  lr: 0.000010  loss: 0.0360  time: 1.0288  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2000/7110]  eta: 1:25:38  lr: 0.000010  loss: 0.0668  time: 0.9718  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2050/7110]  eta: 1:24:51  lr: 0.000010  loss: 0.4520  time: 1.0364  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2100/7110]  eta: 1:23:58  lr: 0.000010  loss: 0.3493  time: 0.9557  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2150/7110]  eta: 1:23:07  lr: 0.000010  loss: 0.1022  time: 1.0164  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2200/7110]  eta: 1:22:20  lr: 0.000010  loss: 0.1007  time: 0.9919  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2250/7110]  eta: 1:21:24  lr: 0.000010  loss: 0.2947  time: 1.0064  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2300/7110]  eta: 1:20:31  lr: 0.000010  loss: 0.0341  time: 0.9890  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2350/7110]  eta: 1:19:42  lr: 0.000010  loss: 0.1145  time: 1.0470  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2400/7110]  eta: 1:18:53  lr: 0.000010  loss: 0.3309  time: 1.0430  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2450/7110]  eta: 1:18:00  lr: 0.000010  loss: 0.0980  time: 1.0389  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2500/7110]  eta: 1:17:10  lr: 0.000010  loss: 0.1048  time: 1.0184  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2550/7110]  eta: 1:16:21  lr: 0.000010  loss: 0.2355  time: 1.0243  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2600/7110]  eta: 1:15:29  lr: 0.000010  loss: 0.0621  time: 0.9664  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2650/7110]  eta: 1:14:40  lr: 0.000010  loss: 0.0290  time: 1.0569  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2700/7110]  eta: 1:13:50  lr: 0.000010  loss: 0.4969  time: 0.9744  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2750/7110]  eta: 1:13:02  lr: 0.000010  loss: 0.1485  time: 1.0001  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2800/7110]  eta: 1:12:13  lr: 0.000010  loss: 0.6822  time: 1.0190  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2850/7110]  eta: 1:11:23  lr: 0.000010  loss: 0.2334  time: 1.0564  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2900/7110]  eta: 1:10:33  lr: 0.000010  loss: 0.2240  time: 1.0480  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [2950/7110]  eta: 1:09:44  lr: 0.000010  loss: 0.0403  time: 1.0624  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3000/7110]  eta: 1:08:56  lr: 0.000010  loss: 0.1655  time: 1.0243  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3050/7110]  eta: 1:08:06  lr: 0.000010  loss: 0.3284  time: 1.0367  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3100/7110]  eta: 1:07:18  lr: 0.000010  loss: 0.4384  time: 1.0861  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3150/7110]  eta: 1:06:27  lr: 0.000010  loss: 0.1880  time: 0.9787  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3200/7110]  eta: 1:05:36  lr: 0.000010  loss: 0.0589  time: 1.0449  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3250/7110]  eta: 1:04:47  lr: 0.000010  loss: 0.6469  time: 0.9822  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3300/7110]  eta: 1:03:57  lr: 0.000010  loss: 1.4311  time: 1.0192  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3350/7110]  eta: 1:03:10  lr: 0.000010  loss: 0.1673  time: 1.0735  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3400/7110]  eta: 1:02:19  lr: 0.000010  loss: 0.5643  time: 0.9602  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3450/7110]  eta: 1:01:26  lr: 0.000010  loss: 0.3847  time: 0.9855  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3500/7110]  eta: 1:00:36  lr: 0.000010  loss: 0.2836  time: 1.0379  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3550/7110]  eta: 0:59:48  lr: 0.000010  loss: 0.4861  time: 1.0408  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3600/7110]  eta: 0:58:57  lr: 0.000010  loss: 0.0293  time: 0.9296  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3650/7110]  eta: 0:58:08  lr: 0.000010  loss: 0.4238  time: 1.0759  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3700/7110]  eta: 0:57:17  lr: 0.000010  loss: 0.1939  time: 0.9718  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3750/7110]  eta: 0:56:25  lr: 0.000010  loss: 0.2087  time: 0.9543  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3800/7110]  eta: 0:55:33  lr: 0.000010  loss: 0.4588  time: 1.0024  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3850/7110]  eta: 0:54:43  lr: 0.000010  loss: 0.0405  time: 1.0668  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3900/7110]  eta: 0:53:52  lr: 0.000010  loss: 0.1096  time: 0.9789  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [3950/7110]  eta: 0:53:00  lr: 0.000010  loss: 0.1144  time: 0.9750  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4000/7110]  eta: 0:52:09  lr: 0.000010  loss: 0.7645  time: 1.0274  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4050/7110]  eta: 0:51:19  lr: 0.000010  loss: 0.5708  time: 0.9919  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4100/7110]  eta: 0:50:27  lr: 0.000010  loss: 0.2618  time: 0.9500  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4150/7110]  eta: 0:49:36  lr: 0.000010  loss: 0.4353  time: 0.9445  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4200/7110]  eta: 0:48:45  lr: 0.000010  loss: 0.2238  time: 0.9931  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4250/7110]  eta: 0:47:55  lr: 0.000010  loss: 0.1097  time: 1.0190  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4300/7110]  eta: 0:47:05  lr: 0.000010  loss: 0.1457  time: 1.0258  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4350/7110]  eta: 0:46:14  lr: 0.000010  loss: 0.5046  time: 0.9260  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4400/7110]  eta: 0:45:23  lr: 0.000010  loss: 0.5494  time: 0.9442  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4450/7110]  eta: 0:44:33  lr: 0.000010  loss: 0.2268  time: 1.0461  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4500/7110]  eta: 0:43:43  lr: 0.000010  loss: 0.2960  time: 1.0618  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4550/7110]  eta: 0:42:54  lr: 0.000010  loss: 0.3032  time: 1.0417  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4600/7110]  eta: 0:42:04  lr: 0.000010  loss: 0.4497  time: 1.0193  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4650/7110]  eta: 0:41:13  lr: 0.000010  loss: 0.3237  time: 0.9999  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4700/7110]  eta: 0:40:23  lr: 0.000010  loss: 0.2796  time: 0.9848  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4750/7110]  eta: 0:39:32  lr: 0.000010  loss: 0.3401  time: 0.9808  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4800/7110]  eta: 0:38:43  lr: 0.000010  loss: 1.0448  time: 1.1028  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4850/7110]  eta: 0:37:53  lr: 0.000010  loss: 0.1587  time: 0.9728  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4900/7110]  eta: 0:37:04  lr: 0.000010  loss: 0.1975  time: 1.0504  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [4950/7110]  eta: 0:36:14  lr: 0.000010  loss: 0.1048  time: 1.0225  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5000/7110]  eta: 0:35:24  lr: 0.000010  loss: 0.0803  time: 1.0331  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5050/7110]  eta: 0:34:34  lr: 0.000010  loss: 0.3389  time: 1.0541  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5100/7110]  eta: 0:33:43  lr: 0.000010  loss: 0.3164  time: 0.9612  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5150/7110]  eta: 0:32:52  lr: 0.000010  loss: 0.1306  time: 0.9873  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5200/7110]  eta: 0:32:02  lr: 0.000010  loss: 0.5255  time: 1.0573  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5250/7110]  eta: 0:31:12  lr: 0.000010  loss: 0.1855  time: 1.0184  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5300/7110]  eta: 0:30:21  lr: 0.000010  loss: 0.1749  time: 0.9894  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5350/7110]  eta: 0:29:30  lr: 0.000010  loss: 0.1283  time: 1.0134  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5400/7110]  eta: 0:28:40  lr: 0.000010  loss: 0.4591  time: 1.0759  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5450/7110]  eta: 0:27:50  lr: 0.000010  loss: 0.2864  time: 0.9466  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5500/7110]  eta: 0:26:59  lr: 0.000010  loss: 0.1443  time: 0.9726  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5550/7110]  eta: 0:26:09  lr: 0.000010  loss: 0.5757  time: 0.9704  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5600/7110]  eta: 0:25:19  lr: 0.000010  loss: 0.1423  time: 1.0529  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5650/7110]  eta: 0:24:28  lr: 0.000010  loss: 0.1243  time: 0.9840  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5700/7110]  eta: 0:23:38  lr: 0.000010  loss: 1.8725  time: 1.0221  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5750/7110]  eta: 0:22:48  lr: 0.000010  loss: 0.7880  time: 1.0099  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5800/7110]  eta: 0:21:57  lr: 0.000010  loss: 0.4514  time: 0.9846  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5850/7110]  eta: 0:21:07  lr: 0.000010  loss: 0.2060  time: 0.9732  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5900/7110]  eta: 0:20:17  lr: 0.000010  loss: 0.1374  time: 0.9977  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [5950/7110]  eta: 0:19:26  lr: 0.000010  loss: 0.1193  time: 1.0078  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6000/7110]  eta: 0:18:36  lr: 0.000010  loss: 0.4148  time: 1.0081  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6050/7110]  eta: 0:17:46  lr: 0.000010  loss: 0.1789  time: 1.0187  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6100/7110]  eta: 0:16:55  lr: 0.000010  loss: 0.0594  time: 0.9297  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6150/7110]  eta: 0:16:05  lr: 0.000010  loss: 0.1307  time: 0.9711  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6200/7110]  eta: 0:15:14  lr: 0.000010  loss: 0.5494  time: 0.8967  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6250/7110]  eta: 0:14:24  lr: 0.000010  loss: 0.0510  time: 1.0200  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6300/7110]  eta: 0:13:34  lr: 0.000010  loss: 0.2020  time: 0.9807  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6350/7110]  eta: 0:12:43  lr: 0.000010  loss: 0.1376  time: 1.0595  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6400/7110]  eta: 0:11:53  lr: 0.000010  loss: 0.1984  time: 0.9482  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6450/7110]  eta: 0:11:03  lr: 0.000010  loss: 0.7264  time: 1.0202  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.4781  time: 1.0114  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.1493  time: 0.9569  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6600/7110]  eta: 0:08:32  lr: 0.000010  loss: 0.1356  time: 1.0278  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6650/7110]  eta: 0:07:42  lr: 0.000010  loss: 0.4401  time: 0.9881  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.2504  time: 0.9853  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.0927  time: 0.9319  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.0985  time: 1.0709  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6850/7110]  eta: 0:04:21  lr: 0.000010  loss: 0.1544  time: 1.0798  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.3245  time: 0.9858  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1981  time: 0.9838  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.0532  time: 1.0234  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.0706  time: 0.9844  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1183  time: 0.9706  data: 0.0000  max mem: 66110
Train: data epoch: [17]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.4142  time: 1.1125  data: 0.0000  max mem: 66110
Train: data epoch: [17] Total time: 1:59:01 (1.0045 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:17:25    time: 20.7188  data: 19.4619  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:09    time: 3.1664  data: 1.7701  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:03    time: 1.3744  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:23    time: 1.2673  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:44    time: 1.2982  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:09    time: 1.4141  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:27:57    time: 1.2446  data: 0.0009  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:14    time: 1.2498  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:26    time: 1.3891  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:25:51    time: 1.3675  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:23    time: 1.4057  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:24:59    time: 1.4252  data: 0.0009  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:24    time: 1.3610  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:10    time: 1.4007  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:34    time: 1.3573  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:10    time: 1.2700  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:52    time: 1.3731  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:25    time: 1.3257  data: 0.0009  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:07    time: 1.3209  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:21:53    time: 1.4252  data: 0.0009  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:31    time: 1.3719  data: 0.0009  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:28    time: 1.5091  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:03    time: 1.4609  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:42    time: 1.2369  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:24    time: 1.3096  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:09    time: 1.3753  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:02    time: 1.5405  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:41    time: 1.4408  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:21    time: 1.2368  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:14    time: 1.4708  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:53    time: 1.4229  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:35    time: 1.2261  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:18    time: 1.3076  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:01    time: 1.3043  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:49    time: 1.3979  data: 0.0009  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:35    time: 1.4694  data: 0.0009  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:15    time: 1.2951  data: 0.0009  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:16:58    time: 1.2243  data: 0.0009  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:48    time: 1.4489  data: 0.0009  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:35    time: 1.5267  data: 0.0009  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:22    time: 1.4640  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:06    time: 1.4162  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:53    time: 1.4183  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:38    time: 1.4168  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:19    time: 1.2028  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:04    time: 1.2128  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:48    time: 1.3338  data: 0.0009  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:33    time: 1.2938  data: 0.0009  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:19    time: 1.3613  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:07    time: 1.4744  data: 0.0009  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:55    time: 1.5573  data: 0.0009  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:42    time: 1.5450  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:28    time: 1.4708  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:15    time: 1.4860  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:02    time: 1.5008  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:48    time: 1.4683  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:31    time: 1.2868  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:16    time: 1.2253  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:02    time: 1.3424  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:45    time: 1.2406  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:33    time: 1.3537  data: 0.0009  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:19    time: 1.5121  data: 0.0009  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:04    time: 1.4028  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:50    time: 1.3837  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:35    time: 1.3476  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:22    time: 1.3910  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:07    time: 1.3807  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:53    time: 1.3482  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:39    time: 1.4228  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:26    time: 1.4567  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:11    time: 1.3868  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:56    time: 1.2262  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:41    time: 1.1798  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:25    time: 1.1488  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:10    time: 1.1272  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:56    time: 1.2457  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:42    time: 1.3608  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:28    time: 1.3937  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:15    time: 1.4506  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:00    time: 1.2953  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:47    time: 1.3296  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:33    time: 1.5008  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:19    time: 1.4546  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:06    time: 1.5238  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:51    time: 1.4160  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:38    time: 1.3093  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:23    time: 1.3256  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:10    time: 1.3551  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:56    time: 1.4983  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:42    time: 1.3497  data: 0.0009  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:28    time: 1.2714  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:14    time: 1.4287  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:00    time: 1.4490  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:46    time: 1.3939  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:32    time: 1.3418  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:18    time: 1.3244  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:04    time: 1.3952  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:50    time: 1.4563  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:36    time: 1.3074  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3058  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4565  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4598  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2962  data: 0.0009  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1486  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2414  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3472  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3913  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4451  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3883  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3363  data: 0.0009  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3071  data: 0.0388  max mem: 66110
Evaluation Total time: 0:25:13 (1.3849 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_17_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [18]  [   0/7110]  eta: 2 days, 6:47:33  lr: 0.000010  loss: 0.3948  time: 27.7431  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [  50/7110]  eta: 2:56:37  lr: 0.000010  loss: 0.7960  time: 0.9802  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 100/7110]  eta: 2:26:25  lr: 0.000010  loss: 0.4937  time: 1.0245  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 150/7110]  eta: 2:14:59  lr: 0.000010  loss: 0.1995  time: 0.9929  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 200/7110]  eta: 2:08:07  lr: 0.000010  loss: 0.3564  time: 0.9356  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 250/7110]  eta: 2:04:22  lr: 0.000010  loss: 0.2869  time: 0.9870  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 300/7110]  eta: 2:01:44  lr: 0.000010  loss: 0.2360  time: 1.0131  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 350/7110]  eta: 1:59:20  lr: 0.000010  loss: 0.0898  time: 0.9678  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 400/7110]  eta: 1:57:05  lr: 0.000010  loss: 0.3440  time: 1.0177  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 450/7110]  eta: 1:55:25  lr: 0.000010  loss: 0.2547  time: 0.9121  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 500/7110]  eta: 1:54:04  lr: 0.000010  loss: 0.2433  time: 0.9688  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 550/7110]  eta: 1:52:52  lr: 0.000010  loss: 0.2794  time: 1.0528  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 600/7110]  eta: 1:51:50  lr: 0.000010  loss: 0.1890  time: 1.0536  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 650/7110]  eta: 1:50:40  lr: 0.000010  loss: 0.3181  time: 1.0048  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 700/7110]  eta: 1:49:27  lr: 0.000010  loss: 0.0784  time: 1.0083  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 750/7110]  eta: 1:48:17  lr: 0.000010  loss: 0.3150  time: 0.9732  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 800/7110]  eta: 1:47:14  lr: 0.000010  loss: 0.1350  time: 0.9304  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 850/7110]  eta: 1:46:18  lr: 0.000010  loss: 0.2279  time: 1.0167  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 900/7110]  eta: 1:45:20  lr: 0.000010  loss: 0.2580  time: 0.9878  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [ 950/7110]  eta: 1:44:42  lr: 0.000010  loss: 0.3393  time: 1.1039  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1000/7110]  eta: 1:43:41  lr: 0.000010  loss: 0.3043  time: 1.0775  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1050/7110]  eta: 1:42:47  lr: 0.000010  loss: 0.3330  time: 1.0186  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1100/7110]  eta: 1:41:51  lr: 0.000010  loss: 0.3826  time: 0.9763  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1150/7110]  eta: 1:40:51  lr: 0.000010  loss: 0.7054  time: 0.9890  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1200/7110]  eta: 1:40:00  lr: 0.000010  loss: 0.2097  time: 0.9811  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1250/7110]  eta: 1:39:08  lr: 0.000010  loss: 0.0611  time: 0.9992  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1300/7110]  eta: 1:38:08  lr: 0.000010  loss: 0.0634  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1350/7110]  eta: 1:37:07  lr: 0.000010  loss: 0.1656  time: 0.9501  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1400/7110]  eta: 1:36:04  lr: 0.000010  loss: 0.2911  time: 0.8957  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1450/7110]  eta: 1:35:08  lr: 0.000010  loss: 0.1628  time: 1.0035  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1500/7110]  eta: 1:34:21  lr: 0.000010  loss: 0.0874  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1550/7110]  eta: 1:33:27  lr: 0.000010  loss: 0.1726  time: 0.9868  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1600/7110]  eta: 1:32:35  lr: 0.000010  loss: 0.2423  time: 1.0153  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1650/7110]  eta: 1:31:45  lr: 0.000010  loss: 0.3073  time: 1.0327  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1700/7110]  eta: 1:30:58  lr: 0.000010  loss: 0.1690  time: 1.0333  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1750/7110]  eta: 1:30:06  lr: 0.000010  loss: 0.4373  time: 1.0070  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1800/7110]  eta: 1:29:14  lr: 0.000010  loss: 0.0832  time: 1.0069  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1850/7110]  eta: 1:28:23  lr: 0.000010  loss: 1.6783  time: 0.9909  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1900/7110]  eta: 1:27:31  lr: 0.000010  loss: 0.4839  time: 0.9989  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [1950/7110]  eta: 1:26:41  lr: 0.000010  loss: 1.8490  time: 1.0113  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2000/7110]  eta: 1:25:53  lr: 0.000010  loss: 0.0931  time: 1.0036  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2050/7110]  eta: 1:25:03  lr: 0.000010  loss: 0.5428  time: 1.0370  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2100/7110]  eta: 1:24:11  lr: 0.000010  loss: 0.0506  time: 1.0086  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2150/7110]  eta: 1:23:17  lr: 0.000010  loss: 0.3541  time: 0.9722  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2200/7110]  eta: 1:22:27  lr: 0.000010  loss: 0.2433  time: 1.0557  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2250/7110]  eta: 1:21:36  lr: 0.000010  loss: 0.0654  time: 1.0087  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2300/7110]  eta: 1:20:48  lr: 0.000010  loss: 1.1756  time: 1.0621  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2350/7110]  eta: 1:19:56  lr: 0.000010  loss: 0.1270  time: 0.9793  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2400/7110]  eta: 1:19:05  lr: 0.000010  loss: 0.1711  time: 0.9587  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2450/7110]  eta: 1:18:13  lr: 0.000010  loss: 0.5851  time: 1.0151  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2500/7110]  eta: 1:17:23  lr: 0.000010  loss: 0.6167  time: 0.9604  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2550/7110]  eta: 1:16:29  lr: 0.000010  loss: 0.1143  time: 0.9717  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2600/7110]  eta: 1:15:40  lr: 0.000010  loss: 0.3514  time: 0.9987  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2650/7110]  eta: 1:14:47  lr: 0.000010  loss: 0.5929  time: 0.9663  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2700/7110]  eta: 1:13:55  lr: 0.000010  loss: 0.2032  time: 0.9814  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2750/7110]  eta: 1:13:08  lr: 0.000010  loss: 0.1861  time: 0.9844  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2800/7110]  eta: 1:12:15  lr: 0.000010  loss: 0.3423  time: 0.9269  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2850/7110]  eta: 1:11:25  lr: 0.000010  loss: 0.1026  time: 0.9781  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2900/7110]  eta: 1:10:34  lr: 0.000010  loss: 0.0885  time: 1.0287  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [2950/7110]  eta: 1:09:43  lr: 0.000010  loss: 0.5223  time: 1.0138  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3000/7110]  eta: 1:08:52  lr: 0.000010  loss: 0.3968  time: 1.0259  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3050/7110]  eta: 1:07:59  lr: 0.000010  loss: 0.5085  time: 0.9650  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3100/7110]  eta: 1:07:12  lr: 0.000010  loss: 0.1061  time: 1.0173  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3150/7110]  eta: 1:06:21  lr: 0.000010  loss: 0.0806  time: 1.0369  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3200/7110]  eta: 1:05:29  lr: 0.000010  loss: 0.2933  time: 0.9722  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3250/7110]  eta: 1:04:38  lr: 0.000010  loss: 0.2487  time: 0.9867  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3300/7110]  eta: 1:03:47  lr: 0.000010  loss: 0.1694  time: 1.0174  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3350/7110]  eta: 1:02:57  lr: 0.000010  loss: 0.0694  time: 1.0442  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3400/7110]  eta: 1:02:06  lr: 0.000010  loss: 0.7660  time: 1.0053  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3450/7110]  eta: 1:01:15  lr: 0.000010  loss: 0.3217  time: 1.0058  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3500/7110]  eta: 1:00:21  lr: 0.000010  loss: 0.9103  time: 0.9512  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3550/7110]  eta: 0:59:30  lr: 0.000010  loss: 0.4318  time: 0.9872  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3600/7110]  eta: 0:58:39  lr: 0.000010  loss: 0.3853  time: 0.9646  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3650/7110]  eta: 0:57:48  lr: 0.000010  loss: 0.3135  time: 0.9849  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3700/7110]  eta: 0:56:58  lr: 0.000010  loss: 0.4570  time: 0.9367  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3750/7110]  eta: 0:56:07  lr: 0.000010  loss: 0.3950  time: 0.9318  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3800/7110]  eta: 0:55:17  lr: 0.000010  loss: 0.1992  time: 0.9956  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3850/7110]  eta: 0:54:25  lr: 0.000010  loss: 0.9105  time: 0.9552  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3900/7110]  eta: 0:53:36  lr: 0.000010  loss: 0.0634  time: 1.0013  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [3950/7110]  eta: 0:52:45  lr: 0.000010  loss: 0.4931  time: 1.0031  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4000/7110]  eta: 0:51:55  lr: 0.000010  loss: 0.0436  time: 1.0295  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4050/7110]  eta: 0:51:04  lr: 0.000010  loss: 0.2775  time: 0.9772  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4100/7110]  eta: 0:50:15  lr: 0.000010  loss: 0.2258  time: 1.0173  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4150/7110]  eta: 0:49:25  lr: 0.000010  loss: 0.0714  time: 0.9776  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4200/7110]  eta: 0:48:35  lr: 0.000010  loss: 0.0226  time: 0.9655  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4250/7110]  eta: 0:47:44  lr: 0.000010  loss: 0.1114  time: 0.9961  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4300/7110]  eta: 0:46:56  lr: 0.000010  loss: 0.0945  time: 1.1223  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4350/7110]  eta: 0:46:05  lr: 0.000010  loss: 0.1215  time: 1.0004  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4400/7110]  eta: 0:45:14  lr: 0.000010  loss: 0.0531  time: 1.0142  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4450/7110]  eta: 0:44:24  lr: 0.000010  loss: 0.7080  time: 1.0516  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4500/7110]  eta: 0:43:33  lr: 0.000010  loss: 0.4951  time: 0.9591  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4550/7110]  eta: 0:42:42  lr: 0.000010  loss: 0.0377  time: 1.0238  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4600/7110]  eta: 0:41:54  lr: 0.000010  loss: 0.1435  time: 1.0385  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4650/7110]  eta: 0:41:03  lr: 0.000010  loss: 0.0826  time: 0.9546  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4700/7110]  eta: 0:40:12  lr: 0.000010  loss: 0.8185  time: 1.0051  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4750/7110]  eta: 0:39:21  lr: 0.000010  loss: 0.0255  time: 0.9711  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4800/7110]  eta: 0:38:31  lr: 0.000010  loss: 0.2836  time: 0.9755  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4850/7110]  eta: 0:37:41  lr: 0.000010  loss: 0.0368  time: 0.9997  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4900/7110]  eta: 0:36:50  lr: 0.000010  loss: 0.1785  time: 1.0279  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [4950/7110]  eta: 0:36:00  lr: 0.000010  loss: 0.3124  time: 0.9775  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5000/7110]  eta: 0:35:10  lr: 0.000010  loss: 0.1211  time: 0.9238  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5050/7110]  eta: 0:34:19  lr: 0.000010  loss: 0.1091  time: 0.9781  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5100/7110]  eta: 0:33:31  lr: 0.000010  loss: 0.2389  time: 1.0662  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5150/7110]  eta: 0:32:40  lr: 0.000010  loss: 0.3033  time: 0.9560  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5200/7110]  eta: 0:31:50  lr: 0.000010  loss: 0.3302  time: 0.9802  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5250/7110]  eta: 0:31:00  lr: 0.000010  loss: 0.3514  time: 1.0198  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5300/7110]  eta: 0:30:09  lr: 0.000010  loss: 0.2101  time: 1.0446  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5350/7110]  eta: 0:29:19  lr: 0.000010  loss: 0.1090  time: 0.9352  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5400/7110]  eta: 0:28:30  lr: 0.000010  loss: 0.4395  time: 1.1123  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5450/7110]  eta: 0:27:41  lr: 0.000010  loss: 0.1256  time: 1.0006  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5500/7110]  eta: 0:26:50  lr: 0.000010  loss: 0.1295  time: 0.9910  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5550/7110]  eta: 0:26:00  lr: 0.000010  loss: 0.5620  time: 0.9531  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5600/7110]  eta: 0:25:10  lr: 0.000010  loss: 0.1844  time: 1.0025  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5650/7110]  eta: 0:24:20  lr: 0.000010  loss: 0.0586  time: 0.9808  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5700/7110]  eta: 0:23:30  lr: 0.000010  loss: 0.0984  time: 0.9740  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5750/7110]  eta: 0:22:40  lr: 0.000010  loss: 0.7936  time: 1.0235  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5800/7110]  eta: 0:21:51  lr: 0.000010  loss: 0.4633  time: 1.0377  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5850/7110]  eta: 0:21:01  lr: 0.000010  loss: 0.1597  time: 1.0615  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5900/7110]  eta: 0:20:11  lr: 0.000010  loss: 0.1464  time: 0.9577  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [5950/7110]  eta: 0:19:21  lr: 0.000010  loss: 0.3383  time: 1.0376  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6000/7110]  eta: 0:18:30  lr: 0.000010  loss: 0.4380  time: 0.9771  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6050/7110]  eta: 0:17:40  lr: 0.000010  loss: 0.1456  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6100/7110]  eta: 0:16:51  lr: 0.000010  loss: 0.5410  time: 1.0378  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6150/7110]  eta: 0:16:01  lr: 0.000010  loss: 0.0605  time: 1.0309  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6200/7110]  eta: 0:15:11  lr: 0.000010  loss: 0.5346  time: 0.9871  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6250/7110]  eta: 0:14:21  lr: 0.000010  loss: 0.4611  time: 0.9885  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6300/7110]  eta: 0:13:31  lr: 0.000010  loss: 0.0643  time: 1.0330  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.0633  time: 0.9963  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6400/7110]  eta: 0:11:50  lr: 0.000010  loss: 0.0385  time: 0.9623  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6450/7110]  eta: 0:11:00  lr: 0.000010  loss: 0.4637  time: 1.0156  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6500/7110]  eta: 0:10:10  lr: 0.000010  loss: 0.2095  time: 1.0069  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6550/7110]  eta: 0:09:20  lr: 0.000010  loss: 0.3786  time: 1.0019  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.6419  time: 1.0060  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.0538  time: 1.0036  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.0333  time: 1.0060  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.2825  time: 0.9780  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.4826  time: 1.0498  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.5112  time: 1.0073  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.4577  time: 1.0230  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.2893  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.0466  time: 1.0138  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.3527  time: 0.9575  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.7417  time: 0.9688  data: 0.0000  max mem: 66110
Train: data epoch: [18]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.4831  time: 1.0858  data: 0.0000  max mem: 66110
Train: data epoch: [18] Total time: 1:58:48 (1.0026 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:10:58    time: 20.3648  data: 19.1072  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:49    time: 3.1481  data: 1.7390  max mem: 66110
Evaluation  [  20/1093]  eta: 0:40:46    time: 1.3755  data: 0.0015  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:28    time: 1.2843  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:40    time: 1.3059  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:04    time: 1.3972  data: 0.0009  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:07    time: 1.2824  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:23    time: 1.2892  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:33    time: 1.3898  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:25:54    time: 1.3505  data: 0.0009  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:27    time: 1.3985  data: 0.0009  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:04    time: 1.4434  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:31    time: 1.3766  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:18    time: 1.4213  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:43    time: 1.3835  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:15    time: 1.2585  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:56    time: 1.3476  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:31    time: 1.3416  data: 0.0009  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:14    time: 1.3533  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:00    time: 1.4455  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:38    time: 1.3767  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:34    time: 1.5035  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:10    time: 1.4694  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:49    time: 1.2554  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:32    time: 1.3371  data: 0.0009  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:17    time: 1.4079  data: 0.0009  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:11    time: 1.5658  data: 0.0009  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:47    time: 1.4217  data: 0.0009  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:27    time: 1.2009  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:19    time: 1.4607  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:58    time: 1.4170  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:39    time: 1.2289  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:24    time: 1.3298  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:07    time: 1.3449  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:55    time: 1.4175  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:41    time: 1.4770  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:22    time: 1.3187  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:05    time: 1.2486  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:55    time: 1.4553  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:41    time: 1.5273  data: 0.0009  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:28    time: 1.4709  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:13    time: 1.4467  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:00    time: 1.4422  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:44    time: 1.4270  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:25    time: 1.2144  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:10    time: 1.2208  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:55    time: 1.3614  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:40    time: 1.3426  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:26    time: 1.3856  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:13    time: 1.4719  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:01    time: 1.5530  data: 0.0009  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:48    time: 1.5696  data: 0.0009  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:34    time: 1.4980  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:21    time: 1.4715  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:07    time: 1.4808  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:53    time: 1.4349  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:35    time: 1.2565  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:21    time: 1.2376  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:06    time: 1.3680  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:50    time: 1.2591  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:37    time: 1.3551  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:23    time: 1.4991  data: 0.0009  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:08    time: 1.3976  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:54    time: 1.3955  data: 0.0009  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:39    time: 1.3596  data: 0.0009  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:26    time: 1.3931  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:11    time: 1.3826  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:57    time: 1.3656  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:43    time: 1.4383  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:30    time: 1.4980  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:15    time: 1.4379  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:59    time: 1.2291  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:44    time: 1.1598  data: 0.0009  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:28    time: 1.1372  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:13    time: 1.1596  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:59    time: 1.2902  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:45    time: 1.3743  data: 0.0009  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:31    time: 1.3791  data: 0.0009  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:18    time: 1.4391  data: 0.0009  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:02    time: 1.2714  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:49    time: 1.3483  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:35    time: 1.5486  data: 0.0009  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:22    time: 1.4592  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:08    time: 1.5204  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:54    time: 1.4130  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:40    time: 1.3436  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:25    time: 1.3316  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:11    time: 1.3248  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:58    time: 1.4963  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:43    time: 1.3638  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:29    time: 1.2968  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:15    time: 1.4352  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:02    time: 1.4432  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.3801  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3371  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.3668  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:06    time: 1.4278  data: 0.0009  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:52    time: 1.4696  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3698  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.3595  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:10    time: 1.4563  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4846  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3361  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1495  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2230  data: 0.0009  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3364  data: 0.0009  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3943  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4411  data: 0.0009  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3777  data: 0.0009  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3319  data: 0.0009  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3024  data: 0.0426  max mem: 66110
Evaluation Total time: 0:25:23 (1.3940 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_18_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [19]  [   0/7110]  eta: 2 days, 6:51:34  lr: 0.000010  loss: 0.0263  time: 27.7770  data: 0.0001  max mem: 66110
Train: data epoch: [19]  [  50/7110]  eta: 2:59:49  lr: 0.000010  loss: 0.5596  time: 1.0444  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 100/7110]  eta: 2:27:08  lr: 0.000010  loss: 0.1565  time: 0.9726  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 150/7110]  eta: 2:14:49  lr: 0.000010  loss: 0.4321  time: 0.9698  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 200/7110]  eta: 2:09:01  lr: 0.000010  loss: 0.0203  time: 0.9969  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 250/7110]  eta: 2:05:50  lr: 0.000010  loss: 0.0560  time: 0.9993  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 300/7110]  eta: 2:02:38  lr: 0.000010  loss: 0.1817  time: 0.9893  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 350/7110]  eta: 2:00:44  lr: 0.000010  loss: 0.0614  time: 0.9791  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 400/7110]  eta: 1:58:39  lr: 0.000010  loss: 0.9397  time: 1.0121  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 450/7110]  eta: 1:56:55  lr: 0.000010  loss: 0.4702  time: 1.0103  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 500/7110]  eta: 1:55:36  lr: 0.000010  loss: 1.6099  time: 0.9917  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 550/7110]  eta: 1:54:27  lr: 0.000010  loss: 0.2048  time: 1.0045  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 600/7110]  eta: 1:53:13  lr: 0.000010  loss: 0.1024  time: 0.9657  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 650/7110]  eta: 1:52:04  lr: 0.000010  loss: 0.0723  time: 0.9797  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 700/7110]  eta: 1:50:30  lr: 0.000010  loss: 0.5575  time: 0.9661  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 750/7110]  eta: 1:49:43  lr: 0.000010  loss: 0.0275  time: 1.0438  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 800/7110]  eta: 1:48:38  lr: 0.000010  loss: 0.2527  time: 1.0381  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 850/7110]  eta: 1:47:36  lr: 0.000010  loss: 0.0933  time: 1.0060  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 900/7110]  eta: 1:46:31  lr: 0.000010  loss: 0.3888  time: 0.9859  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [ 950/7110]  eta: 1:45:28  lr: 0.000010  loss: 0.0433  time: 1.0074  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1000/7110]  eta: 1:44:34  lr: 0.000010  loss: 0.0894  time: 1.0323  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1050/7110]  eta: 1:43:30  lr: 0.000010  loss: 0.2846  time: 0.9493  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1100/7110]  eta: 1:42:36  lr: 0.000010  loss: 1.2116  time: 1.0488  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1150/7110]  eta: 1:41:42  lr: 0.000010  loss: 0.1194  time: 0.9457  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1200/7110]  eta: 1:40:43  lr: 0.000010  loss: 0.4160  time: 1.0214  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1250/7110]  eta: 1:39:42  lr: 0.000010  loss: 0.1007  time: 1.0139  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1300/7110]  eta: 1:38:47  lr: 0.000010  loss: 0.3563  time: 1.0469  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1350/7110]  eta: 1:37:50  lr: 0.000010  loss: 0.2682  time: 1.0333  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1400/7110]  eta: 1:36:55  lr: 0.000010  loss: 0.0571  time: 1.0226  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1450/7110]  eta: 1:35:53  lr: 0.000010  loss: 0.1342  time: 0.9874  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1500/7110]  eta: 1:34:54  lr: 0.000010  loss: 0.5400  time: 0.9499  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1550/7110]  eta: 1:34:01  lr: 0.000010  loss: 0.4112  time: 1.0039  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1600/7110]  eta: 1:33:07  lr: 0.000010  loss: 0.0140  time: 0.9924  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1650/7110]  eta: 1:32:11  lr: 0.000010  loss: 0.4357  time: 0.9571  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1700/7110]  eta: 1:31:16  lr: 0.000010  loss: 0.0320  time: 0.9681  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1750/7110]  eta: 1:30:24  lr: 0.000010  loss: 0.0396  time: 1.0145  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1800/7110]  eta: 1:29:30  lr: 0.000010  loss: 0.0448  time: 0.9693  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1850/7110]  eta: 1:28:41  lr: 0.000010  loss: 0.2097  time: 0.9848  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1900/7110]  eta: 1:27:47  lr: 0.000010  loss: 0.2146  time: 0.9655  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [1950/7110]  eta: 1:26:53  lr: 0.000010  loss: 0.2134  time: 1.0203  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2000/7110]  eta: 1:26:04  lr: 0.000010  loss: 0.2174  time: 1.0699  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2050/7110]  eta: 1:25:13  lr: 0.000010  loss: 0.7003  time: 1.0080  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2100/7110]  eta: 1:24:22  lr: 0.000010  loss: 0.1203  time: 0.9906  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2150/7110]  eta: 1:23:29  lr: 0.000010  loss: 0.5219  time: 0.9607  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2200/7110]  eta: 1:22:40  lr: 0.000010  loss: 0.1566  time: 1.0283  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2250/7110]  eta: 1:21:46  lr: 0.000010  loss: 0.1361  time: 0.9442  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2300/7110]  eta: 1:20:54  lr: 0.000010  loss: 0.2161  time: 1.0307  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2350/7110]  eta: 1:20:03  lr: 0.000010  loss: 0.1865  time: 1.0103  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2400/7110]  eta: 1:19:15  lr: 0.000010  loss: 0.1059  time: 1.0246  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2450/7110]  eta: 1:18:22  lr: 0.000010  loss: 0.8454  time: 0.9519  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2500/7110]  eta: 1:17:29  lr: 0.000010  loss: 0.3074  time: 1.0092  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2550/7110]  eta: 1:16:37  lr: 0.000010  loss: 0.1533  time: 0.9906  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2600/7110]  eta: 1:15:46  lr: 0.000010  loss: 0.3069  time: 1.0167  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2650/7110]  eta: 1:14:54  lr: 0.000010  loss: 0.5781  time: 0.9566  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2700/7110]  eta: 1:14:00  lr: 0.000010  loss: 0.3191  time: 0.9706  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2750/7110]  eta: 1:13:10  lr: 0.000010  loss: 0.1523  time: 0.9869  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2800/7110]  eta: 1:12:20  lr: 0.000010  loss: 0.0957  time: 1.0020  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2850/7110]  eta: 1:11:26  lr: 0.000010  loss: 0.3850  time: 0.9946  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2900/7110]  eta: 1:10:30  lr: 0.000010  loss: 0.0987  time: 0.9767  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [2950/7110]  eta: 1:09:39  lr: 0.000010  loss: 1.7240  time: 1.0035  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3000/7110]  eta: 1:08:47  lr: 0.000010  loss: 0.3038  time: 0.9535  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3050/7110]  eta: 1:07:56  lr: 0.000010  loss: 0.4470  time: 1.0054  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3100/7110]  eta: 1:07:05  lr: 0.000010  loss: 0.3800  time: 1.0382  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3150/7110]  eta: 1:06:13  lr: 0.000010  loss: 0.0800  time: 0.9547  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3200/7110]  eta: 1:05:21  lr: 0.000010  loss: 0.1302  time: 0.9436  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3250/7110]  eta: 1:04:30  lr: 0.000010  loss: 0.2465  time: 0.9632  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3300/7110]  eta: 1:03:41  lr: 0.000010  loss: 0.0319  time: 1.0148  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3350/7110]  eta: 1:02:54  lr: 0.000010  loss: 0.1148  time: 1.0215  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3400/7110]  eta: 1:02:05  lr: 0.000010  loss: 0.0219  time: 0.9922  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3450/7110]  eta: 1:01:16  lr: 0.000010  loss: 0.3468  time: 1.0212  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3500/7110]  eta: 1:00:24  lr: 0.000010  loss: 0.1128  time: 0.9547  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3550/7110]  eta: 0:59:35  lr: 0.000010  loss: 0.1007  time: 1.0617  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3600/7110]  eta: 0:58:43  lr: 0.000010  loss: 0.4586  time: 0.9941  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3650/7110]  eta: 0:57:51  lr: 0.000010  loss: 0.4834  time: 0.9585  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3700/7110]  eta: 0:56:59  lr: 0.000010  loss: 0.1612  time: 0.9491  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3750/7110]  eta: 0:56:10  lr: 0.000010  loss: 0.1058  time: 1.0004  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3800/7110]  eta: 0:55:19  lr: 0.000010  loss: 0.0673  time: 0.9723  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3850/7110]  eta: 0:54:27  lr: 0.000010  loss: 0.2644  time: 0.9686  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3900/7110]  eta: 0:53:35  lr: 0.000010  loss: 0.0675  time: 0.9672  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [3950/7110]  eta: 0:52:44  lr: 0.000010  loss: 0.2792  time: 0.9780  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4000/7110]  eta: 0:51:54  lr: 0.000010  loss: 0.3923  time: 1.0051  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4050/7110]  eta: 0:51:02  lr: 0.000010  loss: 0.1631  time: 0.9370  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4100/7110]  eta: 0:50:13  lr: 0.000010  loss: 0.3874  time: 1.0269  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4150/7110]  eta: 0:49:24  lr: 0.000010  loss: 0.3793  time: 1.0522  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4200/7110]  eta: 0:48:34  lr: 0.000010  loss: 0.1104  time: 0.9711  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4250/7110]  eta: 0:47:45  lr: 0.000010  loss: 0.1071  time: 1.0377  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4300/7110]  eta: 0:46:56  lr: 0.000010  loss: 0.0909  time: 1.0812  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4350/7110]  eta: 0:46:06  lr: 0.000010  loss: 0.1600  time: 0.9930  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4400/7110]  eta: 0:45:16  lr: 0.000010  loss: 0.1714  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4450/7110]  eta: 0:44:26  lr: 0.000010  loss: 0.3960  time: 0.9891  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4500/7110]  eta: 0:43:36  lr: 0.000010  loss: 0.8287  time: 1.0314  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4550/7110]  eta: 0:42:46  lr: 0.000010  loss: 0.0877  time: 1.0140  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4600/7110]  eta: 0:41:56  lr: 0.000010  loss: 0.8079  time: 0.9794  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4650/7110]  eta: 0:41:06  lr: 0.000010  loss: 0.2248  time: 1.0328  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4700/7110]  eta: 0:40:16  lr: 0.000010  loss: 0.1843  time: 1.0195  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4750/7110]  eta: 0:39:25  lr: 0.000010  loss: 0.1711  time: 0.9899  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4800/7110]  eta: 0:38:34  lr: 0.000010  loss: 0.1118  time: 0.9416  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4850/7110]  eta: 0:37:44  lr: 0.000010  loss: 0.1457  time: 1.0490  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4900/7110]  eta: 0:36:54  lr: 0.000010  loss: 0.5455  time: 1.0737  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [4950/7110]  eta: 0:36:04  lr: 0.000010  loss: 0.5066  time: 0.9331  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5000/7110]  eta: 0:35:14  lr: 0.000010  loss: 0.5831  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5050/7110]  eta: 0:34:23  lr: 0.000010  loss: 0.4344  time: 0.9708  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5100/7110]  eta: 0:33:33  lr: 0.000010  loss: 0.2441  time: 0.9884  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5150/7110]  eta: 0:32:42  lr: 0.000010  loss: 0.7988  time: 1.0242  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5200/7110]  eta: 0:31:52  lr: 0.000010  loss: 0.0178  time: 0.9861  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5250/7110]  eta: 0:31:02  lr: 0.000010  loss: 0.0356  time: 1.0071  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5300/7110]  eta: 0:30:13  lr: 0.000010  loss: 0.3036  time: 0.9641  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5350/7110]  eta: 0:29:22  lr: 0.000010  loss: 0.0653  time: 0.9514  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5400/7110]  eta: 0:28:32  lr: 0.000010  loss: 0.1321  time: 0.9684  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5450/7110]  eta: 0:27:42  lr: 0.000010  loss: 0.0597  time: 0.9959  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5500/7110]  eta: 0:26:52  lr: 0.000010  loss: 0.3148  time: 1.0270  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5550/7110]  eta: 0:26:02  lr: 0.000010  loss: 0.1368  time: 0.9842  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5600/7110]  eta: 0:25:12  lr: 0.000010  loss: 0.4767  time: 0.9714  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5650/7110]  eta: 0:24:23  lr: 0.000010  loss: 0.6637  time: 1.0401  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5700/7110]  eta: 0:23:33  lr: 0.000010  loss: 0.1505  time: 1.0016  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5750/7110]  eta: 0:22:43  lr: 0.000010  loss: 0.3379  time: 1.0195  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5800/7110]  eta: 0:21:53  lr: 0.000010  loss: 0.4238  time: 1.0297  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.0854  time: 1.0465  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.0953  time: 1.0287  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [5950/7110]  eta: 0:19:22  lr: 0.000010  loss: 0.2806  time: 0.9827  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6000/7110]  eta: 0:18:32  lr: 0.000010  loss: 0.2275  time: 1.0049  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.0708  time: 1.0098  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.1089  time: 1.0151  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.0986  time: 1.0107  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 1.7178  time: 1.0195  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6250/7110]  eta: 0:14:21  lr: 0.000010  loss: 0.0080  time: 0.9484  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6300/7110]  eta: 0:13:31  lr: 0.000010  loss: 0.0967  time: 1.0202  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.4311  time: 0.9918  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.1242  time: 0.9768  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.3659  time: 1.0489  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.0693  time: 1.0393  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.6265  time: 0.9789  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.3547  time: 1.0349  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.3595  time: 0.9926  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.1608  time: 0.9527  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.2201  time: 0.9585  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.9598  time: 0.9633  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1492  time: 1.0021  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1809  time: 0.9618  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1035  time: 1.0232  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.7552  time: 0.9731  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1528  time: 1.0115  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1757  time: 1.0036  data: 0.0000  max mem: 66110
Train: data epoch: [19]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0710  time: 1.1043  data: 0.0000  max mem: 66110
Train: data epoch: [19] Total time: 1:58:44 (1.0021 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:04:54    time: 20.0312  data: 18.7747  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:40    time: 3.1402  data: 1.7077  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:07    time: 1.4129  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:11    time: 1.2647  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:37    time: 1.2798  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:01    time: 1.4137  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:06    time: 1.2855  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:23    time: 1.2987  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:35    time: 1.3976  data: 0.0011  max mem: 66110
Evaluation  [  90/1093]  eta: 0:25:55    time: 1.3541  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:27    time: 1.3894  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:02    time: 1.4272  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:28    time: 1.3626  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:17    time: 1.4266  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:41    time: 1.3831  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:12    time: 1.2397  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:53    time: 1.3360  data: 0.0012  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:26    time: 1.3174  data: 0.0012  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:09    time: 1.3272  data: 0.0012  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:21:54    time: 1.4347  data: 0.0011  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:32    time: 1.3751  data: 0.0011  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:29    time: 1.5077  data: 0.0012  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:05    time: 1.4701  data: 0.0011  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:44    time: 1.2489  data: 0.0012  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:26    time: 1.3098  data: 0.0012  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:12    time: 1.3882  data: 0.0011  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:05    time: 1.5548  data: 0.0012  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:40    time: 1.3879  data: 0.0011  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:23    time: 1.2232  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:15    time: 1.5045  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:54    time: 1.4163  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:35    time: 1.2184  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:23    time: 1.3974  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:06    time: 1.4120  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:54    time: 1.4076  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:40    time: 1.4713  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:20    time: 1.2968  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:03    time: 1.2269  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:53    time: 1.4439  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:39    time: 1.5132  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:26    time: 1.4595  data: 0.0011  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:10    time: 1.4134  data: 0.0011  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:57    time: 1.4074  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:41    time: 1.4187  data: 0.0011  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:22    time: 1.2156  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:07    time: 1.2277  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:52    time: 1.3487  data: 0.0011  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:36    time: 1.2994  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:23    time: 1.3661  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:10    time: 1.4798  data: 0.0011  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:58    time: 1.5640  data: 0.0011  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:45    time: 1.5576  data: 0.0012  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:31    time: 1.4805  data: 0.0012  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:18    time: 1.4911  data: 0.0011  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:04    time: 1.4816  data: 0.0011  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:50    time: 1.4242  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:33    time: 1.2668  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:18    time: 1.2351  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:04    time: 1.3447  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:47    time: 1.2386  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:35    time: 1.3588  data: 0.0012  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:21    time: 1.5157  data: 0.0012  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:06    time: 1.3934  data: 0.0011  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:52    time: 1.3764  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:37    time: 1.3510  data: 0.0009  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:24    time: 1.4262  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:09    time: 1.4131  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:55    time: 1.3449  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:41    time: 1.3996  data: 0.0011  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:27    time: 1.4627  data: 0.0011  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:13    time: 1.4296  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:57    time: 1.2347  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:42    time: 1.1869  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:27    time: 1.1570  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:12    time: 1.1230  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:58    time: 1.2726  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:44    time: 1.3895  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:30    time: 1.3973  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:16    time: 1.4651  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:01    time: 1.2962  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:48    time: 1.3498  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:34    time: 1.5399  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:21    time: 1.4538  data: 0.0011  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:07    time: 1.5166  data: 0.0012  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:53    time: 1.4183  data: 0.0011  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:39    time: 1.3552  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:25    time: 1.3663  data: 0.0011  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:11    time: 1.3354  data: 0.0011  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:57    time: 1.4902  data: 0.0012  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:43    time: 1.3846  data: 0.0012  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:29    time: 1.2984  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:15    time: 1.4269  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:01    time: 1.4211  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.3637  data: 0.0011  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3132  data: 0.0012  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.2989  data: 0.0011  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.3976  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4690  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3641  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3507  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4771  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4774  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2943  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1451  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2403  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3443  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3831  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4497  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3866  data: 0.0011  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3284  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2981  data: 0.0410  max mem: 66110
Evaluation Total time: 0:25:19 (1.3904 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_19_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [20]  [   0/7110]  eta: 2 days, 7:24:02  lr: 0.000010  loss: 0.1104  time: 28.0510  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [  50/7110]  eta: 2:58:10  lr: 0.000010  loss: 0.1391  time: 0.9799  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 100/7110]  eta: 2:27:47  lr: 0.000010  loss: 0.4471  time: 1.0234  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 150/7110]  eta: 2:16:14  lr: 0.000010  loss: 1.7237  time: 0.9860  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 200/7110]  eta: 2:10:21  lr: 0.000010  loss: 0.2525  time: 0.9905  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 250/7110]  eta: 2:07:23  lr: 0.000010  loss: 0.0261  time: 1.0089  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 300/7110]  eta: 2:04:16  lr: 0.000010  loss: 0.1487  time: 0.9891  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 350/7110]  eta: 2:01:49  lr: 0.000010  loss: 0.0786  time: 1.0040  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 400/7110]  eta: 1:59:26  lr: 0.000010  loss: 0.1366  time: 0.9935  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 450/7110]  eta: 1:57:24  lr: 0.000010  loss: 0.2641  time: 1.0126  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 500/7110]  eta: 1:55:46  lr: 0.000010  loss: 0.1836  time: 0.9928  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 550/7110]  eta: 1:54:31  lr: 0.000010  loss: 0.3943  time: 1.0110  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 600/7110]  eta: 1:53:18  lr: 0.000010  loss: 0.2481  time: 1.0024  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 650/7110]  eta: 1:52:07  lr: 0.000010  loss: 0.5280  time: 1.0399  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 700/7110]  eta: 1:50:53  lr: 0.000010  loss: 0.0200  time: 0.9808  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 750/7110]  eta: 1:49:51  lr: 0.000010  loss: 0.1449  time: 1.0004  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 800/7110]  eta: 1:48:47  lr: 0.000010  loss: 0.5082  time: 0.9915  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 850/7110]  eta: 1:47:35  lr: 0.000010  loss: 0.7719  time: 0.9959  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 900/7110]  eta: 1:46:41  lr: 0.000010  loss: 0.7844  time: 0.9782  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [ 950/7110]  eta: 1:45:46  lr: 0.000010  loss: 0.5661  time: 0.9922  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1000/7110]  eta: 1:44:42  lr: 0.000010  loss: 0.2331  time: 0.9902  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1050/7110]  eta: 1:43:50  lr: 0.000010  loss: 0.3902  time: 1.0712  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1100/7110]  eta: 1:42:55  lr: 0.000010  loss: 0.3067  time: 1.0112  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1150/7110]  eta: 1:41:54  lr: 0.000010  loss: 0.0409  time: 1.0390  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1200/7110]  eta: 1:40:59  lr: 0.000010  loss: 0.1926  time: 1.0371  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1250/7110]  eta: 1:39:57  lr: 0.000010  loss: 0.0476  time: 0.9557  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1300/7110]  eta: 1:39:01  lr: 0.000010  loss: 0.0521  time: 1.0410  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1350/7110]  eta: 1:38:02  lr: 0.000010  loss: 0.2199  time: 0.9715  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1400/7110]  eta: 1:37:11  lr: 0.000010  loss: 0.4608  time: 1.0317  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1450/7110]  eta: 1:36:16  lr: 0.000010  loss: 0.5032  time: 1.0222  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1500/7110]  eta: 1:35:26  lr: 0.000010  loss: 0.5736  time: 1.0686  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1550/7110]  eta: 1:34:36  lr: 0.000010  loss: 0.3423  time: 1.0134  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1600/7110]  eta: 1:33:41  lr: 0.000010  loss: 0.0211  time: 1.0157  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1650/7110]  eta: 1:32:54  lr: 0.000010  loss: 0.1620  time: 1.0590  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1700/7110]  eta: 1:32:03  lr: 0.000010  loss: 0.1056  time: 1.0385  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1750/7110]  eta: 1:31:02  lr: 0.000010  loss: 0.1741  time: 0.9485  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1800/7110]  eta: 1:30:07  lr: 0.000010  loss: 0.0275  time: 0.9498  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1850/7110]  eta: 1:29:13  lr: 0.000010  loss: 0.1330  time: 0.9929  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1900/7110]  eta: 1:28:21  lr: 0.000010  loss: 0.1535  time: 0.9651  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [1950/7110]  eta: 1:27:29  lr: 0.000010  loss: 0.1516  time: 1.0316  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2000/7110]  eta: 1:26:37  lr: 0.000010  loss: 0.0889  time: 0.9980  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2050/7110]  eta: 1:25:47  lr: 0.000010  loss: 0.2383  time: 0.9954  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2100/7110]  eta: 1:24:48  lr: 0.000010  loss: 0.4613  time: 0.9256  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2150/7110]  eta: 1:23:54  lr: 0.000010  loss: 0.6747  time: 0.9465  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2200/7110]  eta: 1:23:04  lr: 0.000010  loss: 0.1436  time: 1.0451  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2250/7110]  eta: 1:22:13  lr: 0.000010  loss: 0.1501  time: 0.9830  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2300/7110]  eta: 1:21:21  lr: 0.000010  loss: 0.1697  time: 0.9673  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2350/7110]  eta: 1:20:31  lr: 0.000010  loss: 0.7653  time: 1.0173  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2400/7110]  eta: 1:19:36  lr: 0.000010  loss: 0.1072  time: 0.9850  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2450/7110]  eta: 1:18:45  lr: 0.000010  loss: 0.1673  time: 0.9704  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2500/7110]  eta: 1:17:52  lr: 0.000010  loss: 0.0475  time: 0.9879  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2550/7110]  eta: 1:16:59  lr: 0.000010  loss: 0.0497  time: 1.0511  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2600/7110]  eta: 1:16:04  lr: 0.000010  loss: 0.1525  time: 0.9693  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2650/7110]  eta: 1:15:10  lr: 0.000010  loss: 0.2806  time: 0.9631  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2700/7110]  eta: 1:14:19  lr: 0.000010  loss: 0.0916  time: 1.0411  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2750/7110]  eta: 1:13:31  lr: 0.000010  loss: 0.0871  time: 1.0324  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2800/7110]  eta: 1:12:39  lr: 0.000010  loss: 0.1609  time: 0.9975  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2850/7110]  eta: 1:11:46  lr: 0.000010  loss: 0.2563  time: 1.0353  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2900/7110]  eta: 1:10:57  lr: 0.000010  loss: 0.5301  time: 1.0193  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [2950/7110]  eta: 1:10:07  lr: 0.000010  loss: 0.1005  time: 1.0454  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3000/7110]  eta: 1:09:16  lr: 0.000010  loss: 0.5098  time: 1.0416  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3050/7110]  eta: 1:08:26  lr: 0.000010  loss: 0.1140  time: 1.0464  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3100/7110]  eta: 1:07:32  lr: 0.000010  loss: 0.2269  time: 0.9582  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3150/7110]  eta: 1:06:41  lr: 0.000010  loss: 0.2462  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3200/7110]  eta: 1:05:50  lr: 0.000010  loss: 0.1832  time: 0.9862  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3250/7110]  eta: 1:04:59  lr: 0.000010  loss: 0.2366  time: 0.9676  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3300/7110]  eta: 1:04:07  lr: 0.000010  loss: 0.6083  time: 0.9908  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3350/7110]  eta: 1:03:17  lr: 0.000010  loss: 0.0967  time: 1.0451  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3400/7110]  eta: 1:02:27  lr: 0.000010  loss: 0.0247  time: 0.9899  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3450/7110]  eta: 1:01:37  lr: 0.000010  loss: 1.3337  time: 1.0226  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3500/7110]  eta: 1:00:46  lr: 0.000010  loss: 1.5603  time: 1.0021  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3550/7110]  eta: 0:59:54  lr: 0.000010  loss: 0.0258  time: 0.9899  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3600/7110]  eta: 0:59:03  lr: 0.000010  loss: 0.7840  time: 0.9828  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3650/7110]  eta: 0:58:13  lr: 0.000010  loss: 0.8361  time: 1.0467  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3700/7110]  eta: 0:57:23  lr: 0.000010  loss: 0.3051  time: 0.9766  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3750/7110]  eta: 0:56:31  lr: 0.000010  loss: 0.4524  time: 0.9775  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3800/7110]  eta: 0:55:39  lr: 0.000010  loss: 0.0548  time: 0.9473  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3850/7110]  eta: 0:54:52  lr: 0.000010  loss: 0.1278  time: 1.0654  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3900/7110]  eta: 0:54:01  lr: 0.000010  loss: 0.3682  time: 0.9746  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [3950/7110]  eta: 0:53:09  lr: 0.000010  loss: 0.1244  time: 0.9229  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4000/7110]  eta: 0:52:17  lr: 0.000010  loss: 0.5356  time: 0.9555  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4050/7110]  eta: 0:51:26  lr: 0.000010  loss: 0.0469  time: 0.9496  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4100/7110]  eta: 0:50:36  lr: 0.000010  loss: 0.6862  time: 1.0558  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4150/7110]  eta: 0:49:45  lr: 0.000010  loss: 0.0263  time: 1.0162  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4200/7110]  eta: 0:48:53  lr: 0.000010  loss: 0.0413  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4250/7110]  eta: 0:48:02  lr: 0.000010  loss: 0.1301  time: 0.9742  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4300/7110]  eta: 0:47:12  lr: 0.000010  loss: 0.3781  time: 1.0030  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4350/7110]  eta: 0:46:22  lr: 0.000010  loss: 0.0953  time: 1.0359  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4400/7110]  eta: 0:45:31  lr: 0.000010  loss: 0.4841  time: 0.9785  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4450/7110]  eta: 0:44:41  lr: 0.000010  loss: 0.1667  time: 1.0135  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4500/7110]  eta: 0:43:50  lr: 0.000010  loss: 0.1630  time: 0.9732  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4550/7110]  eta: 0:42:58  lr: 0.000010  loss: 0.0336  time: 0.9560  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4600/7110]  eta: 0:42:07  lr: 0.000010  loss: 0.2893  time: 0.9601  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4650/7110]  eta: 0:41:16  lr: 0.000010  loss: 0.5994  time: 1.0162  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4700/7110]  eta: 0:40:26  lr: 0.000010  loss: 0.1778  time: 1.0303  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4750/7110]  eta: 0:39:34  lr: 0.000010  loss: 0.1452  time: 0.9431  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4800/7110]  eta: 0:38:45  lr: 0.000010  loss: 0.3701  time: 1.0514  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4850/7110]  eta: 0:37:54  lr: 0.000010  loss: 0.2729  time: 0.9788  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4900/7110]  eta: 0:37:04  lr: 0.000010  loss: 0.1165  time: 1.0649  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [4950/7110]  eta: 0:36:15  lr: 0.000010  loss: 0.7815  time: 1.0296  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5000/7110]  eta: 0:35:25  lr: 0.000010  loss: 0.1969  time: 0.9888  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5050/7110]  eta: 0:34:33  lr: 0.000010  loss: 0.2247  time: 0.9371  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5100/7110]  eta: 0:33:43  lr: 0.000010  loss: 0.2879  time: 1.0814  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5150/7110]  eta: 0:32:52  lr: 0.000010  loss: 0.1975  time: 0.9473  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5200/7110]  eta: 0:32:01  lr: 0.000010  loss: 0.4157  time: 0.9399  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5250/7110]  eta: 0:31:11  lr: 0.000010  loss: 0.2404  time: 1.0168  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5300/7110]  eta: 0:30:20  lr: 0.000010  loss: 0.8342  time: 1.0188  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5350/7110]  eta: 0:29:30  lr: 0.000010  loss: 0.1072  time: 1.0644  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5400/7110]  eta: 0:28:40  lr: 0.000010  loss: 0.0386  time: 1.0441  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5450/7110]  eta: 0:27:49  lr: 0.000010  loss: 0.3085  time: 0.9612  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5500/7110]  eta: 0:26:58  lr: 0.000010  loss: 0.1927  time: 0.9688  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5550/7110]  eta: 0:26:08  lr: 0.000010  loss: 0.1460  time: 0.9707  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5600/7110]  eta: 0:25:17  lr: 0.000010  loss: 0.1322  time: 0.9817  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5650/7110]  eta: 0:24:27  lr: 0.000010  loss: 0.4816  time: 1.0107  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5700/7110]  eta: 0:23:37  lr: 0.000010  loss: 0.0971  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5750/7110]  eta: 0:22:46  lr: 0.000010  loss: 0.1548  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5800/7110]  eta: 0:21:56  lr: 0.000010  loss: 0.0465  time: 1.0301  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5850/7110]  eta: 0:21:06  lr: 0.000010  loss: 0.1235  time: 0.9629  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5900/7110]  eta: 0:20:15  lr: 0.000010  loss: 0.0902  time: 0.9998  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [5950/7110]  eta: 0:19:25  lr: 0.000010  loss: 0.3568  time: 0.9807  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6000/7110]  eta: 0:18:35  lr: 0.000010  loss: 0.0983  time: 0.9790  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6050/7110]  eta: 0:17:44  lr: 0.000010  loss: 0.8122  time: 0.9387  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.1680  time: 0.9335  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.2489  time: 0.9743  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.0838  time: 1.0502  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.4722  time: 0.9960  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6300/7110]  eta: 0:13:33  lr: 0.000010  loss: 0.1861  time: 1.0566  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6350/7110]  eta: 0:12:43  lr: 0.000010  loss: 0.1826  time: 1.0380  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.1094  time: 0.9920  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.3741  time: 0.9635  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.0568  time: 1.0049  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.8716  time: 0.9818  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.1517  time: 1.0034  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1079  time: 0.9192  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 1.4625  time: 1.0263  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.3846  time: 1.0629  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.2311  time: 1.0256  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.0753  time: 1.0509  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.0785  time: 0.9916  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.6289  time: 0.9239  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.4342  time: 0.9699  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.3238  time: 0.9232  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1549  time: 0.9914  data: 0.0000  max mem: 66110
Train: data epoch: [20]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.2684  time: 1.0533  data: 0.0000  max mem: 66110
Train: data epoch: [20] Total time: 1:58:51 (1.0030 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:25:11    time: 21.1454  data: 19.9102  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:41    time: 3.1958  data: 1.8109  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:24    time: 1.3735  data: 0.0009  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:35    time: 1.2683  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:46    time: 1.2803  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:10    time: 1.3996  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:15    time: 1.2947  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:38    time: 1.3284  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:47    time: 1.4205  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:11    time: 1.3749  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:40    time: 1.4101  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:16    time: 1.4360  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:39    time: 1.3642  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:24    time: 1.3958  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:48    time: 1.3641  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:18    time: 1.2401  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:59    time: 1.3397  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:32    time: 1.3279  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:15    time: 1.3293  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:21:58    time: 1.4201  data: 0.0011  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:36    time: 1.3605  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:34    time: 1.5215  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:10    time: 1.4830  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:49    time: 1.2503  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:27    time: 1.2700  data: 0.0009  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:13    time: 1.3394  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:04    time: 1.5278  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:44    time: 1.4353  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:26    time: 1.2908  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:18    time: 1.5035  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:57    time: 1.4183  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:39    time: 1.2284  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:22    time: 1.3090  data: 0.0009  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:04    time: 1.3016  data: 0.0009  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:52    time: 1.3882  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:38    time: 1.4647  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:18    time: 1.2957  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:01    time: 1.2251  data: 0.0009  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:50    time: 1.4274  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:34    time: 1.4294  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:21    time: 1.3848  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:06    time: 1.4431  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:53    time: 1.4410  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:38    time: 1.4077  data: 0.0009  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:20    time: 1.2393  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:05    time: 1.2606  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:51    time: 1.3922  data: 0.0009  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:35    time: 1.3376  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:21    time: 1.3538  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:09    time: 1.4896  data: 0.0009  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:57    time: 1.5799  data: 0.0009  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:44    time: 1.5601  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:30    time: 1.4693  data: 0.0009  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:17    time: 1.4786  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:04    time: 1.4963  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:50    time: 1.4433  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:32    time: 1.2321  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:16    time: 1.1539  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:02    time: 1.3150  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:46    time: 1.2559  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:33    time: 1.3587  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:19    time: 1.5067  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:05    time: 1.3973  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:51    time: 1.3915  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:36    time: 1.3573  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:22    time: 1.3865  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:07    time: 1.3715  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:53    time: 1.3381  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:40    time: 1.4324  data: 0.0009  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:27    time: 1.5458  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:12    time: 1.4633  data: 0.0011  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:57    time: 1.2522  data: 0.0011  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:42    time: 1.1640  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:26    time: 1.0992  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:11    time: 1.1773  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:58    time: 1.3543  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:44    time: 1.4139  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:30    time: 1.4100  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:16    time: 1.4662  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:01    time: 1.2619  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:48    time: 1.2879  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:34    time: 1.4941  data: 0.0009  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:20    time: 1.4165  data: 0.0009  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:07    time: 1.4814  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:52    time: 1.4088  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:38    time: 1.3210  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:24    time: 1.3370  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:10    time: 1.3354  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:57    time: 1.4856  data: 0.0009  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:42    time: 1.3451  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:28    time: 1.2597  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:14    time: 1.4185  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:00    time: 1.4099  data: 0.0009  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:46    time: 1.3503  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:32    time: 1.3316  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:18    time: 1.3239  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:04    time: 1.3855  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4535  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3683  data: 0.0009  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3665  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4776  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4676  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2966  data: 0.0009  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1529  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2354  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3297  data: 0.0009  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3848  data: 0.0009  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4458  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3662  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3173  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2952  data: 0.0459  max mem: 66110
Evaluation Total time: 0:25:15 (1.3867 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_20_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [21]  [   0/7110]  eta: 2 days, 5:38:10  lr: 0.000010  loss: 0.7122  time: 27.1576  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [  50/7110]  eta: 2:55:40  lr: 0.000010  loss: 0.2542  time: 0.9800  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 100/7110]  eta: 2:26:26  lr: 0.000010  loss: 0.2212  time: 0.9979  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 150/7110]  eta: 2:15:11  lr: 0.000010  loss: 0.2447  time: 1.0020  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 200/7110]  eta: 2:09:18  lr: 0.000010  loss: 0.0686  time: 0.9868  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 250/7110]  eta: 2:05:43  lr: 0.000010  loss: 0.3527  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 300/7110]  eta: 2:02:38  lr: 0.000010  loss: 0.8141  time: 1.0248  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 350/7110]  eta: 2:00:07  lr: 0.000010  loss: 0.1175  time: 1.0266  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 400/7110]  eta: 1:58:08  lr: 0.000010  loss: 0.1287  time: 1.0031  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 450/7110]  eta: 1:56:20  lr: 0.000010  loss: 0.3296  time: 0.9613  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 500/7110]  eta: 1:55:34  lr: 0.000010  loss: 0.3824  time: 1.0421  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 550/7110]  eta: 1:54:34  lr: 0.000010  loss: 0.2587  time: 1.0478  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 600/7110]  eta: 1:52:57  lr: 0.000010  loss: 0.1572  time: 0.9871  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 650/7110]  eta: 1:51:40  lr: 0.000010  loss: 0.1765  time: 1.0160  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 700/7110]  eta: 1:50:31  lr: 0.000010  loss: 0.2299  time: 0.9975  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 750/7110]  eta: 1:49:20  lr: 0.000010  loss: 0.0581  time: 0.9861  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 800/7110]  eta: 1:48:13  lr: 0.000010  loss: 0.3419  time: 0.9868  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 850/7110]  eta: 1:47:09  lr: 0.000010  loss: 0.0977  time: 0.9940  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 900/7110]  eta: 1:46:13  lr: 0.000010  loss: 0.6846  time: 1.0152  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [ 950/7110]  eta: 1:45:10  lr: 0.000010  loss: 0.3533  time: 0.9610  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1000/7110]  eta: 1:44:22  lr: 0.000010  loss: 0.1951  time: 1.0352  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1050/7110]  eta: 1:43:26  lr: 0.000010  loss: 0.4946  time: 1.0519  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1100/7110]  eta: 1:42:34  lr: 0.000010  loss: 0.2141  time: 0.9994  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1150/7110]  eta: 1:41:41  lr: 0.000010  loss: 0.1913  time: 1.0272  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1200/7110]  eta: 1:40:44  lr: 0.000010  loss: 0.0839  time: 1.0360  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1250/7110]  eta: 1:39:42  lr: 0.000010  loss: 0.3999  time: 0.9726  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1300/7110]  eta: 1:38:47  lr: 0.000010  loss: 0.2306  time: 0.9925  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1350/7110]  eta: 1:38:00  lr: 0.000010  loss: 0.1035  time: 1.0242  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1400/7110]  eta: 1:37:08  lr: 0.000010  loss: 0.1365  time: 1.0135  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1450/7110]  eta: 1:36:17  lr: 0.000010  loss: 0.1100  time: 1.0025  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1500/7110]  eta: 1:35:22  lr: 0.000010  loss: 0.0790  time: 0.9820  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1550/7110]  eta: 1:34:22  lr: 0.000010  loss: 0.0361  time: 0.9711  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1600/7110]  eta: 1:33:24  lr: 0.000010  loss: 0.0908  time: 1.0506  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1650/7110]  eta: 1:32:32  lr: 0.000010  loss: 0.1529  time: 0.9957  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1700/7110]  eta: 1:31:39  lr: 0.000010  loss: 0.1987  time: 0.9718  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1750/7110]  eta: 1:30:42  lr: 0.000010  loss: 0.0309  time: 0.9451  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1800/7110]  eta: 1:29:49  lr: 0.000010  loss: 0.4794  time: 1.0620  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1850/7110]  eta: 1:28:54  lr: 0.000010  loss: 1.1138  time: 0.9696  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1900/7110]  eta: 1:27:56  lr: 0.000010  loss: 0.1102  time: 0.9591  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [1950/7110]  eta: 1:27:06  lr: 0.000010  loss: 0.4221  time: 1.0439  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2000/7110]  eta: 1:26:15  lr: 0.000010  loss: 0.1274  time: 0.9954  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2050/7110]  eta: 1:25:22  lr: 0.000010  loss: 0.0481  time: 0.9797  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2100/7110]  eta: 1:24:29  lr: 0.000010  loss: 0.0398  time: 0.9716  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2150/7110]  eta: 1:23:37  lr: 0.000010  loss: 1.6418  time: 0.9881  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2200/7110]  eta: 1:22:48  lr: 0.000010  loss: 0.3033  time: 1.0488  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2250/7110]  eta: 1:21:54  lr: 0.000010  loss: 0.3513  time: 0.9806  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2300/7110]  eta: 1:21:02  lr: 0.000010  loss: 0.0827  time: 0.9902  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2350/7110]  eta: 1:20:09  lr: 0.000010  loss: 0.2813  time: 0.9804  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2400/7110]  eta: 1:19:16  lr: 0.000010  loss: 0.0818  time: 0.9494  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2450/7110]  eta: 1:18:25  lr: 0.000010  loss: 0.2789  time: 1.0125  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2500/7110]  eta: 1:17:36  lr: 0.000010  loss: 0.6107  time: 0.9909  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2550/7110]  eta: 1:16:48  lr: 0.000010  loss: 0.1761  time: 0.9865  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2600/7110]  eta: 1:15:58  lr: 0.000010  loss: 0.0369  time: 0.9827  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2650/7110]  eta: 1:15:05  lr: 0.000010  loss: 0.0341  time: 0.9556  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2700/7110]  eta: 1:14:16  lr: 0.000010  loss: 0.2463  time: 1.0506  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2750/7110]  eta: 1:13:25  lr: 0.000010  loss: 0.0738  time: 0.9522  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2800/7110]  eta: 1:12:35  lr: 0.000010  loss: 0.0428  time: 0.9882  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2850/7110]  eta: 1:11:42  lr: 0.000010  loss: 0.2361  time: 0.9886  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2900/7110]  eta: 1:10:50  lr: 0.000010  loss: 0.3720  time: 0.9890  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [2950/7110]  eta: 1:09:59  lr: 0.000010  loss: 0.2026  time: 1.0054  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3000/7110]  eta: 1:09:11  lr: 0.000010  loss: 0.2820  time: 1.0445  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3050/7110]  eta: 1:08:20  lr: 0.000010  loss: 1.6043  time: 1.0219  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3100/7110]  eta: 1:07:30  lr: 0.000010  loss: 0.0117  time: 0.9864  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3150/7110]  eta: 1:06:40  lr: 0.000010  loss: 0.2267  time: 1.0506  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3200/7110]  eta: 1:05:49  lr: 0.000010  loss: 0.2306  time: 0.9836  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3250/7110]  eta: 1:04:58  lr: 0.000010  loss: 0.4343  time: 0.9751  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3300/7110]  eta: 1:04:06  lr: 0.000010  loss: 0.1113  time: 0.9510  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3350/7110]  eta: 1:03:15  lr: 0.000010  loss: 0.3283  time: 0.9519  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3400/7110]  eta: 1:02:25  lr: 0.000010  loss: 0.2021  time: 1.0048  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3450/7110]  eta: 1:01:35  lr: 0.000010  loss: 0.1029  time: 0.9712  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3500/7110]  eta: 1:00:45  lr: 0.000010  loss: 0.1450  time: 1.0547  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3550/7110]  eta: 0:59:56  lr: 0.000010  loss: 0.1534  time: 1.0236  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3600/7110]  eta: 0:59:06  lr: 0.000010  loss: 0.1609  time: 1.0687  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3650/7110]  eta: 0:58:16  lr: 0.000010  loss: 0.0930  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3700/7110]  eta: 0:57:26  lr: 0.000010  loss: 0.3320  time: 1.0130  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3750/7110]  eta: 0:56:36  lr: 0.000010  loss: 0.3550  time: 1.0432  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3800/7110]  eta: 0:55:47  lr: 0.000010  loss: 1.8417  time: 1.0535  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3850/7110]  eta: 0:54:56  lr: 0.000010  loss: 0.0453  time: 0.9907  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3900/7110]  eta: 0:54:04  lr: 0.000010  loss: 0.0666  time: 0.9695  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [3950/7110]  eta: 0:53:11  lr: 0.000010  loss: 0.1561  time: 0.9766  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4000/7110]  eta: 0:52:19  lr: 0.000010  loss: 0.6079  time: 1.0014  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4050/7110]  eta: 0:51:29  lr: 0.000010  loss: 0.2921  time: 1.0014  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4100/7110]  eta: 0:50:38  lr: 0.000010  loss: 0.5888  time: 0.9929  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4150/7110]  eta: 0:49:47  lr: 0.000010  loss: 0.5771  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4200/7110]  eta: 0:48:57  lr: 0.000010  loss: 0.3036  time: 0.9807  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4250/7110]  eta: 0:48:05  lr: 0.000010  loss: 0.3073  time: 0.9544  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4300/7110]  eta: 0:47:13  lr: 0.000010  loss: 0.1649  time: 0.9424  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4350/7110]  eta: 0:46:22  lr: 0.000010  loss: 0.6417  time: 0.9561  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4400/7110]  eta: 0:45:31  lr: 0.000010  loss: 0.0637  time: 1.0172  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4450/7110]  eta: 0:44:40  lr: 0.000010  loss: 0.8847  time: 0.9673  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4500/7110]  eta: 0:43:50  lr: 0.000010  loss: 0.0512  time: 0.9259  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4550/7110]  eta: 0:42:59  lr: 0.000010  loss: 0.2665  time: 0.9631  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4600/7110]  eta: 0:42:08  lr: 0.000010  loss: 0.3103  time: 0.9869  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4650/7110]  eta: 0:41:17  lr: 0.000010  loss: 0.1686  time: 1.0338  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4700/7110]  eta: 0:40:26  lr: 0.000010  loss: 0.1354  time: 0.9586  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4750/7110]  eta: 0:39:36  lr: 0.000010  loss: 0.4903  time: 1.0119  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4800/7110]  eta: 0:38:45  lr: 0.000010  loss: 0.0866  time: 0.9771  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4850/7110]  eta: 0:37:53  lr: 0.000010  loss: 0.4846  time: 0.9423  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4900/7110]  eta: 0:37:03  lr: 0.000010  loss: 0.2627  time: 0.9824  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [4950/7110]  eta: 0:36:12  lr: 0.000010  loss: 0.3564  time: 1.0128  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5000/7110]  eta: 0:35:22  lr: 0.000010  loss: 0.3709  time: 1.0127  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5050/7110]  eta: 0:34:32  lr: 0.000010  loss: 0.2365  time: 0.9773  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5100/7110]  eta: 0:33:40  lr: 0.000010  loss: 0.1311  time: 0.9313  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5150/7110]  eta: 0:32:50  lr: 0.000010  loss: 0.4200  time: 0.9965  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5200/7110]  eta: 0:31:59  lr: 0.000010  loss: 0.2183  time: 0.9477  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5250/7110]  eta: 0:31:09  lr: 0.000010  loss: 0.1991  time: 0.9965  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5300/7110]  eta: 0:30:19  lr: 0.000010  loss: 0.2117  time: 0.9682  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5350/7110]  eta: 0:29:28  lr: 0.000010  loss: 0.1450  time: 0.9656  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5400/7110]  eta: 0:28:38  lr: 0.000010  loss: 0.1238  time: 1.0140  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5450/7110]  eta: 0:27:47  lr: 0.000010  loss: 0.2058  time: 0.9782  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5500/7110]  eta: 0:26:57  lr: 0.000010  loss: 0.0431  time: 0.9713  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5550/7110]  eta: 0:26:07  lr: 0.000010  loss: 0.6288  time: 1.0176  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5600/7110]  eta: 0:25:16  lr: 0.000010  loss: 0.1247  time: 0.9915  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5650/7110]  eta: 0:24:26  lr: 0.000010  loss: 0.0911  time: 0.9718  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5700/7110]  eta: 0:23:36  lr: 0.000010  loss: 0.1144  time: 0.9719  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5750/7110]  eta: 0:22:45  lr: 0.000010  loss: 0.2926  time: 1.0270  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5800/7110]  eta: 0:21:55  lr: 0.000010  loss: 0.2794  time: 0.9922  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5850/7110]  eta: 0:21:05  lr: 0.000010  loss: 0.1188  time: 1.0019  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.0584  time: 1.0318  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [5950/7110]  eta: 0:19:24  lr: 0.000010  loss: 0.1774  time: 0.9586  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6000/7110]  eta: 0:18:34  lr: 0.000010  loss: 0.3118  time: 1.0226  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.3921  time: 0.9556  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.3810  time: 0.9340  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.2668  time: 0.9749  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.1399  time: 0.9652  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.1299  time: 1.0361  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.2043  time: 1.0281  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.1621  time: 1.0059  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.2810  time: 1.0729  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.0850  time: 0.9919  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.3969  time: 1.0478  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.3546  time: 1.0411  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.4673  time: 1.0603  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1521  time: 1.0080  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.0960  time: 0.9761  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.7710  time: 1.0511  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.0472  time: 0.9926  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.6815  time: 0.9535  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1667  time: 0.9574  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.3510  time: 1.0623  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.4227  time: 0.9838  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.6855  time: 0.9740  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1506  time: 1.0054  data: 0.0000  max mem: 66110
Train: data epoch: [21]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.2210  time: 1.1110  data: 0.0000  max mem: 66110
Train: data epoch: [21] Total time: 1:58:53 (1.0033 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:29:40    time: 21.3907  data: 19.9638  max mem: 66110
Evaluation  [  10/1093]  eta: 0:58:14    time: 3.2268  data: 1.8157  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:36    time: 1.3738  data: 0.0009  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:50    time: 1.2742  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:08    time: 1.3120  data: 0.0009  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:26    time: 1.4171  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:13    time: 1.2443  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:31    time: 1.2654  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:42    time: 1.4055  data: 0.0012  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:02    time: 1.3585  data: 0.0011  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:34    time: 1.4007  data: 0.0009  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:10    time: 1.4376  data: 0.0009  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:37    time: 1.3809  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:26    time: 1.4478  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:49    time: 1.3910  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:21    time: 1.2454  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:02    time: 1.3470  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:34    time: 1.3239  data: 0.0009  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:15    time: 1.3118  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:02    time: 1.4405  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:40    time: 1.3942  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:36    time: 1.5057  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:11    time: 1.4594  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:50    time: 1.2392  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:32    time: 1.3246  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:17    time: 1.3901  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:10    time: 1.5524  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:45    time: 1.4048  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:26    time: 1.1888  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:18    time: 1.4709  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:57    time: 1.4229  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:38    time: 1.2160  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:21    time: 1.2934  data: 0.0009  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:04    time: 1.3176  data: 0.0009  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:52    time: 1.4225  data: 0.0009  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:38    time: 1.4769  data: 0.0009  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:19    time: 1.3017  data: 0.0009  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:03    time: 1.2630  data: 0.0009  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:52    time: 1.4686  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:36    time: 1.4550  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:23    time: 1.3985  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:09    time: 1.4521  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:56    time: 1.4606  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:41    time: 1.4248  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:23    time: 1.2430  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:08    time: 1.2543  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:53    time: 1.3685  data: 0.0009  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:37    time: 1.3161  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:23    time: 1.3583  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:11    time: 1.4841  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:59    time: 1.5644  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:46    time: 1.5765  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:32    time: 1.4990  data: 0.0009  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:19    time: 1.4624  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:05    time: 1.4672  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:51    time: 1.4689  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:34    time: 1.2797  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:19    time: 1.2143  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:05    time: 1.3559  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:48    time: 1.2477  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:35    time: 1.3540  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:21    time: 1.5042  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:07    time: 1.3945  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:53    time: 1.4048  data: 0.0009  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:38    time: 1.3743  data: 0.0009  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:25    time: 1.3900  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:10    time: 1.3813  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:56    time: 1.3511  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:42    time: 1.4239  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:28    time: 1.4716  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:13    time: 1.4010  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:58    time: 1.2247  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:42    time: 1.1530  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:27    time: 1.1143  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:12    time: 1.1407  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:58    time: 1.3166  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:44    time: 1.3987  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:30    time: 1.3676  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:17    time: 1.4591  data: 0.0009  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:02    time: 1.3272  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:49    time: 1.3671  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:35    time: 1.5310  data: 0.0009  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:21    time: 1.4616  data: 0.0009  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:08    time: 1.5263  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:53    time: 1.4073  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:39    time: 1.3087  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:25    time: 1.3195  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:11    time: 1.3329  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:57    time: 1.4900  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:43    time: 1.3640  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:29    time: 1.2825  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:15    time: 1.4216  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:01    time: 1.4389  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.3792  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3019  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.2988  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.3652  data: 0.0009  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4394  data: 0.0009  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3738  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3455  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4399  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4482  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.3184  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1734  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2368  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3314  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3716  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4437  data: 0.0011  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3865  data: 0.0011  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3257  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2962  data: 0.0444  max mem: 66110
Evaluation Total time: 0:25:18 (1.3891 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_21_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [22]  [   0/7110]  eta: 2 days, 5:49:40  lr: 0.000010  loss: 0.4957  time: 27.2546  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [  50/7110]  eta: 2:55:06  lr: 0.000010  loss: 0.0842  time: 0.9989  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 100/7110]  eta: 2:25:05  lr: 0.000010  loss: 0.1035  time: 1.0040  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 150/7110]  eta: 2:14:25  lr: 0.000010  loss: 0.4445  time: 0.9647  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 200/7110]  eta: 2:09:28  lr: 0.000010  loss: 0.0355  time: 1.0576  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 250/7110]  eta: 2:05:14  lr: 0.000010  loss: 0.0934  time: 0.9990  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 300/7110]  eta: 2:02:25  lr: 0.000010  loss: 0.0864  time: 0.9916  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 350/7110]  eta: 2:00:59  lr: 0.000010  loss: 0.1669  time: 1.0651  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 400/7110]  eta: 1:58:46  lr: 0.000010  loss: 0.0953  time: 0.9730  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 450/7110]  eta: 1:57:24  lr: 0.000010  loss: 0.7345  time: 1.0885  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 500/7110]  eta: 1:55:47  lr: 0.000010  loss: 0.3776  time: 0.9994  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 550/7110]  eta: 1:54:32  lr: 0.000010  loss: 0.0777  time: 0.9671  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 600/7110]  eta: 1:53:29  lr: 0.000010  loss: 0.4738  time: 1.0609  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 650/7110]  eta: 1:52:11  lr: 0.000010  loss: 0.3387  time: 0.9979  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 700/7110]  eta: 1:51:12  lr: 0.000010  loss: 0.9817  time: 1.0258  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 750/7110]  eta: 1:49:53  lr: 0.000010  loss: 0.0730  time: 1.0012  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 800/7110]  eta: 1:48:33  lr: 0.000010  loss: 0.1298  time: 0.9274  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 850/7110]  eta: 1:47:33  lr: 0.000010  loss: 0.2447  time: 0.9958  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 900/7110]  eta: 1:46:34  lr: 0.000010  loss: 0.1360  time: 0.9640  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [ 950/7110]  eta: 1:45:44  lr: 0.000010  loss: 0.2382  time: 1.0011  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1000/7110]  eta: 1:44:36  lr: 0.000010  loss: 0.0827  time: 1.0051  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1050/7110]  eta: 1:43:36  lr: 0.000010  loss: 0.2721  time: 1.0077  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1100/7110]  eta: 1:42:32  lr: 0.000010  loss: 0.0921  time: 0.9796  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1150/7110]  eta: 1:41:32  lr: 0.000010  loss: 0.1073  time: 0.9997  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1200/7110]  eta: 1:40:30  lr: 0.000010  loss: 0.5391  time: 0.9929  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1250/7110]  eta: 1:39:38  lr: 0.000010  loss: 0.0966  time: 1.0611  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1300/7110]  eta: 1:38:50  lr: 0.000010  loss: 0.1962  time: 1.0415  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1350/7110]  eta: 1:37:52  lr: 0.000010  loss: 0.0200  time: 0.9678  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1400/7110]  eta: 1:37:08  lr: 0.000010  loss: 0.3161  time: 1.0628  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1450/7110]  eta: 1:36:12  lr: 0.000010  loss: 0.3977  time: 0.9748  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1500/7110]  eta: 1:35:15  lr: 0.000010  loss: 0.5088  time: 0.9808  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1550/7110]  eta: 1:34:19  lr: 0.000010  loss: 0.2042  time: 1.0136  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1600/7110]  eta: 1:33:26  lr: 0.000010  loss: 0.4448  time: 0.9844  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1650/7110]  eta: 1:32:32  lr: 0.000010  loss: 0.0777  time: 0.9990  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1700/7110]  eta: 1:31:37  lr: 0.000010  loss: 0.2138  time: 0.9944  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1750/7110]  eta: 1:30:51  lr: 0.000010  loss: 0.4007  time: 1.0089  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1800/7110]  eta: 1:29:56  lr: 0.000010  loss: 0.3413  time: 0.9683  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1850/7110]  eta: 1:29:09  lr: 0.000010  loss: 0.0612  time: 1.0195  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1900/7110]  eta: 1:28:14  lr: 0.000010  loss: 0.3373  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [1950/7110]  eta: 1:27:25  lr: 0.000010  loss: 0.1887  time: 1.0167  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2000/7110]  eta: 1:26:30  lr: 0.000010  loss: 0.1473  time: 0.9986  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2050/7110]  eta: 1:25:34  lr: 0.000010  loss: 0.4196  time: 1.0441  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2100/7110]  eta: 1:24:40  lr: 0.000010  loss: 0.8624  time: 1.0103  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2150/7110]  eta: 1:23:47  lr: 0.000010  loss: 0.0567  time: 1.0287  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2200/7110]  eta: 1:22:56  lr: 0.000010  loss: 0.0381  time: 1.0218  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2250/7110]  eta: 1:22:02  lr: 0.000010  loss: 0.5331  time: 0.9801  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2300/7110]  eta: 1:21:11  lr: 0.000010  loss: 1.7725  time: 0.9935  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2350/7110]  eta: 1:20:20  lr: 0.000010  loss: 0.1406  time: 1.0302  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2400/7110]  eta: 1:19:28  lr: 0.000010  loss: 0.1168  time: 1.0043  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2450/7110]  eta: 1:18:37  lr: 0.000010  loss: 0.9639  time: 1.0790  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2500/7110]  eta: 1:17:42  lr: 0.000010  loss: 0.0717  time: 0.9318  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2550/7110]  eta: 1:16:54  lr: 0.000010  loss: 0.1726  time: 1.0393  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2600/7110]  eta: 1:16:01  lr: 0.000010  loss: 0.1186  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2650/7110]  eta: 1:15:06  lr: 0.000010  loss: 0.0964  time: 0.9674  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2700/7110]  eta: 1:14:18  lr: 0.000010  loss: 0.3159  time: 1.0509  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2750/7110]  eta: 1:13:25  lr: 0.000010  loss: 0.0544  time: 0.9596  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2800/7110]  eta: 1:12:35  lr: 0.000010  loss: 0.3864  time: 1.0264  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2850/7110]  eta: 1:11:45  lr: 0.000010  loss: 0.2320  time: 1.0326  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2900/7110]  eta: 1:10:54  lr: 0.000010  loss: 0.0645  time: 0.9831  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [2950/7110]  eta: 1:10:03  lr: 0.000010  loss: 0.0489  time: 1.0182  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3000/7110]  eta: 1:09:11  lr: 0.000010  loss: 0.5553  time: 0.9723  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3050/7110]  eta: 1:08:22  lr: 0.000010  loss: 0.2738  time: 1.0712  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3100/7110]  eta: 1:07:32  lr: 0.000010  loss: 0.1011  time: 0.9824  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3150/7110]  eta: 1:06:39  lr: 0.000010  loss: 0.0388  time: 0.9992  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3200/7110]  eta: 1:05:47  lr: 0.000010  loss: 0.0444  time: 1.0048  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3250/7110]  eta: 1:04:55  lr: 0.000010  loss: 0.1771  time: 0.9179  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3300/7110]  eta: 1:04:05  lr: 0.000010  loss: 1.1325  time: 1.0159  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3350/7110]  eta: 1:03:13  lr: 0.000010  loss: 0.0559  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3400/7110]  eta: 1:02:19  lr: 0.000010  loss: 0.1353  time: 0.9505  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3450/7110]  eta: 1:01:28  lr: 0.000010  loss: 0.1510  time: 1.0068  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3500/7110]  eta: 1:00:39  lr: 0.000010  loss: 0.1594  time: 1.0244  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3550/7110]  eta: 0:59:49  lr: 0.000010  loss: 0.3667  time: 1.0200  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3600/7110]  eta: 0:59:00  lr: 0.000010  loss: 0.3287  time: 1.0586  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3650/7110]  eta: 0:58:09  lr: 0.000010  loss: 0.3454  time: 1.0021  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3700/7110]  eta: 0:57:17  lr: 0.000010  loss: 0.0558  time: 0.9835  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3750/7110]  eta: 0:56:24  lr: 0.000010  loss: 0.2250  time: 0.9481  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3800/7110]  eta: 0:55:32  lr: 0.000010  loss: 0.1131  time: 0.9705  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3850/7110]  eta: 0:54:40  lr: 0.000010  loss: 0.0627  time: 1.0142  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3900/7110]  eta: 0:53:50  lr: 0.000010  loss: 0.0901  time: 0.9423  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [3950/7110]  eta: 0:52:58  lr: 0.000010  loss: 0.0474  time: 0.9233  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4000/7110]  eta: 0:52:07  lr: 0.000010  loss: 0.8896  time: 1.0402  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4050/7110]  eta: 0:51:18  lr: 0.000010  loss: 0.3530  time: 0.9964  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4100/7110]  eta: 0:50:27  lr: 0.000010  loss: 0.1315  time: 1.0018  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4150/7110]  eta: 0:49:36  lr: 0.000010  loss: 0.3941  time: 0.9668  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4200/7110]  eta: 0:48:47  lr: 0.000010  loss: 0.2938  time: 1.0317  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4250/7110]  eta: 0:47:57  lr: 0.000010  loss: 0.4155  time: 1.0806  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4300/7110]  eta: 0:47:06  lr: 0.000010  loss: 0.5127  time: 0.9606  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4350/7110]  eta: 0:46:16  lr: 0.000010  loss: 0.0810  time: 0.9884  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4400/7110]  eta: 0:45:25  lr: 0.000010  loss: 0.0769  time: 0.9752  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4450/7110]  eta: 0:44:34  lr: 0.000010  loss: 0.6476  time: 1.0007  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4500/7110]  eta: 0:43:43  lr: 0.000010  loss: 0.2800  time: 0.9820  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4550/7110]  eta: 0:42:53  lr: 0.000010  loss: 0.1037  time: 0.9905  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4600/7110]  eta: 0:42:01  lr: 0.000010  loss: 0.1736  time: 0.9636  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4650/7110]  eta: 0:41:10  lr: 0.000010  loss: 0.6557  time: 0.9213  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4700/7110]  eta: 0:40:20  lr: 0.000010  loss: 0.0529  time: 0.9756  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4750/7110]  eta: 0:39:29  lr: 0.000010  loss: 0.3376  time: 0.9765  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4800/7110]  eta: 0:38:39  lr: 0.000010  loss: 0.0588  time: 1.0125  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4850/7110]  eta: 0:37:49  lr: 0.000010  loss: 0.1310  time: 0.9975  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4900/7110]  eta: 0:36:59  lr: 0.000010  loss: 0.6175  time: 1.0102  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [4950/7110]  eta: 0:36:08  lr: 0.000010  loss: 0.3987  time: 1.0327  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5000/7110]  eta: 0:35:18  lr: 0.000010  loss: 0.1463  time: 0.9600  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5050/7110]  eta: 0:34:27  lr: 0.000010  loss: 0.1503  time: 1.0043  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5100/7110]  eta: 0:33:37  lr: 0.000010  loss: 0.1743  time: 0.9459  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5150/7110]  eta: 0:32:47  lr: 0.000010  loss: 0.0770  time: 1.0032  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5200/7110]  eta: 0:31:57  lr: 0.000010  loss: 0.3534  time: 1.0159  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5250/7110]  eta: 0:31:07  lr: 0.000010  loss: 0.0450  time: 0.9955  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5300/7110]  eta: 0:30:16  lr: 0.000010  loss: 0.3045  time: 0.9871  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5350/7110]  eta: 0:29:26  lr: 0.000010  loss: 0.2285  time: 1.0003  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5400/7110]  eta: 0:28:35  lr: 0.000010  loss: 0.7818  time: 0.9789  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5450/7110]  eta: 0:27:45  lr: 0.000010  loss: 0.2663  time: 0.9681  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5500/7110]  eta: 0:26:55  lr: 0.000010  loss: 0.9669  time: 1.0461  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5550/7110]  eta: 0:26:05  lr: 0.000010  loss: 0.1506  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5600/7110]  eta: 0:25:14  lr: 0.000010  loss: 0.4009  time: 1.0249  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5650/7110]  eta: 0:24:25  lr: 0.000010  loss: 0.4196  time: 1.0640  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.4858  time: 1.0171  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.4694  time: 1.0338  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.0849  time: 1.0180  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 0.2807  time: 0.9758  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.0516  time: 1.0071  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [5950/7110]  eta: 0:19:24  lr: 0.000010  loss: 0.2650  time: 0.9794  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6000/7110]  eta: 0:18:34  lr: 0.000010  loss: 0.1951  time: 0.9987  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6050/7110]  eta: 0:17:44  lr: 0.000010  loss: 0.1058  time: 1.0332  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.0861  time: 1.0118  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.4679  time: 1.0341  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.2082  time: 0.9949  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.4869  time: 1.0035  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6300/7110]  eta: 0:13:33  lr: 0.000010  loss: 0.0334  time: 1.0377  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.4544  time: 0.9759  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.2105  time: 0.9126  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.3829  time: 1.0065  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.6228  time: 1.0682  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.5217  time: 0.9869  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.2567  time: 1.0966  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.3092  time: 1.0656  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.0402  time: 0.9317  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.3569  time: 1.0557  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.2644  time: 0.9706  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.0733  time: 0.9247  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 1.0020  time: 0.9850  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1544  time: 0.9712  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.0259  time: 0.9786  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.3362  time: 0.9862  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.2069  time: 0.9243  data: 0.0000  max mem: 66110
Train: data epoch: [22]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.3405  time: 1.0653  data: 0.0000  max mem: 66110
Train: data epoch: [22] Total time: 1:58:47 (1.0025 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:23:12    time: 21.0365  data: 19.7755  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:37    time: 3.1922  data: 1.7986  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:30    time: 1.3856  data: 0.0009  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:38    time: 1.2755  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:48    time: 1.2786  data: 0.0009  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:13    time: 1.4023  data: 0.0009  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:05    time: 1.2600  data: 0.0009  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:23    time: 1.2705  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:34    time: 1.3974  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:03    time: 1.3898  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:40    time: 1.4588  data: 0.0009  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:18    time: 1.4789  data: 0.0009  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:42    time: 1.3873  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:26    time: 1.4014  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:49    time: 1.3539  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:24    time: 1.2688  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:02    time: 1.3570  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:34    time: 1.3041  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:16    time: 1.3186  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:04    time: 1.4567  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:41    time: 1.4003  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:34    time: 1.4659  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:08    time: 1.4109  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:47    time: 1.2297  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:31    time: 1.3311  data: 0.0009  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:16    time: 1.4088  data: 0.0009  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:09    time: 1.5502  data: 0.0009  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:44    time: 1.3906  data: 0.0009  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:25    time: 1.1951  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:17    time: 1.4826  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:56    time: 1.4338  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:39    time: 1.2443  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:23    time: 1.3409  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:06    time: 1.3465  data: 0.0009  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:54    time: 1.4186  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:40    time: 1.4694  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:21    time: 1.3127  data: 0.0009  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:04    time: 1.2401  data: 0.0009  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:53    time: 1.4246  data: 0.0009  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:37    time: 1.4704  data: 0.0009  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:24    time: 1.4212  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:10    time: 1.4387  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:57    time: 1.4497  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:41    time: 1.4164  data: 0.0009  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:22    time: 1.2067  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:07    time: 1.2297  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:53    time: 1.3731  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:37    time: 1.3267  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:23    time: 1.3673  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:10    time: 1.4753  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:59    time: 1.5629  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:46    time: 1.5810  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:32    time: 1.4864  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:19    time: 1.4710  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:05    time: 1.4932  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:51    time: 1.4301  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:33    time: 1.2489  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:19    time: 1.2354  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:04    time: 1.3729  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:48    time: 1.2793  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:35    time: 1.3666  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:21    time: 1.4995  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:07    time: 1.3898  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:52    time: 1.3564  data: 0.0009  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:38    time: 1.3429  data: 0.0009  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:24    time: 1.3992  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:09    time: 1.3550  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:55    time: 1.3221  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:41    time: 1.4212  data: 0.0009  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:28    time: 1.5029  data: 0.0009  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:13    time: 1.4522  data: 0.0009  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:58    time: 1.2642  data: 0.0009  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:43    time: 1.2113  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:28    time: 1.1778  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:13    time: 1.2093  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:59    time: 1.3399  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:45    time: 1.3869  data: 0.0009  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:31    time: 1.3697  data: 0.0009  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:17    time: 1.4317  data: 0.0009  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:02    time: 1.3080  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:49    time: 1.3659  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:35    time: 1.5315  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:21    time: 1.4496  data: 0.0009  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:08    time: 1.5106  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:54    time: 1.4074  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:40    time: 1.3411  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:25    time: 1.3424  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:11    time: 1.3385  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:58    time: 1.5100  data: 0.0009  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:43    time: 1.3669  data: 0.0009  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:29    time: 1.2788  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:15    time: 1.4254  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:02    time: 1.4648  data: 0.0009  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.4088  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3510  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.3427  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.3752  data: 0.0009  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4335  data: 0.0009  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3277  data: 0.0009  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3218  data: 0.0009  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4328  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4280  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2773  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1421  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2441  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3394  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3815  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4356  data: 0.0009  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3758  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3305  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3037  data: 0.0411  max mem: 66110
Evaluation Total time: 0:25:20 (1.3910 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_22_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [23]  [   0/7110]  eta: 2 days, 5:54:17  lr: 0.000010  loss: 0.2744  time: 27.2936  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [  50/7110]  eta: 2:59:28  lr: 0.000010  loss: 0.0874  time: 1.0368  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 100/7110]  eta: 2:30:38  lr: 0.000010  loss: 0.0518  time: 1.1144  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 150/7110]  eta: 2:18:35  lr: 0.000010  loss: 0.2113  time: 1.0227  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 200/7110]  eta: 2:12:28  lr: 0.000010  loss: 0.7165  time: 0.9877  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 250/7110]  eta: 2:07:47  lr: 0.000010  loss: 0.5122  time: 1.0020  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 300/7110]  eta: 2:03:50  lr: 0.000010  loss: 0.1318  time: 0.9340  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 350/7110]  eta: 2:01:48  lr: 0.000010  loss: 0.1552  time: 1.0619  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 400/7110]  eta: 2:00:02  lr: 0.000010  loss: 0.1645  time: 1.0398  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 450/7110]  eta: 1:58:17  lr: 0.000010  loss: 0.3072  time: 0.9851  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 500/7110]  eta: 1:56:41  lr: 0.000010  loss: 0.4779  time: 1.0122  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 550/7110]  eta: 1:55:18  lr: 0.000010  loss: 0.4593  time: 1.0202  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 600/7110]  eta: 1:53:56  lr: 0.000010  loss: 0.2046  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 650/7110]  eta: 1:52:59  lr: 0.000010  loss: 0.0675  time: 1.0289  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 700/7110]  eta: 1:51:36  lr: 0.000010  loss: 0.0924  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 750/7110]  eta: 1:50:13  lr: 0.000010  loss: 0.2101  time: 0.9653  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 800/7110]  eta: 1:49:14  lr: 0.000010  loss: 0.5480  time: 1.0196  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 850/7110]  eta: 1:48:17  lr: 0.000010  loss: 0.4511  time: 0.9931  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 900/7110]  eta: 1:47:04  lr: 0.000010  loss: 0.1065  time: 0.9311  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [ 950/7110]  eta: 1:46:15  lr: 0.000010  loss: 0.3560  time: 1.0662  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1000/7110]  eta: 1:45:14  lr: 0.000010  loss: 0.2143  time: 0.9867  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1050/7110]  eta: 1:44:17  lr: 0.000010  loss: 0.6362  time: 0.9685  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1100/7110]  eta: 1:43:13  lr: 0.000010  loss: 0.8707  time: 0.9623  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1150/7110]  eta: 1:42:07  lr: 0.000010  loss: 0.5178  time: 0.9840  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1200/7110]  eta: 1:41:12  lr: 0.000010  loss: 1.1562  time: 1.0014  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1250/7110]  eta: 1:40:18  lr: 0.000010  loss: 0.1045  time: 0.9808  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1300/7110]  eta: 1:39:18  lr: 0.000010  loss: 0.4768  time: 0.9483  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1350/7110]  eta: 1:38:23  lr: 0.000010  loss: 0.1718  time: 1.0086  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1400/7110]  eta: 1:37:16  lr: 0.000010  loss: 0.1398  time: 0.9380  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1450/7110]  eta: 1:36:16  lr: 0.000010  loss: 0.1845  time: 0.9620  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1500/7110]  eta: 1:35:19  lr: 0.000010  loss: 0.2022  time: 0.9925  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1550/7110]  eta: 1:34:22  lr: 0.000010  loss: 0.6593  time: 1.0283  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1600/7110]  eta: 1:33:17  lr: 0.000010  loss: 0.0826  time: 0.8931  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1650/7110]  eta: 1:32:23  lr: 0.000010  loss: 0.1991  time: 0.9768  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1700/7110]  eta: 1:31:32  lr: 0.000010  loss: 0.1107  time: 0.9845  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1750/7110]  eta: 1:30:36  lr: 0.000010  loss: 1.6217  time: 0.9495  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1800/7110]  eta: 1:29:38  lr: 0.000010  loss: 0.0630  time: 0.9404  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1850/7110]  eta: 1:28:48  lr: 0.000010  loss: 0.5094  time: 1.0083  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1900/7110]  eta: 1:27:55  lr: 0.000010  loss: 0.0898  time: 0.9789  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [1950/7110]  eta: 1:27:01  lr: 0.000010  loss: 0.1840  time: 0.9401  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2000/7110]  eta: 1:26:13  lr: 0.000010  loss: 0.7474  time: 1.0419  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2050/7110]  eta: 1:25:21  lr: 0.000010  loss: 0.0913  time: 1.0265  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2100/7110]  eta: 1:24:27  lr: 0.000010  loss: 0.4418  time: 1.0514  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2150/7110]  eta: 1:23:32  lr: 0.000010  loss: 0.1370  time: 0.9933  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2200/7110]  eta: 1:22:41  lr: 0.000010  loss: 1.1332  time: 1.0595  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2250/7110]  eta: 1:21:47  lr: 0.000010  loss: 0.1634  time: 0.9735  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2300/7110]  eta: 1:20:54  lr: 0.000010  loss: 0.0762  time: 0.9814  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2350/7110]  eta: 1:20:04  lr: 0.000010  loss: 0.0697  time: 1.0631  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2400/7110]  eta: 1:19:11  lr: 0.000010  loss: 0.2649  time: 0.9542  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2450/7110]  eta: 1:18:19  lr: 0.000010  loss: 0.0330  time: 0.9864  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2500/7110]  eta: 1:17:30  lr: 0.000010  loss: 0.1902  time: 0.9506  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2550/7110]  eta: 1:16:36  lr: 0.000010  loss: 0.1711  time: 0.9301  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2600/7110]  eta: 1:15:44  lr: 0.000010  loss: 1.4945  time: 1.0037  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2650/7110]  eta: 1:14:55  lr: 0.000010  loss: 0.2193  time: 1.0870  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2700/7110]  eta: 1:14:06  lr: 0.000010  loss: 0.5910  time: 1.0304  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2750/7110]  eta: 1:13:14  lr: 0.000010  loss: 0.0628  time: 0.9864  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2800/7110]  eta: 1:12:26  lr: 0.000010  loss: 0.0674  time: 1.0833  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2850/7110]  eta: 1:11:34  lr: 0.000010  loss: 0.6228  time: 1.0042  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2900/7110]  eta: 1:10:45  lr: 0.000010  loss: 0.1961  time: 1.0430  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [2950/7110]  eta: 1:09:53  lr: 0.000010  loss: 0.7175  time: 1.0297  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3000/7110]  eta: 1:09:03  lr: 0.000010  loss: 0.3268  time: 0.9816  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3050/7110]  eta: 1:08:11  lr: 0.000010  loss: 0.3993  time: 0.9591  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3100/7110]  eta: 1:07:19  lr: 0.000010  loss: 0.1091  time: 0.9868  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3150/7110]  eta: 1:06:30  lr: 0.000010  loss: 0.0255  time: 1.0540  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3200/7110]  eta: 1:05:41  lr: 0.000010  loss: 0.3664  time: 1.0424  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3250/7110]  eta: 1:04:49  lr: 0.000010  loss: 0.3739  time: 0.9666  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3300/7110]  eta: 1:04:01  lr: 0.000010  loss: 0.3876  time: 1.0142  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3350/7110]  eta: 1:03:09  lr: 0.000010  loss: 0.1215  time: 0.9620  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3400/7110]  eta: 1:02:18  lr: 0.000010  loss: 0.2328  time: 0.9730  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3450/7110]  eta: 1:01:25  lr: 0.000010  loss: 0.1381  time: 0.9943  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3500/7110]  eta: 1:00:33  lr: 0.000010  loss: 0.1880  time: 0.9387  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3550/7110]  eta: 0:59:42  lr: 0.000010  loss: 0.2503  time: 0.9918  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3600/7110]  eta: 0:58:51  lr: 0.000010  loss: 0.4313  time: 1.0092  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3650/7110]  eta: 0:58:01  lr: 0.000010  loss: 0.5195  time: 0.9546  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3700/7110]  eta: 0:57:09  lr: 0.000010  loss: 0.2346  time: 0.9526  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3750/7110]  eta: 0:56:20  lr: 0.000010  loss: 0.2857  time: 1.0505  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3800/7110]  eta: 0:55:29  lr: 0.000010  loss: 0.1192  time: 1.0101  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3850/7110]  eta: 0:54:40  lr: 0.000010  loss: 1.2821  time: 0.9931  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3900/7110]  eta: 0:53:49  lr: 0.000010  loss: 0.6747  time: 0.9781  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [3950/7110]  eta: 0:52:58  lr: 0.000010  loss: 0.4425  time: 0.9745  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4000/7110]  eta: 0:52:08  lr: 0.000010  loss: 0.1179  time: 1.0100  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4050/7110]  eta: 0:51:17  lr: 0.000010  loss: 0.0978  time: 0.9981  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4100/7110]  eta: 0:50:25  lr: 0.000010  loss: 0.4226  time: 0.9919  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4150/7110]  eta: 0:49:35  lr: 0.000010  loss: 0.1439  time: 0.9924  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4200/7110]  eta: 0:48:44  lr: 0.000010  loss: 0.5881  time: 0.9838  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4250/7110]  eta: 0:47:52  lr: 0.000010  loss: 0.3944  time: 0.9576  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4300/7110]  eta: 0:47:02  lr: 0.000010  loss: 0.1840  time: 0.9983  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4350/7110]  eta: 0:46:11  lr: 0.000010  loss: 0.0679  time: 0.9533  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4400/7110]  eta: 0:45:20  lr: 0.000010  loss: 0.2834  time: 0.9978  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4450/7110]  eta: 0:44:30  lr: 0.000010  loss: 0.2454  time: 0.9666  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4500/7110]  eta: 0:43:39  lr: 0.000010  loss: 0.2293  time: 0.9847  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4550/7110]  eta: 0:42:48  lr: 0.000010  loss: 0.1556  time: 0.9378  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4600/7110]  eta: 0:41:57  lr: 0.000010  loss: 0.4363  time: 0.9635  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4650/7110]  eta: 0:41:07  lr: 0.000010  loss: 0.1443  time: 1.0001  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4700/7110]  eta: 0:40:17  lr: 0.000010  loss: 0.2421  time: 1.0569  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4750/7110]  eta: 0:39:27  lr: 0.000010  loss: 0.2082  time: 1.0294  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4800/7110]  eta: 0:38:37  lr: 0.000010  loss: 0.7702  time: 1.0384  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4850/7110]  eta: 0:37:47  lr: 0.000010  loss: 0.2951  time: 0.9708  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4900/7110]  eta: 0:36:57  lr: 0.000010  loss: 0.2470  time: 1.0227  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [4950/7110]  eta: 0:36:06  lr: 0.000010  loss: 0.1297  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5000/7110]  eta: 0:35:15  lr: 0.000010  loss: 0.0734  time: 0.9960  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5050/7110]  eta: 0:34:25  lr: 0.000010  loss: 0.3355  time: 0.9546  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5100/7110]  eta: 0:33:34  lr: 0.000010  loss: 0.2805  time: 0.9835  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5150/7110]  eta: 0:32:44  lr: 0.000010  loss: 0.0130  time: 0.9691  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5200/7110]  eta: 0:31:53  lr: 0.000010  loss: 0.3193  time: 0.9646  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5250/7110]  eta: 0:31:03  lr: 0.000010  loss: 0.1965  time: 1.0505  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5300/7110]  eta: 0:30:13  lr: 0.000010  loss: 0.4224  time: 0.9755  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5350/7110]  eta: 0:29:23  lr: 0.000010  loss: 0.1967  time: 0.9461  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5400/7110]  eta: 0:28:32  lr: 0.000010  loss: 0.3232  time: 0.9448  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5450/7110]  eta: 0:27:43  lr: 0.000010  loss: 0.3601  time: 1.0760  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5500/7110]  eta: 0:26:53  lr: 0.000010  loss: 0.1864  time: 0.9930  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5550/7110]  eta: 0:26:02  lr: 0.000010  loss: 0.1643  time: 0.9558  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5600/7110]  eta: 0:25:12  lr: 0.000010  loss: 0.1424  time: 1.0044  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5650/7110]  eta: 0:24:22  lr: 0.000010  loss: 0.2841  time: 1.0453  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5700/7110]  eta: 0:23:32  lr: 0.000010  loss: 0.3540  time: 0.9992  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5750/7110]  eta: 0:22:42  lr: 0.000010  loss: 0.2820  time: 0.9819  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5800/7110]  eta: 0:21:52  lr: 0.000010  loss: 0.1110  time: 1.0160  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5850/7110]  eta: 0:21:02  lr: 0.000010  loss: 0.3554  time: 0.9415  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5900/7110]  eta: 0:20:12  lr: 0.000010  loss: 0.1646  time: 0.9123  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [5950/7110]  eta: 0:19:22  lr: 0.000010  loss: 0.8738  time: 0.9932  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6000/7110]  eta: 0:18:32  lr: 0.000010  loss: 0.1853  time: 1.0728  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6050/7110]  eta: 0:17:41  lr: 0.000010  loss: 0.1914  time: 0.9858  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6100/7110]  eta: 0:16:51  lr: 0.000010  loss: 0.2734  time: 1.0327  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6150/7110]  eta: 0:16:01  lr: 0.000010  loss: 0.1132  time: 0.9342  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6200/7110]  eta: 0:15:11  lr: 0.000010  loss: 1.0665  time: 0.9894  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6250/7110]  eta: 0:14:21  lr: 0.000010  loss: 0.5432  time: 0.9977  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6300/7110]  eta: 0:13:31  lr: 0.000010  loss: 0.8645  time: 0.9887  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.0947  time: 1.0265  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.2910  time: 0.9597  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.0898  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.1669  time: 0.9931  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6550/7110]  eta: 0:09:20  lr: 0.000010  loss: 0.6891  time: 0.9492  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.4497  time: 1.0313  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.0789  time: 1.0756  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.0977  time: 0.9703  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.0957  time: 0.9997  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.1595  time: 1.0007  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.0813  time: 0.9848  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.4050  time: 0.9948  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.5499  time: 0.9479  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.4832  time: 0.9660  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.0949  time: 0.9600  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.2293  time: 0.9331  data: 0.0000  max mem: 66110
Train: data epoch: [23]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.3903  time: 1.0713  data: 0.0000  max mem: 66110
Train: data epoch: [23] Total time: 1:58:45 (1.0022 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:21:01    time: 20.9162  data: 19.6526  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:25    time: 3.1814  data: 1.7875  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:31    time: 1.3923  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:27    time: 1.2649  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:47    time: 1.2754  data: 0.0009  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:11    time: 1.4136  data: 0.0009  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:16    time: 1.2951  data: 0.0009  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:35    time: 1.3179  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:41    time: 1.3957  data: 0.0009  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:08    time: 1.3694  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:40    time: 1.4282  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:19    time: 1.4649  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:43    time: 1.3910  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:27    time: 1.4007  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:50    time: 1.3591  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:22    time: 1.2504  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:02    time: 1.3471  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:34    time: 1.3180  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:17    time: 1.3326  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:03    time: 1.4448  data: 0.0009  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:40    time: 1.3766  data: 0.0009  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:37    time: 1.5022  data: 0.0009  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:12    time: 1.4626  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:50    time: 1.2433  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:32    time: 1.3125  data: 0.0009  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:18    time: 1.4019  data: 0.0009  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:11    time: 1.5596  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:46    time: 1.3997  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:29    time: 1.2451  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:22    time: 1.5224  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:01    time: 1.4391  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:42    time: 1.2276  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:26    time: 1.3241  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:09    time: 1.3478  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:57    time: 1.4146  data: 0.0009  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:42    time: 1.4657  data: 0.0009  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:23    time: 1.2913  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:07    time: 1.2577  data: 0.0009  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:56    time: 1.4591  data: 0.0009  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:42    time: 1.5128  data: 0.0009  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:29    time: 1.4801  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:14    time: 1.4548  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:00    time: 1.4277  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:45    time: 1.4006  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:25    time: 1.2111  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:11    time: 1.2186  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:56    time: 1.3674  data: 0.0009  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:40    time: 1.3235  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:26    time: 1.3570  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:13    time: 1.4784  data: 0.0009  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:01    time: 1.5685  data: 0.0009  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:48    time: 1.5626  data: 0.0009  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:34    time: 1.4822  data: 0.0009  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:21    time: 1.4886  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:07    time: 1.4972  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:53    time: 1.4349  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:36    time: 1.2664  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:21    time: 1.2552  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:07    time: 1.3866  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:51    time: 1.3000  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:38    time: 1.4066  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:24    time: 1.5248  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:10    time: 1.4103  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:56    time: 1.4040  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:41    time: 1.3666  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:28    time: 1.4248  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:13    time: 1.4211  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:59    time: 1.3713  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:45    time: 1.4405  data: 0.0009  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:31    time: 1.5009  data: 0.0009  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:17    time: 1.4505  data: 0.0009  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:01    time: 1.2813  data: 0.0009  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:46    time: 1.2232  data: 0.0009  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:30    time: 1.1510  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:16    time: 1.1598  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:02    time: 1.3361  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:48    time: 1.4163  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:34    time: 1.4044  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:20    time: 1.4546  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:05    time: 1.2881  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:51    time: 1.3536  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:37    time: 1.5394  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:24    time: 1.4573  data: 0.0009  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:10    time: 1.5184  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:55    time: 1.4158  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:41    time: 1.3447  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:27    time: 1.3505  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:13    time: 1.3483  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:59    time: 1.5009  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:45    time: 1.3692  data: 0.0011  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:31    time: 1.2883  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:17    time: 1.4231  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:03    time: 1.4319  data: 0.0009  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:48    time: 1.3695  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:34    time: 1.3424  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:20    time: 1.3496  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:06    time: 1.3802  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:52    time: 1.4396  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:38    time: 1.3728  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.3765  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:10    time: 1.4682  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4898  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3335  data: 0.0009  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1499  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2569  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3627  data: 0.0009  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3964  data: 0.0009  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4639  data: 0.0009  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.4270  data: 0.0009  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3792  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3545  data: 0.0462  max mem: 66110
Evaluation Total time: 0:25:31 (1.4008 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_23_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [24]  [   0/7110]  eta: 2 days, 5:47:15  lr: 0.000010  loss: 0.5612  time: 27.2342  data: 0.0001  max mem: 66110
Train: data epoch: [24]  [  50/7110]  eta: 2:59:18  lr: 0.000010  loss: 0.6149  time: 0.9837  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 100/7110]  eta: 2:28:09  lr: 0.000010  loss: 0.1848  time: 1.0379  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 150/7110]  eta: 2:17:56  lr: 0.000010  loss: 0.0909  time: 1.0675  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 200/7110]  eta: 2:10:45  lr: 0.000010  loss: 0.1714  time: 0.9702  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 250/7110]  eta: 2:06:39  lr: 0.000010  loss: 0.6601  time: 0.9637  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 300/7110]  eta: 2:03:45  lr: 0.000010  loss: 0.3314  time: 0.9642  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 350/7110]  eta: 2:01:13  lr: 0.000010  loss: 0.0229  time: 0.9633  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 400/7110]  eta: 1:58:59  lr: 0.000010  loss: 0.2318  time: 1.0026  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 450/7110]  eta: 1:57:27  lr: 0.000010  loss: 0.2992  time: 1.0569  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 500/7110]  eta: 1:56:06  lr: 0.000010  loss: 0.1087  time: 0.9672  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 550/7110]  eta: 1:54:54  lr: 0.000010  loss: 0.1519  time: 1.0119  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 600/7110]  eta: 1:53:32  lr: 0.000010  loss: 0.1228  time: 0.9310  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 650/7110]  eta: 1:52:14  lr: 0.000010  loss: 0.2471  time: 1.0190  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 700/7110]  eta: 1:50:42  lr: 0.000010  loss: 0.4002  time: 0.9577  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 750/7110]  eta: 1:49:31  lr: 0.000010  loss: 0.1869  time: 0.9775  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 800/7110]  eta: 1:48:15  lr: 0.000010  loss: 0.0961  time: 0.9636  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 850/7110]  eta: 1:47:03  lr: 0.000010  loss: 0.3800  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 900/7110]  eta: 1:46:15  lr: 0.000010  loss: 0.4523  time: 1.0228  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [ 950/7110]  eta: 1:45:11  lr: 0.000010  loss: 0.1694  time: 0.9612  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1000/7110]  eta: 1:44:23  lr: 0.000010  loss: 0.3901  time: 1.0825  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1050/7110]  eta: 1:43:25  lr: 0.000010  loss: 0.5246  time: 1.0250  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1100/7110]  eta: 1:42:20  lr: 0.000010  loss: 0.3198  time: 0.9697  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1150/7110]  eta: 1:41:19  lr: 0.000010  loss: 0.1836  time: 0.9386  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1200/7110]  eta: 1:40:33  lr: 0.000010  loss: 0.0940  time: 1.0296  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1250/7110]  eta: 1:39:44  lr: 0.000010  loss: 0.1797  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1300/7110]  eta: 1:38:44  lr: 0.000010  loss: 0.3118  time: 1.0422  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1350/7110]  eta: 1:37:42  lr: 0.000010  loss: 0.2178  time: 0.9905  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1400/7110]  eta: 1:36:52  lr: 0.000010  loss: 0.0775  time: 1.0200  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1450/7110]  eta: 1:35:53  lr: 0.000010  loss: 0.4096  time: 0.9757  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1500/7110]  eta: 1:34:58  lr: 0.000010  loss: 0.2356  time: 0.9741  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1550/7110]  eta: 1:34:03  lr: 0.000010  loss: 0.0850  time: 0.9874  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1600/7110]  eta: 1:33:05  lr: 0.000010  loss: 0.2150  time: 0.9719  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1650/7110]  eta: 1:32:13  lr: 0.000010  loss: 0.0710  time: 1.0062  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1700/7110]  eta: 1:31:17  lr: 0.000010  loss: 0.0635  time: 1.0015  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1750/7110]  eta: 1:30:26  lr: 0.000010  loss: 0.0285  time: 1.0437  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1800/7110]  eta: 1:29:34  lr: 0.000010  loss: 0.3602  time: 1.0380  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1850/7110]  eta: 1:28:39  lr: 0.000010  loss: 0.1810  time: 0.9677  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1900/7110]  eta: 1:27:48  lr: 0.000010  loss: 0.1650  time: 0.9452  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [1950/7110]  eta: 1:26:58  lr: 0.000010  loss: 0.2147  time: 0.9929  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2000/7110]  eta: 1:26:09  lr: 0.000010  loss: 0.2271  time: 0.9955  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2050/7110]  eta: 1:25:18  lr: 0.000010  loss: 0.3801  time: 1.0122  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2100/7110]  eta: 1:24:28  lr: 0.000010  loss: 0.5023  time: 1.0030  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2150/7110]  eta: 1:23:38  lr: 0.000010  loss: 0.4309  time: 0.9969  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2200/7110]  eta: 1:22:46  lr: 0.000010  loss: 0.2932  time: 1.0028  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2250/7110]  eta: 1:21:54  lr: 0.000010  loss: 0.0830  time: 1.0242  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2300/7110]  eta: 1:21:01  lr: 0.000010  loss: 0.0747  time: 0.9737  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2350/7110]  eta: 1:20:13  lr: 0.000010  loss: 0.2925  time: 1.0018  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2400/7110]  eta: 1:19:23  lr: 0.000010  loss: 0.1056  time: 1.0719  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2450/7110]  eta: 1:18:33  lr: 0.000010  loss: 0.3967  time: 0.9910  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2500/7110]  eta: 1:17:44  lr: 0.000010  loss: 0.3991  time: 1.0187  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2550/7110]  eta: 1:16:54  lr: 0.000010  loss: 0.1599  time: 1.0307  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2600/7110]  eta: 1:16:03  lr: 0.000010  loss: 0.1527  time: 1.0427  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2650/7110]  eta: 1:15:07  lr: 0.000010  loss: 0.3816  time: 0.9702  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2700/7110]  eta: 1:14:17  lr: 0.000010  loss: 0.0515  time: 0.9871  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2750/7110]  eta: 1:13:25  lr: 0.000010  loss: 0.1614  time: 1.0235  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2800/7110]  eta: 1:12:34  lr: 0.000010  loss: 0.0901  time: 1.0143  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2850/7110]  eta: 1:11:43  lr: 0.000010  loss: 0.0340  time: 1.0204  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2900/7110]  eta: 1:10:50  lr: 0.000010  loss: 0.1597  time: 0.9544  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [2950/7110]  eta: 1:10:00  lr: 0.000010  loss: 0.2083  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3000/7110]  eta: 1:09:11  lr: 0.000010  loss: 0.2063  time: 1.0369  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3050/7110]  eta: 1:08:19  lr: 0.000010  loss: 0.2674  time: 1.0168  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3100/7110]  eta: 1:07:26  lr: 0.000010  loss: 0.3932  time: 0.9686  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3150/7110]  eta: 1:06:34  lr: 0.000010  loss: 0.0795  time: 0.9690  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3200/7110]  eta: 1:05:42  lr: 0.000010  loss: 0.5017  time: 0.9424  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3250/7110]  eta: 1:04:53  lr: 0.000010  loss: 0.0219  time: 1.0513  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3300/7110]  eta: 1:04:02  lr: 0.000010  loss: 0.2097  time: 1.0108  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3350/7110]  eta: 1:03:10  lr: 0.000010  loss: 0.0813  time: 0.9656  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3400/7110]  eta: 1:02:22  lr: 0.000010  loss: 0.4035  time: 1.0302  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3450/7110]  eta: 1:01:29  lr: 0.000010  loss: 0.0317  time: 1.0027  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3500/7110]  eta: 1:00:37  lr: 0.000010  loss: 0.1581  time: 0.9495  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3550/7110]  eta: 0:59:44  lr: 0.000010  loss: 0.1889  time: 0.9781  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3600/7110]  eta: 0:58:53  lr: 0.000010  loss: 0.7871  time: 0.9777  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3650/7110]  eta: 0:58:04  lr: 0.000010  loss: 0.2391  time: 1.0430  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3700/7110]  eta: 0:57:14  lr: 0.000010  loss: 0.2662  time: 1.0178  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3750/7110]  eta: 0:56:23  lr: 0.000010  loss: 0.3702  time: 0.9627  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3800/7110]  eta: 0:55:30  lr: 0.000010  loss: 0.4943  time: 0.9379  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3850/7110]  eta: 0:54:41  lr: 0.000010  loss: 1.3640  time: 1.0337  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3900/7110]  eta: 0:53:49  lr: 0.000010  loss: 0.2035  time: 0.9425  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [3950/7110]  eta: 0:52:58  lr: 0.000010  loss: 1.1998  time: 0.9622  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4000/7110]  eta: 0:52:08  lr: 0.000010  loss: 0.3920  time: 1.0391  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4050/7110]  eta: 0:51:18  lr: 0.000010  loss: 0.3704  time: 1.0142  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4100/7110]  eta: 0:50:27  lr: 0.000010  loss: 0.3163  time: 0.9624  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4150/7110]  eta: 0:49:37  lr: 0.000010  loss: 0.1128  time: 0.9954  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4200/7110]  eta: 0:48:47  lr: 0.000010  loss: 0.1427  time: 1.0297  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4250/7110]  eta: 0:47:56  lr: 0.000010  loss: 0.6224  time: 0.9568  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4300/7110]  eta: 0:47:06  lr: 0.000010  loss: 0.3865  time: 1.0142  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4350/7110]  eta: 0:46:16  lr: 0.000010  loss: 0.7345  time: 1.0119  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4400/7110]  eta: 0:45:25  lr: 0.000010  loss: 0.1210  time: 0.9766  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4450/7110]  eta: 0:44:34  lr: 0.000010  loss: 0.0709  time: 0.9334  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4500/7110]  eta: 0:43:44  lr: 0.000010  loss: 0.1382  time: 0.9895  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4550/7110]  eta: 0:42:53  lr: 0.000010  loss: 0.0676  time: 1.0051  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4600/7110]  eta: 0:42:03  lr: 0.000010  loss: 0.1138  time: 1.0124  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4650/7110]  eta: 0:41:11  lr: 0.000010  loss: 0.1540  time: 0.9537  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4700/7110]  eta: 0:40:21  lr: 0.000010  loss: 0.5051  time: 0.9536  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4750/7110]  eta: 0:39:31  lr: 0.000010  loss: 0.0552  time: 1.0195  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4800/7110]  eta: 0:38:41  lr: 0.000010  loss: 0.0503  time: 1.0257  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4850/7110]  eta: 0:37:50  lr: 0.000010  loss: 1.8643  time: 0.9925  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4900/7110]  eta: 0:37:00  lr: 0.000010  loss: 0.1488  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [4950/7110]  eta: 0:36:10  lr: 0.000010  loss: 0.0782  time: 1.0133  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5000/7110]  eta: 0:35:20  lr: 0.000010  loss: 0.0411  time: 1.0048  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5050/7110]  eta: 0:34:29  lr: 0.000010  loss: 0.5357  time: 0.9531  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5100/7110]  eta: 0:33:38  lr: 0.000010  loss: 0.3615  time: 0.9745  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5150/7110]  eta: 0:32:47  lr: 0.000010  loss: 0.0369  time: 0.9539  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5200/7110]  eta: 0:31:57  lr: 0.000010  loss: 0.4642  time: 1.0215  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5250/7110]  eta: 0:31:06  lr: 0.000010  loss: 0.3529  time: 0.9797  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5300/7110]  eta: 0:30:16  lr: 0.000010  loss: 0.1244  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5350/7110]  eta: 0:29:25  lr: 0.000010  loss: 0.0207  time: 1.0174  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5400/7110]  eta: 0:28:34  lr: 0.000010  loss: 0.0396  time: 1.0194  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5450/7110]  eta: 0:27:44  lr: 0.000010  loss: 0.0876  time: 0.9821  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5500/7110]  eta: 0:26:54  lr: 0.000010  loss: 0.0703  time: 0.9773  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5550/7110]  eta: 0:26:04  lr: 0.000010  loss: 0.0180  time: 0.9971  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5600/7110]  eta: 0:25:13  lr: 0.000010  loss: 0.8302  time: 0.9649  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5650/7110]  eta: 0:24:23  lr: 0.000010  loss: 0.1707  time: 1.0544  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.2538  time: 1.0488  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.1527  time: 0.9910  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.7388  time: 0.9923  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.0894  time: 0.9646  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.2421  time: 1.0244  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 1.1333  time: 0.9709  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6000/7110]  eta: 0:18:32  lr: 0.000010  loss: 0.3042  time: 0.9995  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.0803  time: 0.9939  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.3883  time: 1.0241  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.3632  time: 1.0223  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.1909  time: 0.9846  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.2660  time: 1.0121  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.2575  time: 0.9928  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.3358  time: 0.9965  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.0962  time: 0.9783  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.2638  time: 0.9463  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.1901  time: 0.9741  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.4020  time: 1.0206  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.3178  time: 0.9964  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.2523  time: 1.0022  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.1905  time: 0.9849  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.1310  time: 1.0057  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.3089  time: 1.0406  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1881  time: 0.9473  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1862  time: 0.9361  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.2607  time: 1.0262  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.5764  time: 1.0487  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.0673  time: 0.9599  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.2713  time: 1.0132  data: 0.0000  max mem: 66110
Train: data epoch: [24]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.5003  time: 1.1218  data: 0.0000  max mem: 66110
Train: data epoch: [24] Total time: 1:58:45 (1.0021 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:14:38    time: 20.5659  data: 19.3212  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:52    time: 3.1514  data: 1.7573  max mem: 66110
Evaluation  [  20/1093]  eta: 0:40:45    time: 1.3644  data: 0.0009  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:02    time: 1.2447  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:19    time: 1.2656  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:29:46    time: 1.3909  data: 0.0009  max mem: 66110
Evaluation  [  60/1093]  eta: 0:27:44    time: 1.2554  data: 0.0009  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:03    time: 1.2648  data: 0.0009  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:14    time: 1.3832  data: 0.0009  max mem: 66110
Evaluation  [  90/1093]  eta: 0:25:41    time: 1.3579  data: 0.0009  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:13    time: 1.4023  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:24:53    time: 1.4388  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:18    time: 1.3710  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:08    time: 1.4194  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:32    time: 1.3813  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:06    time: 1.2476  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:48    time: 1.3542  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:21    time: 1.3301  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:04    time: 1.3255  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:21:51    time: 1.4422  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:29    time: 1.3866  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:21    time: 1.4444  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:20:56    time: 1.3919  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:35    time: 1.2249  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:18    time: 1.2980  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:03    time: 1.3800  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:19:56    time: 1.5433  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:32    time: 1.3856  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:12    time: 1.1744  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:05    time: 1.4569  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:44    time: 1.4192  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:28    time: 1.2471  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:14    time: 1.3827  data: 0.0009  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:17:57    time: 1.3586  data: 0.0009  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:45    time: 1.3984  data: 0.0009  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:31    time: 1.4699  data: 0.0009  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:12    time: 1.2943  data: 0.0009  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:16:57    time: 1.2615  data: 0.0009  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:46    time: 1.4634  data: 0.0009  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:29    time: 1.4314  data: 0.0009  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:16    time: 1.3821  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:15:59    time: 1.3531  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:47    time: 1.3610  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:32    time: 1.4170  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:14    time: 1.2393  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:14:59    time: 1.2545  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:44    time: 1.3556  data: 0.0009  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:29    time: 1.3215  data: 0.0009  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:16    time: 1.3781  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:03    time: 1.4766  data: 0.0009  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:52    time: 1.5575  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:39    time: 1.5482  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:25    time: 1.4505  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:11    time: 1.4391  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:12:58    time: 1.4678  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:44    time: 1.4261  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:27    time: 1.2454  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:13    time: 1.2534  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:11:58    time: 1.3806  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:42    time: 1.2379  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:29    time: 1.3415  data: 0.0009  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:15    time: 1.5002  data: 0.0009  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:01    time: 1.3973  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:47    time: 1.3798  data: 0.0009  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:32    time: 1.3388  data: 0.0009  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:19    time: 1.3845  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:04    time: 1.3786  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:50    time: 1.3451  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:37    time: 1.4149  data: 0.0009  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:23    time: 1.4789  data: 0.0009  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:09    time: 1.4200  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:54    time: 1.2316  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:39    time: 1.1992  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:24    time: 1.1721  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:09    time: 1.1796  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:55    time: 1.3325  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:41    time: 1.3983  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:28    time: 1.3906  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:14    time: 1.4500  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:06:59    time: 1.3110  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:46    time: 1.3795  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:33    time: 1.5408  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:19    time: 1.4613  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:06    time: 1.5219  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:51    time: 1.4093  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:38    time: 1.3457  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:23    time: 1.3661  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:10    time: 1.3542  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:56    time: 1.5017  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:42    time: 1.3562  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:28    time: 1.2749  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:14    time: 1.4249  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:00    time: 1.4246  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:46    time: 1.3640  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:32    time: 1.3308  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:18    time: 1.3323  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:04    time: 1.4020  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:50    time: 1.4626  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:36    time: 1.3694  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3699  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4616  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4492  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2978  data: 0.0009  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1550  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2587  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3591  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3816  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4444  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3833  data: 0.0009  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3395  data: 0.0009  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3192  data: 0.0477  max mem: 66110
Evaluation Total time: 0:25:15 (1.3862 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_24_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [25]  [   0/7110]  eta: 2 days, 6:34:32  lr: 0.000010  loss: 0.0924  time: 27.6333  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [  50/7110]  eta: 3:00:54  lr: 0.000010  loss: 0.1288  time: 1.0158  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 100/7110]  eta: 2:27:38  lr: 0.000010  loss: 0.2654  time: 1.0142  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 150/7110]  eta: 2:16:26  lr: 0.000010  loss: 0.5613  time: 1.0113  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 200/7110]  eta: 2:10:37  lr: 0.000010  loss: 0.1117  time: 0.9767  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 250/7110]  eta: 2:06:37  lr: 0.000010  loss: 0.2522  time: 0.9482  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 300/7110]  eta: 2:03:20  lr: 0.000010  loss: 0.6256  time: 0.9686  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 350/7110]  eta: 2:00:10  lr: 0.000010  loss: 0.3284  time: 0.9282  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 400/7110]  eta: 1:58:45  lr: 0.000010  loss: 0.1136  time: 1.0232  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 450/7110]  eta: 1:56:51  lr: 0.000010  loss: 0.3567  time: 1.0080  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 500/7110]  eta: 1:55:23  lr: 0.000010  loss: 1.1130  time: 0.9902  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 550/7110]  eta: 1:54:19  lr: 0.000010  loss: 0.5363  time: 1.0364  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 600/7110]  eta: 1:53:02  lr: 0.000010  loss: 0.4274  time: 0.9538  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 650/7110]  eta: 1:52:08  lr: 0.000010  loss: 0.4025  time: 1.0603  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 700/7110]  eta: 1:51:00  lr: 0.000010  loss: 0.2109  time: 0.9879  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 750/7110]  eta: 1:49:48  lr: 0.000010  loss: 0.8824  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 800/7110]  eta: 1:48:48  lr: 0.000010  loss: 0.5295  time: 1.0149  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 850/7110]  eta: 1:47:46  lr: 0.000010  loss: 0.5164  time: 0.9734  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 900/7110]  eta: 1:46:32  lr: 0.000010  loss: 0.4392  time: 0.9514  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [ 950/7110]  eta: 1:45:38  lr: 0.000010  loss: 0.1816  time: 1.0367  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1000/7110]  eta: 1:44:39  lr: 0.000010  loss: 0.1077  time: 1.0104  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1050/7110]  eta: 1:43:32  lr: 0.000010  loss: 0.2100  time: 0.9512  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1100/7110]  eta: 1:42:25  lr: 0.000010  loss: 0.1066  time: 1.0104  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1150/7110]  eta: 1:41:25  lr: 0.000010  loss: 0.2777  time: 0.9957  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1200/7110]  eta: 1:40:15  lr: 0.000010  loss: 0.1268  time: 0.9425  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1250/7110]  eta: 1:39:15  lr: 0.000010  loss: 0.1385  time: 1.0153  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1300/7110]  eta: 1:38:26  lr: 0.000010  loss: 0.3965  time: 1.0078  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1350/7110]  eta: 1:37:31  lr: 0.000010  loss: 0.3189  time: 0.9766  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1400/7110]  eta: 1:36:38  lr: 0.000010  loss: 0.2162  time: 0.9847  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1450/7110]  eta: 1:35:43  lr: 0.000010  loss: 0.3661  time: 0.9359  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1500/7110]  eta: 1:34:46  lr: 0.000010  loss: 0.1932  time: 0.9287  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1550/7110]  eta: 1:33:49  lr: 0.000010  loss: 0.2925  time: 1.0177  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1600/7110]  eta: 1:32:59  lr: 0.000010  loss: 0.4318  time: 1.0240  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1650/7110]  eta: 1:32:00  lr: 0.000010  loss: 0.2418  time: 1.0028  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1700/7110]  eta: 1:31:07  lr: 0.000010  loss: 0.4928  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1750/7110]  eta: 1:30:22  lr: 0.000010  loss: 0.1647  time: 1.0670  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1800/7110]  eta: 1:29:26  lr: 0.000010  loss: 0.1985  time: 0.9562  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1850/7110]  eta: 1:28:31  lr: 0.000010  loss: 0.0885  time: 0.9699  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1900/7110]  eta: 1:27:41  lr: 0.000010  loss: 0.1653  time: 1.0540  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [1950/7110]  eta: 1:26:46  lr: 0.000010  loss: 0.5181  time: 0.9876  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2000/7110]  eta: 1:25:55  lr: 0.000010  loss: 0.0702  time: 0.9996  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2050/7110]  eta: 1:25:05  lr: 0.000010  loss: 1.8541  time: 1.0508  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2100/7110]  eta: 1:24:20  lr: 0.000010  loss: 0.0464  time: 1.1108  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2150/7110]  eta: 1:23:26  lr: 0.000010  loss: 0.1956  time: 1.0162  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2200/7110]  eta: 1:22:39  lr: 0.000010  loss: 0.0275  time: 1.0133  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2250/7110]  eta: 1:21:46  lr: 0.000010  loss: 0.2172  time: 1.0008  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2300/7110]  eta: 1:20:54  lr: 0.000010  loss: 0.0564  time: 1.0578  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2350/7110]  eta: 1:19:59  lr: 0.000010  loss: 0.1641  time: 0.9490  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2400/7110]  eta: 1:19:06  lr: 0.000010  loss: 0.2253  time: 0.9609  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2450/7110]  eta: 1:18:18  lr: 0.000010  loss: 0.3429  time: 1.0167  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2500/7110]  eta: 1:17:29  lr: 0.000010  loss: 0.2967  time: 1.0273  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2550/7110]  eta: 1:16:35  lr: 0.000010  loss: 0.0284  time: 0.9790  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2600/7110]  eta: 1:15:42  lr: 0.000010  loss: 0.4421  time: 1.0179  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2650/7110]  eta: 1:14:48  lr: 0.000010  loss: 0.4314  time: 1.0100  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2700/7110]  eta: 1:13:56  lr: 0.000010  loss: 0.3857  time: 0.9797  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2750/7110]  eta: 1:13:05  lr: 0.000010  loss: 0.7657  time: 0.9462  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2800/7110]  eta: 1:12:14  lr: 0.000010  loss: 0.2529  time: 1.0158  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2850/7110]  eta: 1:11:23  lr: 0.000010  loss: 0.1957  time: 0.9500  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2900/7110]  eta: 1:10:30  lr: 0.000010  loss: 0.5468  time: 0.9457  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [2950/7110]  eta: 1:09:38  lr: 0.000010  loss: 0.2540  time: 0.9935  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3000/7110]  eta: 1:08:46  lr: 0.000010  loss: 0.2407  time: 0.9886  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3050/7110]  eta: 1:07:55  lr: 0.000010  loss: 0.6611  time: 0.9799  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3100/7110]  eta: 1:07:02  lr: 0.000010  loss: 0.3453  time: 0.9591  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3150/7110]  eta: 1:06:10  lr: 0.000010  loss: 0.0723  time: 0.9229  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3200/7110]  eta: 1:05:19  lr: 0.000010  loss: 0.4591  time: 0.9932  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3250/7110]  eta: 1:04:27  lr: 0.000010  loss: 0.0584  time: 0.9827  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3300/7110]  eta: 1:03:36  lr: 0.000010  loss: 0.4681  time: 0.9221  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3350/7110]  eta: 1:02:47  lr: 0.000010  loss: 0.0626  time: 1.0019  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3400/7110]  eta: 1:01:57  lr: 0.000010  loss: 0.1578  time: 0.9953  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3450/7110]  eta: 1:01:05  lr: 0.000010  loss: 1.4586  time: 1.0036  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3500/7110]  eta: 1:00:15  lr: 0.000010  loss: 0.1740  time: 0.9929  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3550/7110]  eta: 0:59:26  lr: 0.000010  loss: 0.3677  time: 0.9992  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3600/7110]  eta: 0:58:34  lr: 0.000010  loss: 0.1986  time: 0.9675  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3650/7110]  eta: 0:57:43  lr: 0.000010  loss: 0.3916  time: 0.9657  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3700/7110]  eta: 0:56:50  lr: 0.000010  loss: 0.9639  time: 0.9580  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3750/7110]  eta: 0:56:00  lr: 0.000010  loss: 0.0550  time: 1.0035  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3800/7110]  eta: 0:55:10  lr: 0.000010  loss: 0.0641  time: 1.0499  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3850/7110]  eta: 0:54:21  lr: 0.000010  loss: 0.2985  time: 1.0242  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3900/7110]  eta: 0:53:30  lr: 0.000010  loss: 0.1929  time: 1.0006  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [3950/7110]  eta: 0:52:40  lr: 0.000010  loss: 0.0966  time: 0.9580  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4000/7110]  eta: 0:51:50  lr: 0.000010  loss: 0.5489  time: 1.0174  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4050/7110]  eta: 0:51:00  lr: 0.000010  loss: 0.1055  time: 0.9837  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4100/7110]  eta: 0:50:09  lr: 0.000010  loss: 0.2398  time: 0.9792  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4150/7110]  eta: 0:49:20  lr: 0.000010  loss: 0.3638  time: 1.0099  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4200/7110]  eta: 0:48:31  lr: 0.000010  loss: 0.1480  time: 1.0340  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4250/7110]  eta: 0:47:40  lr: 0.000010  loss: 0.1126  time: 0.9686  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4300/7110]  eta: 0:46:50  lr: 0.000010  loss: 0.3470  time: 0.9990  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4350/7110]  eta: 0:46:01  lr: 0.000010  loss: 0.1799  time: 1.0802  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4400/7110]  eta: 0:45:11  lr: 0.000010  loss: 0.1457  time: 0.9905  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4450/7110]  eta: 0:44:22  lr: 0.000010  loss: 0.2780  time: 0.9623  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4500/7110]  eta: 0:43:31  lr: 0.000010  loss: 0.2200  time: 1.0121  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4550/7110]  eta: 0:42:40  lr: 0.000010  loss: 0.4712  time: 0.9452  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4600/7110]  eta: 0:41:51  lr: 0.000010  loss: 0.2232  time: 0.9908  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4650/7110]  eta: 0:41:00  lr: 0.000010  loss: 0.1133  time: 1.0484  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4700/7110]  eta: 0:40:11  lr: 0.000010  loss: 0.3947  time: 1.0633  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4750/7110]  eta: 0:39:21  lr: 0.000010  loss: 0.4750  time: 1.0381  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4800/7110]  eta: 0:38:31  lr: 0.000010  loss: 0.7967  time: 1.0156  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4850/7110]  eta: 0:37:41  lr: 0.000010  loss: 0.1682  time: 1.0362  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4900/7110]  eta: 0:36:51  lr: 0.000010  loss: 0.2192  time: 1.0031  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [4950/7110]  eta: 0:36:01  lr: 0.000010  loss: 0.1004  time: 1.0343  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5000/7110]  eta: 0:35:11  lr: 0.000010  loss: 0.1804  time: 1.0186  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5050/7110]  eta: 0:34:22  lr: 0.000010  loss: 0.2550  time: 0.9852  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5100/7110]  eta: 0:33:31  lr: 0.000010  loss: 0.1817  time: 0.9631  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5150/7110]  eta: 0:32:42  lr: 0.000010  loss: 0.4500  time: 1.0223  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5200/7110]  eta: 0:31:51  lr: 0.000010  loss: 0.2716  time: 0.9869  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5250/7110]  eta: 0:31:01  lr: 0.000010  loss: 0.9666  time: 0.9873  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5300/7110]  eta: 0:30:11  lr: 0.000010  loss: 0.3619  time: 0.9998  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5350/7110]  eta: 0:29:21  lr: 0.000010  loss: 0.8396  time: 1.0467  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5400/7110]  eta: 0:28:31  lr: 0.000010  loss: 0.2076  time: 0.9852  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5450/7110]  eta: 0:27:41  lr: 0.000010  loss: 0.0968  time: 1.0244  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5500/7110]  eta: 0:26:51  lr: 0.000010  loss: 0.4106  time: 0.9511  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5550/7110]  eta: 0:26:01  lr: 0.000010  loss: 0.2269  time: 1.0438  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5600/7110]  eta: 0:25:11  lr: 0.000010  loss: 0.6556  time: 0.9712  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5650/7110]  eta: 0:24:21  lr: 0.000010  loss: 0.1666  time: 0.9167  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5700/7110]  eta: 0:23:31  lr: 0.000010  loss: 0.0630  time: 0.9448  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5750/7110]  eta: 0:22:40  lr: 0.000010  loss: 0.1004  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5800/7110]  eta: 0:21:51  lr: 0.000010  loss: 0.1901  time: 1.0915  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5850/7110]  eta: 0:21:01  lr: 0.000010  loss: 0.1652  time: 0.9984  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5900/7110]  eta: 0:20:11  lr: 0.000010  loss: 0.9097  time: 0.9310  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [5950/7110]  eta: 0:19:21  lr: 0.000010  loss: 0.4324  time: 1.0343  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6000/7110]  eta: 0:18:31  lr: 0.000010  loss: 0.4425  time: 0.9487  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6050/7110]  eta: 0:17:41  lr: 0.000010  loss: 0.7044  time: 1.0200  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6100/7110]  eta: 0:16:51  lr: 0.000010  loss: 0.1392  time: 1.0030  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6150/7110]  eta: 0:16:01  lr: 0.000010  loss: 0.4313  time: 0.9918  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6200/7110]  eta: 0:15:11  lr: 0.000010  loss: 0.1525  time: 1.0513  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6250/7110]  eta: 0:14:21  lr: 0.000010  loss: 0.0949  time: 0.9939  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6300/7110]  eta: 0:13:31  lr: 0.000010  loss: 0.0733  time: 0.9721  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.4866  time: 0.9581  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.2973  time: 0.9577  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6450/7110]  eta: 0:11:00  lr: 0.000010  loss: 0.4010  time: 1.0382  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.2727  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6550/7110]  eta: 0:09:20  lr: 0.000010  loss: 0.1885  time: 0.9882  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.3902  time: 1.0114  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.8888  time: 1.0088  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 1.7746  time: 1.0552  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.9009  time: 0.9682  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.1721  time: 0.9994  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.0766  time: 0.9820  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.0587  time: 0.9937  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.0694  time: 1.0678  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.1907  time: 1.0761  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.2011  time: 0.9349  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1508  time: 1.0645  data: 0.0000  max mem: 66110
Train: data epoch: [25]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.3908  time: 1.1666  data: 0.0000  max mem: 66110
Train: data epoch: [25] Total time: 1:58:46 (1.0024 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:16:07    time: 20.6473  data: 19.4168  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:36    time: 3.1919  data: 1.7660  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:08    time: 1.3828  data: 0.0009  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:09    time: 1.2326  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:34    time: 1.2721  data: 0.0009  max mem: 66110
Evaluation  [  50/1093]  eta: 0:29:59    time: 1.4106  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:27:55    time: 1.2586  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:12    time: 1.2658  data: 0.0011  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:22    time: 1.3827  data: 0.0011  max mem: 66110
Evaluation  [  90/1093]  eta: 0:25:47    time: 1.3576  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:22    time: 1.4122  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:01    time: 1.4559  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:26    time: 1.3803  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:13    time: 1.4099  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:38    time: 1.3699  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:11    time: 1.2495  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:55    time: 1.3776  data: 0.0011  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:28    time: 1.3568  data: 0.0012  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:10    time: 1.3213  data: 0.0011  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:21:55    time: 1.4218  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:33    time: 1.3729  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:25    time: 1.4427  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:00    time: 1.3973  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:39    time: 1.2312  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:21    time: 1.2952  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:06    time: 1.3786  data: 0.0009  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:00    time: 1.5481  data: 0.0009  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:36    time: 1.3937  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:18    time: 1.2164  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:10    time: 1.4890  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:50    time: 1.4384  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:31    time: 1.2263  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:17    time: 1.3383  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:00    time: 1.3675  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:49    time: 1.4143  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:35    time: 1.4812  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:15    time: 1.2909  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:00    time: 1.2593  data: 0.0009  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:50    time: 1.4844  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:33    time: 1.4484  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:20    time: 1.3928  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:03    time: 1.3620  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:50    time: 1.3420  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:34    time: 1.3947  data: 0.0009  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:15    time: 1.2029  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:01    time: 1.2142  data: 0.0011  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:46    time: 1.3549  data: 0.0012  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:31    time: 1.3341  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:18    time: 1.3876  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:05    time: 1.4761  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:53    time: 1.5532  data: 0.0009  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:40    time: 1.5641  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:27    time: 1.4910  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:13    time: 1.4596  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:00    time: 1.4641  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:46    time: 1.4329  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:29    time: 1.2521  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:14    time: 1.2298  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:00    time: 1.3742  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:43    time: 1.2541  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:31    time: 1.3440  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:17    time: 1.5115  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:03    time: 1.4060  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:49    time: 1.3898  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:34    time: 1.3540  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:21    time: 1.3902  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:06    time: 1.3851  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:52    time: 1.3522  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:38    time: 1.4195  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:25    time: 1.4865  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:11    time: 1.4435  data: 0.0009  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:55    time: 1.2245  data: 0.0009  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:40    time: 1.1527  data: 0.0009  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:25    time: 1.1502  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:10    time: 1.1678  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:56    time: 1.2978  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:42    time: 1.3725  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:28    time: 1.3850  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:15    time: 1.4466  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:06:59    time: 1.2664  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:46    time: 1.3150  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:33    time: 1.5199  data: 0.0009  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:19    time: 1.4529  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:06    time: 1.5173  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:51    time: 1.4301  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:38    time: 1.3608  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:23    time: 1.3509  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:09    time: 1.3087  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:56    time: 1.4633  data: 0.0009  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:41    time: 1.3604  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:28    time: 1.2757  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:14    time: 1.4197  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:00    time: 1.4150  data: 0.0009  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:46    time: 1.3528  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:32    time: 1.3279  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:18    time: 1.3252  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:04    time: 1.3622  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:50    time: 1.4306  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:36    time: 1.3719  data: 0.0009  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:22    time: 1.3635  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4531  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4467  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2977  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1519  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2532  data: 0.0009  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3475  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3871  data: 0.0009  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4478  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:17    time: 1.3620  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3158  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2873  data: 0.0421  max mem: 66110
Evaluation Total time: 0:25:12 (1.3838 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_25_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [26]  [   0/7110]  eta: 2 days, 6:15:29  lr: 0.000010  loss: 0.3065  time: 27.4725  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [  50/7110]  eta: 2:58:16  lr: 0.000010  loss: 0.1078  time: 0.9644  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 100/7110]  eta: 2:27:51  lr: 0.000010  loss: 0.0599  time: 1.0235  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 150/7110]  eta: 2:17:09  lr: 0.000010  loss: 0.3055  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 200/7110]  eta: 2:10:29  lr: 0.000010  loss: 0.3029  time: 0.9603  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 250/7110]  eta: 2:06:45  lr: 0.000010  loss: 0.0967  time: 0.9792  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 300/7110]  eta: 2:03:44  lr: 0.000010  loss: 0.1156  time: 0.9558  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 350/7110]  eta: 2:01:48  lr: 0.000010  loss: 0.5597  time: 1.0538  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 400/7110]  eta: 2:00:05  lr: 0.000010  loss: 0.0552  time: 1.0509  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 450/7110]  eta: 1:58:26  lr: 0.000010  loss: 0.3081  time: 1.0375  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 500/7110]  eta: 1:56:45  lr: 0.000010  loss: 0.1541  time: 1.0162  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 550/7110]  eta: 1:55:20  lr: 0.000010  loss: 0.4482  time: 0.9953  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 600/7110]  eta: 1:53:47  lr: 0.000010  loss: 1.4648  time: 0.9559  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 650/7110]  eta: 1:52:19  lr: 0.000010  loss: 0.3404  time: 0.9796  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 700/7110]  eta: 1:51:10  lr: 0.000010  loss: 0.3919  time: 0.9789  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 750/7110]  eta: 1:49:55  lr: 0.000010  loss: 0.1716  time: 1.0042  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 800/7110]  eta: 1:48:59  lr: 0.000010  loss: 0.0751  time: 1.0096  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 850/7110]  eta: 1:48:02  lr: 0.000010  loss: 0.1226  time: 1.0385  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 900/7110]  eta: 1:47:05  lr: 0.000010  loss: 0.0890  time: 0.9786  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [ 950/7110]  eta: 1:45:53  lr: 0.000010  loss: 0.4748  time: 1.0167  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1000/7110]  eta: 1:44:51  lr: 0.000010  loss: 0.1015  time: 0.9701  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1050/7110]  eta: 1:43:54  lr: 0.000010  loss: 0.0574  time: 0.9948  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1100/7110]  eta: 1:43:04  lr: 0.000010  loss: 0.1627  time: 1.0610  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1150/7110]  eta: 1:42:01  lr: 0.000010  loss: 0.3807  time: 1.0447  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1200/7110]  eta: 1:41:01  lr: 0.000010  loss: 0.2628  time: 0.9728  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1250/7110]  eta: 1:40:11  lr: 0.000010  loss: 0.6911  time: 1.0267  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1300/7110]  eta: 1:39:05  lr: 0.000010  loss: 0.1227  time: 0.9808  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1350/7110]  eta: 1:38:08  lr: 0.000010  loss: 0.1776  time: 0.9977  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1400/7110]  eta: 1:37:11  lr: 0.000010  loss: 0.3563  time: 0.9754  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1450/7110]  eta: 1:36:13  lr: 0.000010  loss: 0.2858  time: 1.0114  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1500/7110]  eta: 1:35:17  lr: 0.000010  loss: 0.5205  time: 1.0636  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1550/7110]  eta: 1:34:18  lr: 0.000010  loss: 0.3773  time: 0.9680  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1600/7110]  eta: 1:33:31  lr: 0.000010  loss: 0.4739  time: 1.0246  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1650/7110]  eta: 1:32:40  lr: 0.000010  loss: 0.1437  time: 1.0355  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1700/7110]  eta: 1:31:46  lr: 0.000010  loss: 0.1667  time: 0.9860  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1750/7110]  eta: 1:30:52  lr: 0.000010  loss: 0.1320  time: 0.9802  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1800/7110]  eta: 1:29:56  lr: 0.000010  loss: 0.2367  time: 0.9696  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1850/7110]  eta: 1:29:03  lr: 0.000010  loss: 0.3847  time: 0.9694  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1900/7110]  eta: 1:28:08  lr: 0.000010  loss: 1.1799  time: 0.9604  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [1950/7110]  eta: 1:27:20  lr: 0.000010  loss: 0.2190  time: 0.9715  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2000/7110]  eta: 1:26:25  lr: 0.000010  loss: 0.2375  time: 0.9679  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2050/7110]  eta: 1:25:38  lr: 0.000010  loss: 0.4583  time: 1.0046  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2100/7110]  eta: 1:24:49  lr: 0.000010  loss: 0.0041  time: 1.0424  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2150/7110]  eta: 1:23:55  lr: 0.000010  loss: 0.5061  time: 1.0123  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2200/7110]  eta: 1:23:01  lr: 0.000010  loss: 0.0606  time: 0.9332  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2250/7110]  eta: 1:22:09  lr: 0.000010  loss: 0.2329  time: 0.9891  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2300/7110]  eta: 1:21:15  lr: 0.000010  loss: 0.6239  time: 0.9399  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2350/7110]  eta: 1:20:28  lr: 0.000010  loss: 0.2118  time: 1.0340  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2400/7110]  eta: 1:19:35  lr: 0.000010  loss: 0.1552  time: 1.0298  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2450/7110]  eta: 1:18:46  lr: 0.000010  loss: 0.1310  time: 1.0629  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2500/7110]  eta: 1:17:54  lr: 0.000010  loss: 0.0384  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2550/7110]  eta: 1:16:58  lr: 0.000010  loss: 0.6533  time: 0.9487  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2600/7110]  eta: 1:16:07  lr: 0.000010  loss: 1.0115  time: 1.0100  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2650/7110]  eta: 1:15:14  lr: 0.000010  loss: 0.1530  time: 0.9708  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2700/7110]  eta: 1:14:23  lr: 0.000010  loss: 0.0722  time: 1.0283  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2750/7110]  eta: 1:13:30  lr: 0.000010  loss: 0.0791  time: 0.9800  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2800/7110]  eta: 1:12:37  lr: 0.000010  loss: 0.0521  time: 1.0205  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2850/7110]  eta: 1:11:45  lr: 0.000010  loss: 0.0346  time: 0.9944  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2900/7110]  eta: 1:10:53  lr: 0.000010  loss: 0.3765  time: 0.9792  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [2950/7110]  eta: 1:10:01  lr: 0.000010  loss: 0.2426  time: 0.9613  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3000/7110]  eta: 1:09:10  lr: 0.000010  loss: 0.1203  time: 0.9630  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3050/7110]  eta: 1:08:17  lr: 0.000010  loss: 0.0278  time: 0.9728  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3100/7110]  eta: 1:07:25  lr: 0.000010  loss: 0.8102  time: 1.0104  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3150/7110]  eta: 1:06:36  lr: 0.000010  loss: 0.5635  time: 0.9822  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3200/7110]  eta: 1:05:43  lr: 0.000010  loss: 0.1203  time: 1.0250  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3250/7110]  eta: 1:04:50  lr: 0.000010  loss: 0.1147  time: 0.9306  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3300/7110]  eta: 1:04:00  lr: 0.000010  loss: 0.3426  time: 1.0612  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3350/7110]  eta: 1:03:07  lr: 0.000010  loss: 0.1985  time: 0.9836  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3400/7110]  eta: 1:02:17  lr: 0.000010  loss: 0.2579  time: 1.0098  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3450/7110]  eta: 1:01:26  lr: 0.000010  loss: 0.1079  time: 0.9820  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3500/7110]  eta: 1:00:37  lr: 0.000010  loss: 0.3000  time: 1.0146  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3550/7110]  eta: 0:59:45  lr: 0.000010  loss: 0.1159  time: 0.9356  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3600/7110]  eta: 0:58:55  lr: 0.000010  loss: 0.6810  time: 0.9889  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3650/7110]  eta: 0:58:06  lr: 0.000010  loss: 0.1247  time: 1.0574  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3700/7110]  eta: 0:57:15  lr: 0.000010  loss: 0.0941  time: 0.9854  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3750/7110]  eta: 0:56:26  lr: 0.000010  loss: 0.0862  time: 0.9882  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3800/7110]  eta: 0:55:34  lr: 0.000010  loss: 0.1189  time: 0.9379  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3850/7110]  eta: 0:54:42  lr: 0.000010  loss: 0.1382  time: 0.9724  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3900/7110]  eta: 0:53:52  lr: 0.000010  loss: 0.1512  time: 1.0206  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [3950/7110]  eta: 0:53:02  lr: 0.000010  loss: 0.2163  time: 1.0523  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4000/7110]  eta: 0:52:10  lr: 0.000010  loss: 0.4718  time: 0.9834  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4050/7110]  eta: 0:51:19  lr: 0.000010  loss: 0.0928  time: 0.9800  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4100/7110]  eta: 0:50:28  lr: 0.000010  loss: 0.1401  time: 0.9930  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4150/7110]  eta: 0:49:38  lr: 0.000010  loss: 0.7640  time: 1.0607  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4200/7110]  eta: 0:48:48  lr: 0.000010  loss: 0.5008  time: 0.9795  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4250/7110]  eta: 0:47:57  lr: 0.000010  loss: 0.3026  time: 0.9555  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4300/7110]  eta: 0:47:06  lr: 0.000010  loss: 0.0654  time: 0.9538  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4350/7110]  eta: 0:46:14  lr: 0.000010  loss: 0.3183  time: 0.9536  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4400/7110]  eta: 0:45:23  lr: 0.000010  loss: 0.0236  time: 0.9544  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4450/7110]  eta: 0:44:33  lr: 0.000010  loss: 0.0527  time: 1.0685  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4500/7110]  eta: 0:43:41  lr: 0.000010  loss: 0.2950  time: 0.9577  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4550/7110]  eta: 0:42:51  lr: 0.000010  loss: 0.2615  time: 1.0215  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4600/7110]  eta: 0:42:00  lr: 0.000010  loss: 0.1617  time: 0.9867  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4650/7110]  eta: 0:41:09  lr: 0.000010  loss: 0.8832  time: 1.0181  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4700/7110]  eta: 0:40:18  lr: 0.000010  loss: 0.1040  time: 0.9760  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4750/7110]  eta: 0:39:27  lr: 0.000010  loss: 0.2724  time: 0.9728  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4800/7110]  eta: 0:38:37  lr: 0.000010  loss: 0.3581  time: 0.9867  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4850/7110]  eta: 0:37:47  lr: 0.000010  loss: 0.3137  time: 0.9879  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4900/7110]  eta: 0:36:57  lr: 0.000010  loss: 0.2047  time: 1.0214  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [4950/7110]  eta: 0:36:07  lr: 0.000010  loss: 0.5809  time: 0.9618  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5000/7110]  eta: 0:35:17  lr: 0.000010  loss: 0.7501  time: 1.0524  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5050/7110]  eta: 0:34:26  lr: 0.000010  loss: 0.3673  time: 1.0337  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5100/7110]  eta: 0:33:36  lr: 0.000010  loss: 0.2327  time: 1.0308  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5150/7110]  eta: 0:32:47  lr: 0.000010  loss: 0.3337  time: 1.0060  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5200/7110]  eta: 0:31:57  lr: 0.000010  loss: 0.1364  time: 0.9830  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5250/7110]  eta: 0:31:06  lr: 0.000010  loss: 0.4030  time: 1.0301  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5300/7110]  eta: 0:30:16  lr: 0.000010  loss: 0.3438  time: 1.0167  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5350/7110]  eta: 0:29:26  lr: 0.000010  loss: 0.0556  time: 0.9845  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5400/7110]  eta: 0:28:36  lr: 0.000010  loss: 0.4724  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5450/7110]  eta: 0:27:46  lr: 0.000010  loss: 0.1062  time: 0.9890  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5500/7110]  eta: 0:26:55  lr: 0.000010  loss: 0.1538  time: 1.0077  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5550/7110]  eta: 0:26:05  lr: 0.000010  loss: 0.1300  time: 0.9888  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5600/7110]  eta: 0:25:15  lr: 0.000010  loss: 0.1788  time: 0.9804  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5650/7110]  eta: 0:24:24  lr: 0.000010  loss: 0.0218  time: 1.0079  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.1487  time: 1.0822  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.2457  time: 1.0341  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.3939  time: 0.9684  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 0.3841  time: 1.0709  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.1418  time: 0.9806  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [5950/7110]  eta: 0:19:24  lr: 0.000010  loss: 0.1474  time: 1.0350  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.0791  time: 0.9801  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.0769  time: 0.9598  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.2734  time: 1.0145  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.1514  time: 1.0186  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.2246  time: 1.0294  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.3110  time: 1.0188  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6300/7110]  eta: 0:13:33  lr: 0.000010  loss: 0.3116  time: 0.9890  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.0351  time: 0.9737  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.0989  time: 1.0302  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.4530  time: 0.9674  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.0341  time: 0.9598  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.0684  time: 1.0213  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6600/7110]  eta: 0:08:32  lr: 0.000010  loss: 0.2772  time: 1.0011  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.0110  time: 0.9334  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.3300  time: 0.9596  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.2547  time: 0.9599  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.2249  time: 0.9559  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.3679  time: 1.0344  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.2453  time: 1.0214  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.5713  time: 1.0339  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.3056  time: 0.9989  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.0534  time: 0.9927  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.4984  time: 0.9944  data: 0.0000  max mem: 66110
Train: data epoch: [26]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0851  time: 1.1098  data: 0.0000  max mem: 66110
Train: data epoch: [26] Total time: 1:58:57 (1.0038 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:14:00    time: 20.5313  data: 19.2746  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:41    time: 3.1410  data: 1.7531  max mem: 66110
Evaluation  [  20/1093]  eta: 0:40:49    time: 1.3700  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:33:59    time: 1.2456  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:19    time: 1.2617  data: 0.0009  max mem: 66110
Evaluation  [  50/1093]  eta: 0:29:48    time: 1.3999  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:27:38    time: 1.2381  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:26:59    time: 1.2461  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:13    time: 1.3947  data: 0.0009  max mem: 66110
Evaluation  [  90/1093]  eta: 0:25:39    time: 1.3625  data: 0.0009  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:20    time: 1.4434  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:01    time: 1.4931  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:26    time: 1.3827  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:11    time: 1.4006  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:36    time: 1.3592  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:08    time: 1.2376  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:52    time: 1.3656  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:25    time: 1.3508  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:08    time: 1.3274  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:21:54    time: 1.4359  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:32    time: 1.3801  data: 0.0009  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:29    time: 1.5080  data: 0.0009  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:04    time: 1.4609  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:44    time: 1.2418  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:26    time: 1.3148  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:11    time: 1.3855  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:05    time: 1.5567  data: 0.0009  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:40    time: 1.3978  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:22    time: 1.2071  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:15    time: 1.4966  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:54    time: 1.4331  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:34    time: 1.1989  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:18    time: 1.2964  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:02    time: 1.3472  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:50    time: 1.4192  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:36    time: 1.4809  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:18    time: 1.3291  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:01    time: 1.2611  data: 0.0009  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:51    time: 1.4595  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:37    time: 1.5093  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:23    time: 1.4381  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:09    time: 1.4521  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:55    time: 1.4238  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:40    time: 1.3787  data: 0.0009  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:21    time: 1.2162  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:06    time: 1.2393  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:51    time: 1.3597  data: 0.0009  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:35    time: 1.2953  data: 0.0009  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:21    time: 1.3493  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:09    time: 1.4746  data: 0.0009  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:57    time: 1.5630  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:44    time: 1.5622  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:30    time: 1.4808  data: 0.0009  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:16    time: 1.4543  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:03    time: 1.4620  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:49    time: 1.4351  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:32    time: 1.2663  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:17    time: 1.2586  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:03    time: 1.3747  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:47    time: 1.2687  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:34    time: 1.3682  data: 0.0009  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:20    time: 1.4989  data: 0.0009  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:06    time: 1.3944  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:52    time: 1.3877  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:37    time: 1.3576  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:23    time: 1.3901  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:08    time: 1.3828  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:54    time: 1.3611  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:41    time: 1.4426  data: 0.0009  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:27    time: 1.4980  data: 0.0009  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:13    time: 1.4421  data: 0.0009  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:57    time: 1.2447  data: 0.0009  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:42    time: 1.1861  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:27    time: 1.1486  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:12    time: 1.1177  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:58    time: 1.2761  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:44    time: 1.3872  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:30    time: 1.3904  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:16    time: 1.4516  data: 0.0009  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:01    time: 1.2888  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:48    time: 1.3636  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:34    time: 1.5450  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:21    time: 1.4580  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:07    time: 1.5232  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:53    time: 1.4096  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:39    time: 1.3327  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:25    time: 1.3320  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:11    time: 1.3364  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:57    time: 1.4998  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:43    time: 1.3584  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:29    time: 1.2384  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:15    time: 1.3787  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:01    time: 1.4444  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.3860  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3264  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.3277  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.3837  data: 0.0009  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4303  data: 0.0009  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3128  data: 0.0009  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3003  data: 0.0009  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4318  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4870  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.3260  data: 0.0011  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1402  data: 0.0011  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2447  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3544  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3957  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4432  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3882  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3506  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3210  data: 0.0416  max mem: 66110
Evaluation Total time: 0:25:17 (1.3883 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_26_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [27]  [   0/7110]  eta: 2 days, 6:26:40  lr: 0.000010  loss: 0.0694  time: 27.5669  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [  50/7110]  eta: 2:57:13  lr: 0.000010  loss: 0.0423  time: 0.9866  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 100/7110]  eta: 2:26:27  lr: 0.000010  loss: 0.0658  time: 0.9994  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 150/7110]  eta: 2:16:16  lr: 0.000010  loss: 0.3797  time: 1.0156  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 200/7110]  eta: 2:10:16  lr: 0.000010  loss: 0.0344  time: 0.9590  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 250/7110]  eta: 2:06:33  lr: 0.000010  loss: 0.1660  time: 1.0079  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 300/7110]  eta: 2:04:36  lr: 0.000010  loss: 0.1164  time: 1.0437  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 350/7110]  eta: 2:02:12  lr: 0.000010  loss: 0.1512  time: 1.0068  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 400/7110]  eta: 2:00:36  lr: 0.000010  loss: 0.0549  time: 1.0351  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 450/7110]  eta: 1:58:28  lr: 0.000010  loss: 0.1991  time: 0.9814  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 500/7110]  eta: 1:56:55  lr: 0.000010  loss: 0.3983  time: 0.9799  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 550/7110]  eta: 1:55:04  lr: 0.000010  loss: 0.2370  time: 0.9969  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 600/7110]  eta: 1:53:45  lr: 0.000010  loss: 0.5303  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 650/7110]  eta: 1:52:22  lr: 0.000010  loss: 0.1392  time: 1.0288  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 700/7110]  eta: 1:51:24  lr: 0.000010  loss: 0.2859  time: 1.0899  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 750/7110]  eta: 1:50:14  lr: 0.000010  loss: 0.3509  time: 0.9971  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 800/7110]  eta: 1:49:01  lr: 0.000010  loss: 0.6526  time: 0.9706  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 850/7110]  eta: 1:47:45  lr: 0.000010  loss: 0.1169  time: 0.9411  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 900/7110]  eta: 1:46:43  lr: 0.000010  loss: 0.1442  time: 1.0429  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [ 950/7110]  eta: 1:45:45  lr: 0.000010  loss: 0.1495  time: 1.0878  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1000/7110]  eta: 1:44:39  lr: 0.000010  loss: 0.3615  time: 0.9897  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1050/7110]  eta: 1:43:43  lr: 0.000010  loss: 0.0461  time: 1.0183  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1100/7110]  eta: 1:42:33  lr: 0.000010  loss: 0.8523  time: 0.9606  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1150/7110]  eta: 1:41:44  lr: 0.000010  loss: 0.1666  time: 1.0664  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1200/7110]  eta: 1:40:55  lr: 0.000010  loss: 0.5051  time: 1.0391  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1250/7110]  eta: 1:39:49  lr: 0.000010  loss: 0.6309  time: 1.0007  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1300/7110]  eta: 1:38:53  lr: 0.000010  loss: 0.2264  time: 0.9817  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1350/7110]  eta: 1:38:01  lr: 0.000010  loss: 0.1852  time: 0.9847  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1400/7110]  eta: 1:36:59  lr: 0.000010  loss: 0.3121  time: 0.9490  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1450/7110]  eta: 1:36:14  lr: 0.000010  loss: 1.8365  time: 1.0305  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1500/7110]  eta: 1:35:18  lr: 0.000010  loss: 0.3309  time: 0.9767  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1550/7110]  eta: 1:34:25  lr: 0.000010  loss: 0.3069  time: 0.9692  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1600/7110]  eta: 1:33:32  lr: 0.000010  loss: 0.0877  time: 1.0134  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1650/7110]  eta: 1:32:38  lr: 0.000010  loss: 0.4540  time: 1.0254  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1700/7110]  eta: 1:31:48  lr: 0.000010  loss: 0.0435  time: 1.0091  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1750/7110]  eta: 1:31:01  lr: 0.000010  loss: 0.0599  time: 1.0139  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1800/7110]  eta: 1:30:03  lr: 0.000010  loss: 0.4214  time: 1.0225  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1850/7110]  eta: 1:29:11  lr: 0.000010  loss: 0.1039  time: 0.9815  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1900/7110]  eta: 1:28:21  lr: 0.000010  loss: 0.1481  time: 1.0149  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [1950/7110]  eta: 1:27:27  lr: 0.000010  loss: 0.0547  time: 0.9596  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2000/7110]  eta: 1:26:32  lr: 0.000010  loss: 0.0801  time: 0.9900  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2050/7110]  eta: 1:25:41  lr: 0.000010  loss: 0.1486  time: 0.9879  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2100/7110]  eta: 1:24:45  lr: 0.000010  loss: 0.8739  time: 1.0087  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2150/7110]  eta: 1:23:53  lr: 0.000010  loss: 0.1913  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2200/7110]  eta: 1:23:05  lr: 0.000010  loss: 0.7370  time: 1.0640  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2250/7110]  eta: 1:22:16  lr: 0.000010  loss: 0.2894  time: 1.0392  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2300/7110]  eta: 1:21:20  lr: 0.000010  loss: 0.7838  time: 0.9544  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2350/7110]  eta: 1:20:27  lr: 0.000010  loss: 0.4646  time: 1.0084  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2400/7110]  eta: 1:19:35  lr: 0.000010  loss: 0.4824  time: 0.9451  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2450/7110]  eta: 1:18:45  lr: 0.000010  loss: 0.5070  time: 1.0202  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2500/7110]  eta: 1:17:55  lr: 0.000010  loss: 0.2391  time: 1.0460  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2550/7110]  eta: 1:17:01  lr: 0.000010  loss: 0.1969  time: 0.9707  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2600/7110]  eta: 1:16:07  lr: 0.000010  loss: 0.0590  time: 0.9676  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2650/7110]  eta: 1:15:14  lr: 0.000010  loss: 0.2762  time: 0.9667  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2700/7110]  eta: 1:14:21  lr: 0.000010  loss: 0.2186  time: 0.9923  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2750/7110]  eta: 1:13:28  lr: 0.000010  loss: 0.2904  time: 0.9979  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2800/7110]  eta: 1:12:35  lr: 0.000010  loss: 0.3481  time: 0.9847  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2850/7110]  eta: 1:11:43  lr: 0.000010  loss: 0.2599  time: 0.9640  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2900/7110]  eta: 1:10:51  lr: 0.000010  loss: 0.0913  time: 0.9613  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [2950/7110]  eta: 1:10:01  lr: 0.000010  loss: 0.1656  time: 1.0481  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3000/7110]  eta: 1:09:11  lr: 0.000010  loss: 0.3112  time: 1.0304  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3050/7110]  eta: 1:08:18  lr: 0.000010  loss: 0.0406  time: 0.9205  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3100/7110]  eta: 1:07:27  lr: 0.000010  loss: 0.2582  time: 0.9981  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3150/7110]  eta: 1:06:35  lr: 0.000010  loss: 0.0244  time: 1.0077  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3200/7110]  eta: 1:05:44  lr: 0.000010  loss: 0.1814  time: 0.9943  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3250/7110]  eta: 1:04:52  lr: 0.000010  loss: 0.0864  time: 0.9682  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3300/7110]  eta: 1:03:59  lr: 0.000010  loss: 0.2596  time: 0.9574  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3350/7110]  eta: 1:03:10  lr: 0.000010  loss: 0.2515  time: 0.9720  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3400/7110]  eta: 1:02:18  lr: 0.000010  loss: 0.3863  time: 0.9354  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3450/7110]  eta: 1:01:26  lr: 0.000010  loss: 0.1641  time: 1.0109  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3500/7110]  eta: 1:00:35  lr: 0.000010  loss: 0.3244  time: 0.9976  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3550/7110]  eta: 0:59:45  lr: 0.000010  loss: 0.1014  time: 0.9992  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3600/7110]  eta: 0:58:54  lr: 0.000010  loss: 1.6839  time: 1.0343  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3650/7110]  eta: 0:58:04  lr: 0.000010  loss: 0.1555  time: 1.0815  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3700/7110]  eta: 0:57:14  lr: 0.000010  loss: 0.2179  time: 1.0440  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3750/7110]  eta: 0:56:23  lr: 0.000010  loss: 0.2127  time: 1.0122  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3800/7110]  eta: 0:55:33  lr: 0.000010  loss: 0.1153  time: 0.9883  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3850/7110]  eta: 0:54:41  lr: 0.000010  loss: 0.1810  time: 0.9721  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3900/7110]  eta: 0:53:50  lr: 0.000010  loss: 0.5019  time: 0.9788  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [3950/7110]  eta: 0:53:00  lr: 0.000010  loss: 0.4335  time: 0.9957  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4000/7110]  eta: 0:52:10  lr: 0.000010  loss: 0.4501  time: 1.0510  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4050/7110]  eta: 0:51:20  lr: 0.000010  loss: 0.4463  time: 1.0348  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4100/7110]  eta: 0:50:29  lr: 0.000010  loss: 0.1484  time: 0.9605  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4150/7110]  eta: 0:49:40  lr: 0.000010  loss: 0.0364  time: 1.0979  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4200/7110]  eta: 0:48:50  lr: 0.000010  loss: 0.1162  time: 1.0043  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4250/7110]  eta: 0:47:59  lr: 0.000010  loss: 0.0756  time: 1.0325  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4300/7110]  eta: 0:47:09  lr: 0.000010  loss: 0.1308  time: 1.0640  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4350/7110]  eta: 0:46:17  lr: 0.000010  loss: 0.5978  time: 0.9800  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4400/7110]  eta: 0:45:26  lr: 0.000010  loss: 0.2419  time: 0.9715  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4450/7110]  eta: 0:44:34  lr: 0.000010  loss: 0.1117  time: 1.0143  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4500/7110]  eta: 0:43:44  lr: 0.000010  loss: 0.0728  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4550/7110]  eta: 0:42:53  lr: 0.000010  loss: 0.8614  time: 0.9864  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4600/7110]  eta: 0:42:03  lr: 0.000010  loss: 0.2287  time: 1.0178  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4650/7110]  eta: 0:41:13  lr: 0.000010  loss: 0.0539  time: 1.0289  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4700/7110]  eta: 0:40:22  lr: 0.000010  loss: 0.0619  time: 0.9526  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4750/7110]  eta: 0:39:32  lr: 0.000010  loss: 0.4114  time: 1.0639  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4800/7110]  eta: 0:38:43  lr: 0.000010  loss: 0.1423  time: 1.0661  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4850/7110]  eta: 0:37:51  lr: 0.000010  loss: 0.5108  time: 0.9219  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4900/7110]  eta: 0:37:01  lr: 0.000010  loss: 0.0944  time: 0.9920  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [4950/7110]  eta: 0:36:11  lr: 0.000010  loss: 0.1120  time: 1.0112  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5000/7110]  eta: 0:35:19  lr: 0.000010  loss: 0.1538  time: 0.9884  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5050/7110]  eta: 0:34:30  lr: 0.000010  loss: 0.2457  time: 0.9915  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5100/7110]  eta: 0:33:39  lr: 0.000010  loss: 0.3139  time: 0.9726  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5150/7110]  eta: 0:32:49  lr: 0.000010  loss: 0.0468  time: 0.9965  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5200/7110]  eta: 0:31:59  lr: 0.000010  loss: 0.3623  time: 1.0218  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5250/7110]  eta: 0:31:09  lr: 0.000010  loss: 0.8338  time: 1.0097  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5300/7110]  eta: 0:30:19  lr: 0.000010  loss: 0.3903  time: 1.0796  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5350/7110]  eta: 0:29:28  lr: 0.000010  loss: 0.4636  time: 0.9562  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5400/7110]  eta: 0:28:37  lr: 0.000010  loss: 0.4093  time: 0.9397  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5450/7110]  eta: 0:27:46  lr: 0.000010  loss: 0.0638  time: 0.9358  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5500/7110]  eta: 0:26:56  lr: 0.000010  loss: 0.3483  time: 0.9417  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5550/7110]  eta: 0:26:05  lr: 0.000010  loss: 0.3217  time: 1.0115  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5600/7110]  eta: 0:25:15  lr: 0.000010  loss: 0.1255  time: 1.0276  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5650/7110]  eta: 0:24:25  lr: 0.000010  loss: 0.0993  time: 0.9989  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.1664  time: 0.9730  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.0361  time: 0.9790  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.6905  time: 1.0196  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 0.1347  time: 0.9671  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.5507  time: 1.0243  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.1336  time: 1.0240  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.2562  time: 0.9694  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.3380  time: 1.0262  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.0623  time: 0.9739  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.0769  time: 1.0333  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.1397  time: 0.9685  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.4380  time: 0.9476  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.3152  time: 0.9694  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.2073  time: 1.0473  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.3034  time: 1.0412  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.4483  time: 0.9824  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.0979  time: 1.0022  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.3282  time: 1.0035  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.0436  time: 0.9421  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.2467  time: 1.0048  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.2408  time: 0.9792  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.3598  time: 1.0332  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.2589  time: 0.9549  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.0427  time: 0.9374  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1894  time: 1.0159  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.0628  time: 1.0124  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.5230  time: 0.9580  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.0763  time: 1.0340  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1392  time: 0.9619  data: 0.0000  max mem: 66110
Train: data epoch: [27]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.5537  time: 1.0572  data: 0.0000  max mem: 66110
Train: data epoch: [27] Total time: 1:58:50 (1.0028 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:20:32    time: 20.8897  data: 19.6507  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:20    time: 3.1771  data: 1.7873  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:08    time: 1.3716  data: 0.0009  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:10    time: 1.2419  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:26    time: 1.2564  data: 0.0009  max mem: 66110
Evaluation  [  50/1093]  eta: 0:29:50    time: 1.3874  data: 0.0009  max mem: 66110
Evaluation  [  60/1093]  eta: 0:27:55    time: 1.2741  data: 0.0009  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:13    time: 1.2926  data: 0.0009  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:25    time: 1.3917  data: 0.0009  max mem: 66110
Evaluation  [  90/1093]  eta: 0:25:52    time: 1.3712  data: 0.0009  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:26    time: 1.4239  data: 0.0009  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:03    time: 1.4509  data: 0.0009  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:30    time: 1.3809  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:17    time: 1.4198  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:41    time: 1.3722  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:17    time: 1.2744  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:55    time: 1.3547  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:28    time: 1.3051  data: 0.0009  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:09    time: 1.3129  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:21:58    time: 1.4485  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:35    time: 1.4013  data: 0.0009  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:33    time: 1.5176  data: 0.0009  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:10    time: 1.4933  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:49    time: 1.2595  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:31    time: 1.3260  data: 0.0009  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:17    time: 1.4027  data: 0.0009  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:10    time: 1.5527  data: 0.0009  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:45    time: 1.3932  data: 0.0009  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:27    time: 1.2122  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:19    time: 1.4924  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:57    time: 1.4175  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:38    time: 1.2022  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:24    time: 1.3470  data: 0.0009  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:06    time: 1.3639  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:54    time: 1.3987  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:40    time: 1.4735  data: 0.0009  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:20    time: 1.2883  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:04    time: 1.2514  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:53    time: 1.4615  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:39    time: 1.5017  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:26    time: 1.4547  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:08    time: 1.3543  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:55    time: 1.3399  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:39    time: 1.4005  data: 0.0009  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:21    time: 1.2311  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:06    time: 1.2366  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:51    time: 1.3557  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:36    time: 1.3400  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:22    time: 1.3869  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:09    time: 1.4659  data: 0.0009  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:58    time: 1.5535  data: 0.0009  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:44    time: 1.5474  data: 0.0009  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:30    time: 1.4638  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:17    time: 1.4615  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:03    time: 1.4728  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:49    time: 1.4219  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:32    time: 1.2529  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:17    time: 1.2356  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:03    time: 1.3510  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:46    time: 1.2635  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:34    time: 1.3686  data: 0.0009  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:20    time: 1.5058  data: 0.0009  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:05    time: 1.4018  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:51    time: 1.3934  data: 0.0009  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:37    time: 1.3578  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:23    time: 1.3897  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:08    time: 1.3726  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:54    time: 1.3401  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:40    time: 1.4093  data: 0.0009  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:27    time: 1.4610  data: 0.0009  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:12    time: 1.4276  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:57    time: 1.2445  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:42    time: 1.1827  data: 0.0009  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:26    time: 1.1584  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:12    time: 1.1988  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:58    time: 1.3549  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:44    time: 1.3977  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:30    time: 1.3711  data: 0.0011  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:16    time: 1.4329  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:01    time: 1.2675  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:48    time: 1.3420  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:34    time: 1.5477  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:20    time: 1.4517  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:07    time: 1.5145  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:53    time: 1.4300  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:39    time: 1.3608  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:25    time: 1.3562  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:11    time: 1.3347  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:57    time: 1.4848  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:42    time: 1.3296  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:29    time: 1.2427  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:15    time: 1.4179  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:01    time: 1.4402  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.3811  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3357  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.3271  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.3514  data: 0.0009  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4093  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3593  data: 0.0009  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3645  data: 0.0009  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4560  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4487  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2761  data: 0.0009  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1320  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2484  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3478  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3781  data: 0.0009  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4319  data: 0.0009  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3636  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3308  data: 0.0011  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3018  data: 0.0421  max mem: 66110
Evaluation Total time: 0:25:16 (1.3877 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_27_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [28]  [   0/7110]  eta: 2 days, 5:24:29  lr: 0.000010  loss: 0.4318  time: 27.0421  data: 0.0001  max mem: 66110
Train: data epoch: [28]  [  50/7110]  eta: 2:59:51  lr: 0.000010  loss: 0.4213  time: 1.0952  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 100/7110]  eta: 2:26:36  lr: 0.000010  loss: 0.2690  time: 0.9448  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 150/7110]  eta: 2:16:54  lr: 0.000010  loss: 0.1494  time: 1.0212  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 200/7110]  eta: 2:09:56  lr: 0.000010  loss: 0.0983  time: 0.9970  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 250/7110]  eta: 2:05:45  lr: 0.000010  loss: 0.5473  time: 0.9685  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 300/7110]  eta: 2:02:16  lr: 0.000010  loss: 0.1301  time: 0.9621  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 350/7110]  eta: 2:00:19  lr: 0.000010  loss: 0.0844  time: 0.9917  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 400/7110]  eta: 1:58:08  lr: 0.000010  loss: 0.1087  time: 0.9392  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 450/7110]  eta: 1:56:39  lr: 0.000010  loss: 0.1236  time: 1.0295  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 500/7110]  eta: 1:55:24  lr: 0.000010  loss: 0.2321  time: 1.0349  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 550/7110]  eta: 1:54:05  lr: 0.000010  loss: 0.4939  time: 0.9557  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 600/7110]  eta: 1:52:55  lr: 0.000010  loss: 0.0904  time: 0.9775  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 650/7110]  eta: 1:51:36  lr: 0.000010  loss: 0.2200  time: 0.9705  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 700/7110]  eta: 1:50:35  lr: 0.000010  loss: 0.2481  time: 1.0338  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 750/7110]  eta: 1:49:15  lr: 0.000010  loss: 0.5279  time: 0.9722  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 800/7110]  eta: 1:48:15  lr: 0.000010  loss: 0.4488  time: 1.0112  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 850/7110]  eta: 1:47:23  lr: 0.000010  loss: 0.2313  time: 1.0322  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 900/7110]  eta: 1:46:05  lr: 0.000010  loss: 0.1432  time: 0.9586  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [ 950/7110]  eta: 1:45:16  lr: 0.000010  loss: 0.1783  time: 1.0697  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1000/7110]  eta: 1:44:15  lr: 0.000010  loss: 0.0389  time: 1.0274  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1050/7110]  eta: 1:43:06  lr: 0.000010  loss: 0.3294  time: 0.9560  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1100/7110]  eta: 1:41:58  lr: 0.000010  loss: 0.2809  time: 0.9985  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1150/7110]  eta: 1:41:00  lr: 0.000010  loss: 0.1711  time: 1.0098  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1200/7110]  eta: 1:40:03  lr: 0.000010  loss: 0.1748  time: 0.9413  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1250/7110]  eta: 1:39:21  lr: 0.000010  loss: 0.0529  time: 0.9907  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1300/7110]  eta: 1:38:29  lr: 0.000010  loss: 0.0343  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1350/7110]  eta: 1:37:42  lr: 0.000010  loss: 0.1124  time: 0.9790  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1400/7110]  eta: 1:36:49  lr: 0.000010  loss: 0.2089  time: 0.9873  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1450/7110]  eta: 1:35:59  lr: 0.000010  loss: 0.3633  time: 1.0191  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1500/7110]  eta: 1:35:02  lr: 0.000010  loss: 0.0691  time: 0.9897  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1550/7110]  eta: 1:34:11  lr: 0.000010  loss: 0.0887  time: 1.0338  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1600/7110]  eta: 1:33:18  lr: 0.000010  loss: 0.1491  time: 0.9732  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1650/7110]  eta: 1:32:30  lr: 0.000010  loss: 0.2636  time: 1.0218  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1700/7110]  eta: 1:31:40  lr: 0.000010  loss: 0.3374  time: 1.0051  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1750/7110]  eta: 1:30:45  lr: 0.000010  loss: 0.2391  time: 1.0157  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1800/7110]  eta: 1:29:46  lr: 0.000010  loss: 0.1049  time: 0.9336  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1850/7110]  eta: 1:28:59  lr: 0.000010  loss: 0.4085  time: 1.1100  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1900/7110]  eta: 1:28:10  lr: 0.000010  loss: 0.3978  time: 0.9372  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [1950/7110]  eta: 1:27:15  lr: 0.000010  loss: 0.3176  time: 0.9443  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2000/7110]  eta: 1:26:18  lr: 0.000010  loss: 0.1648  time: 0.9503  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2050/7110]  eta: 1:25:23  lr: 0.000010  loss: 0.7472  time: 0.9811  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2100/7110]  eta: 1:24:30  lr: 0.000010  loss: 0.2766  time: 0.9588  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2150/7110]  eta: 1:23:35  lr: 0.000010  loss: 0.0690  time: 0.9284  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2200/7110]  eta: 1:22:43  lr: 0.000010  loss: 0.0526  time: 0.9812  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2250/7110]  eta: 1:21:48  lr: 0.000010  loss: 0.0451  time: 1.0021  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2300/7110]  eta: 1:20:56  lr: 0.000010  loss: 1.5869  time: 1.0285  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2350/7110]  eta: 1:20:01  lr: 0.000010  loss: 0.7291  time: 0.9784  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2400/7110]  eta: 1:19:09  lr: 0.000010  loss: 0.2754  time: 1.0178  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2450/7110]  eta: 1:18:19  lr: 0.000010  loss: 0.3179  time: 1.0170  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2500/7110]  eta: 1:17:28  lr: 0.000010  loss: 0.8453  time: 0.9883  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2550/7110]  eta: 1:16:39  lr: 0.000010  loss: 0.2113  time: 1.0096  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2600/7110]  eta: 1:15:48  lr: 0.000010  loss: 0.2295  time: 1.0179  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2650/7110]  eta: 1:14:59  lr: 0.000010  loss: 0.1118  time: 1.0433  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2700/7110]  eta: 1:14:06  lr: 0.000010  loss: 0.4976  time: 0.9301  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2750/7110]  eta: 1:13:18  lr: 0.000010  loss: 0.3336  time: 1.0278  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2800/7110]  eta: 1:12:29  lr: 0.000010  loss: 0.4208  time: 1.0445  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2850/7110]  eta: 1:11:37  lr: 0.000010  loss: 0.4451  time: 1.0240  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2900/7110]  eta: 1:10:49  lr: 0.000010  loss: 0.4270  time: 1.0507  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [2950/7110]  eta: 1:09:57  lr: 0.000010  loss: 0.3597  time: 0.9861  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3000/7110]  eta: 1:09:06  lr: 0.000010  loss: 0.2512  time: 1.0112  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3050/7110]  eta: 1:08:16  lr: 0.000010  loss: 0.1203  time: 1.0032  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3100/7110]  eta: 1:07:23  lr: 0.000010  loss: 0.3371  time: 0.9692  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3150/7110]  eta: 1:06:30  lr: 0.000010  loss: 0.2985  time: 0.9639  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3200/7110]  eta: 1:05:40  lr: 0.000010  loss: 0.1365  time: 0.9902  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3250/7110]  eta: 1:04:47  lr: 0.000010  loss: 0.3618  time: 0.9701  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3300/7110]  eta: 1:03:57  lr: 0.000010  loss: 0.2245  time: 1.0045  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3350/7110]  eta: 1:03:05  lr: 0.000010  loss: 0.0165  time: 0.9814  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3400/7110]  eta: 1:02:13  lr: 0.000010  loss: 0.0967  time: 0.9636  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3450/7110]  eta: 1:01:22  lr: 0.000010  loss: 0.2205  time: 0.9717  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3500/7110]  eta: 1:00:30  lr: 0.000010  loss: 0.0441  time: 0.9567  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3550/7110]  eta: 0:59:40  lr: 0.000010  loss: 0.0152  time: 1.0002  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3600/7110]  eta: 0:58:52  lr: 0.000010  loss: 0.4470  time: 1.0469  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3650/7110]  eta: 0:58:00  lr: 0.000010  loss: 0.0720  time: 0.9526  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3700/7110]  eta: 0:57:07  lr: 0.000010  loss: 0.1908  time: 0.9243  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3750/7110]  eta: 0:56:15  lr: 0.000010  loss: 0.1211  time: 1.0000  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3800/7110]  eta: 0:55:25  lr: 0.000010  loss: 0.2344  time: 0.9679  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3850/7110]  eta: 0:54:33  lr: 0.000010  loss: 0.1456  time: 0.9807  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3900/7110]  eta: 0:53:42  lr: 0.000010  loss: 0.3445  time: 0.9768  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [3950/7110]  eta: 0:52:51  lr: 0.000010  loss: 0.2457  time: 0.9620  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4000/7110]  eta: 0:52:00  lr: 0.000010  loss: 0.0442  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4050/7110]  eta: 0:51:09  lr: 0.000010  loss: 0.2134  time: 0.9805  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4100/7110]  eta: 0:50:18  lr: 0.000010  loss: 0.4932  time: 0.9697  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4150/7110]  eta: 0:49:28  lr: 0.000010  loss: 0.0651  time: 1.0551  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4200/7110]  eta: 0:48:38  lr: 0.000010  loss: 0.0480  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4250/7110]  eta: 0:47:47  lr: 0.000010  loss: 0.1125  time: 0.9913  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4300/7110]  eta: 0:46:57  lr: 0.000010  loss: 0.3379  time: 1.0385  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4350/7110]  eta: 0:46:09  lr: 0.000010  loss: 0.1398  time: 1.0698  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4400/7110]  eta: 0:45:18  lr: 0.000010  loss: 0.4691  time: 0.9550  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4450/7110]  eta: 0:44:27  lr: 0.000010  loss: 0.1489  time: 0.9818  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4500/7110]  eta: 0:43:36  lr: 0.000010  loss: 1.6654  time: 1.0077  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4550/7110]  eta: 0:42:46  lr: 0.000010  loss: 0.1626  time: 0.9877  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4600/7110]  eta: 0:41:56  lr: 0.000010  loss: 0.1725  time: 1.0066  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4650/7110]  eta: 0:41:06  lr: 0.000010  loss: 0.1632  time: 0.9771  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4700/7110]  eta: 0:40:15  lr: 0.000010  loss: 0.4251  time: 0.9523  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4750/7110]  eta: 0:39:26  lr: 0.000010  loss: 0.1960  time: 1.1188  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4800/7110]  eta: 0:38:36  lr: 0.000010  loss: 0.6076  time: 1.0001  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4850/7110]  eta: 0:37:46  lr: 0.000010  loss: 0.2102  time: 1.0398  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4900/7110]  eta: 0:36:57  lr: 0.000010  loss: 0.5316  time: 1.0117  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [4950/7110]  eta: 0:36:06  lr: 0.000010  loss: 0.3439  time: 0.9504  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5000/7110]  eta: 0:35:16  lr: 0.000010  loss: 0.3308  time: 1.0443  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5050/7110]  eta: 0:34:26  lr: 0.000010  loss: 0.0433  time: 1.0228  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5100/7110]  eta: 0:33:36  lr: 0.000010  loss: 0.1394  time: 0.9938  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5150/7110]  eta: 0:32:46  lr: 0.000010  loss: 0.4924  time: 0.9643  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5200/7110]  eta: 0:31:55  lr: 0.000010  loss: 0.1425  time: 0.9728  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5250/7110]  eta: 0:31:06  lr: 0.000010  loss: 0.2577  time: 1.0515  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5300/7110]  eta: 0:30:16  lr: 0.000010  loss: 0.4997  time: 1.0686  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5350/7110]  eta: 0:29:25  lr: 0.000010  loss: 0.2158  time: 1.0158  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5400/7110]  eta: 0:28:35  lr: 0.000010  loss: 0.4048  time: 0.9706  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5450/7110]  eta: 0:27:45  lr: 0.000010  loss: 0.1610  time: 1.0030  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5500/7110]  eta: 0:26:55  lr: 0.000010  loss: 0.1871  time: 0.9890  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5550/7110]  eta: 0:26:05  lr: 0.000010  loss: 0.0870  time: 1.0388  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5600/7110]  eta: 0:25:15  lr: 0.000010  loss: 0.0225  time: 1.0111  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5650/7110]  eta: 0:24:24  lr: 0.000010  loss: 0.0654  time: 0.9703  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5700/7110]  eta: 0:23:35  lr: 0.000010  loss: 0.4790  time: 1.0132  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.2674  time: 0.9535  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.0273  time: 0.9714  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.0870  time: 0.9979  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.1650  time: 0.9952  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [5950/7110]  eta: 0:19:22  lr: 0.000010  loss: 0.1343  time: 0.9444  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6000/7110]  eta: 0:18:32  lr: 0.000010  loss: 0.6437  time: 0.9332  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6050/7110]  eta: 0:17:41  lr: 0.000010  loss: 0.6292  time: 0.9633  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.2311  time: 1.0560  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.2157  time: 0.9852  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6200/7110]  eta: 0:15:11  lr: 0.000010  loss: 0.1927  time: 0.9803  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6250/7110]  eta: 0:14:21  lr: 0.000010  loss: 0.2632  time: 1.0262  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6300/7110]  eta: 0:13:31  lr: 0.000010  loss: 0.1949  time: 1.0022  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.2060  time: 1.0156  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.1766  time: 1.0356  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.0697  time: 0.9752  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.3863  time: 1.0081  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6550/7110]  eta: 0:09:20  lr: 0.000010  loss: 0.0695  time: 0.9900  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.4257  time: 1.0084  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.7422  time: 0.9623  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.1386  time: 1.0924  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.0348  time: 0.9653  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.6398  time: 1.0846  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.5207  time: 0.9974  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.2332  time: 1.0327  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.2017  time: 0.9625  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.1221  time: 0.9992  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.3916  time: 1.0245  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.3745  time: 0.9420  data: 0.0000  max mem: 66110
Train: data epoch: [28]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0541  time: 1.0933  data: 0.0000  max mem: 66110
Train: data epoch: [28] Total time: 1:58:44 (1.0021 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:26:54    time: 21.2392  data: 19.9900  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:58    time: 3.2116  data: 1.8183  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:30    time: 1.3750  data: 0.0011  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:24    time: 1.2443  data: 0.0011  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:38    time: 1.2591  data: 0.0011  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:02    time: 1.3972  data: 0.0011  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:09    time: 1.2934  data: 0.0011  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:32    time: 1.3270  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:41    time: 1.4128  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:07    time: 1.3737  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:38    time: 1.4202  data: 0.0011  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:14    time: 1.4437  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:39    time: 1.3745  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:24    time: 1.4070  data: 0.0012  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:47    time: 1.3610  data: 0.0011  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:19    time: 1.2480  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:58    time: 1.3368  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:29    time: 1.2924  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:11    time: 1.3007  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:21:55    time: 1.4145  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:33    time: 1.3674  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:30    time: 1.5068  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:06    time: 1.4690  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:43    time: 1.2216  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:25    time: 1.2787  data: 0.0009  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:10    time: 1.3843  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:04    time: 1.5516  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:39    time: 1.3890  data: 0.0011  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:19    time: 1.1744  data: 0.0012  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:12    time: 1.4607  data: 0.0011  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:50    time: 1.4126  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:30    time: 1.1766  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:14    time: 1.2766  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:17:57    time: 1.3233  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:46    time: 1.4188  data: 0.0011  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:32    time: 1.4849  data: 0.0012  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:13    time: 1.3031  data: 0.0011  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:16:58    time: 1.2675  data: 0.0011  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:46    time: 1.4463  data: 0.0011  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:31    time: 1.4578  data: 0.0011  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:19    time: 1.4452  data: 0.0011  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:04    time: 1.4189  data: 0.0012  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:50    time: 1.3946  data: 0.0011  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:35    time: 1.4058  data: 0.0011  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:16    time: 1.2239  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:02    time: 1.2231  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:47    time: 1.3507  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:31    time: 1.3043  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:18    time: 1.3566  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:05    time: 1.4833  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:54    time: 1.5665  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:41    time: 1.5793  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:27    time: 1.4921  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:14    time: 1.4780  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:01    time: 1.4973  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:46    time: 1.4322  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:29    time: 1.2330  data: 0.0011  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:14    time: 1.2084  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:00    time: 1.3514  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:44    time: 1.2699  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:31    time: 1.3729  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:18    time: 1.5123  data: 0.0011  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:03    time: 1.4008  data: 0.0011  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:49    time: 1.3881  data: 0.0011  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:35    time: 1.3579  data: 0.0012  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:21    time: 1.3894  data: 0.0011  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:06    time: 1.3750  data: 0.0011  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:52    time: 1.3424  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:38    time: 1.3956  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:25    time: 1.4649  data: 0.0011  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:10    time: 1.4273  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:55    time: 1.2008  data: 0.0009  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:39    time: 1.1114  data: 0.0009  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:24    time: 1.1034  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:09    time: 1.1590  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:56    time: 1.3478  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:42    time: 1.4128  data: 0.0012  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:28    time: 1.3753  data: 0.0012  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:14    time: 1.4468  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:06:59    time: 1.2686  data: 0.0011  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:46    time: 1.3336  data: 0.0011  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:33    time: 1.5377  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:19    time: 1.4463  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:06    time: 1.5122  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:51    time: 1.4276  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:37    time: 1.3411  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:23    time: 1.3253  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:09    time: 1.3203  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:56    time: 1.4894  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:41    time: 1.3626  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:27    time: 1.2264  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:14    time: 1.3703  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:00    time: 1.4338  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:46    time: 1.3776  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:32    time: 1.3367  data: 0.0011  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:18    time: 1.3440  data: 0.0012  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:04    time: 1.3881  data: 0.0012  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:50    time: 1.4397  data: 0.0011  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:36    time: 1.3641  data: 0.0012  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:22    time: 1.3601  data: 0.0012  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4758  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4669  data: 0.0011  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2863  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1406  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2470  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3504  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3645  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4166  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:17    time: 1.3569  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3110  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2806  data: 0.0404  max mem: 66110
Evaluation Total time: 0:25:11 (1.3830 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_28_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [29]  [   0/7110]  eta: 2 days, 5:39:43  lr: 0.000010  loss: 0.2746  time: 27.1707  data: 0.0001  max mem: 66110
Train: data epoch: [29]  [  50/7110]  eta: 2:55:43  lr: 0.000010  loss: 0.0808  time: 0.9842  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 100/7110]  eta: 2:25:16  lr: 0.000010  loss: 0.1112  time: 0.9877  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 150/7110]  eta: 2:15:30  lr: 0.000010  loss: 1.1298  time: 0.9785  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 200/7110]  eta: 2:09:26  lr: 0.000010  loss: 0.1436  time: 1.0039  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 250/7110]  eta: 2:05:55  lr: 0.000010  loss: 0.6694  time: 1.0230  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 300/7110]  eta: 2:02:50  lr: 0.000010  loss: 0.1340  time: 0.9828  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 350/7110]  eta: 2:00:38  lr: 0.000010  loss: 0.0425  time: 1.0134  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 400/7110]  eta: 1:59:32  lr: 0.000010  loss: 0.3678  time: 1.0610  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 450/7110]  eta: 1:57:29  lr: 0.000010  loss: 0.1540  time: 0.9649  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 500/7110]  eta: 1:56:21  lr: 0.000010  loss: 1.8083  time: 1.0438  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 550/7110]  eta: 1:54:40  lr: 0.000010  loss: 0.3033  time: 1.0183  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 600/7110]  eta: 1:52:57  lr: 0.000010  loss: 0.0739  time: 0.9676  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 650/7110]  eta: 1:51:40  lr: 0.000010  loss: 0.0830  time: 0.9693  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 700/7110]  eta: 1:50:36  lr: 0.000010  loss: 0.0516  time: 1.0290  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 750/7110]  eta: 1:49:26  lr: 0.000010  loss: 0.0457  time: 1.0002  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 800/7110]  eta: 1:48:29  lr: 0.000010  loss: 0.1295  time: 1.0069  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 850/7110]  eta: 1:47:37  lr: 0.000010  loss: 0.1643  time: 0.9988  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 900/7110]  eta: 1:46:40  lr: 0.000010  loss: 0.7809  time: 1.0532  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [ 950/7110]  eta: 1:45:50  lr: 0.000010  loss: 0.2733  time: 1.0395  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1000/7110]  eta: 1:44:33  lr: 0.000010  loss: 0.3941  time: 0.9897  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1050/7110]  eta: 1:43:24  lr: 0.000010  loss: 0.3719  time: 0.9588  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1100/7110]  eta: 1:42:26  lr: 0.000010  loss: 0.0923  time: 0.9925  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1150/7110]  eta: 1:41:30  lr: 0.000010  loss: 0.1349  time: 1.0537  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1200/7110]  eta: 1:40:35  lr: 0.000010  loss: 0.4034  time: 0.9990  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1250/7110]  eta: 1:39:41  lr: 0.000010  loss: 0.9453  time: 0.9991  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1300/7110]  eta: 1:38:50  lr: 0.000010  loss: 0.1711  time: 0.9767  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1350/7110]  eta: 1:37:49  lr: 0.000010  loss: 0.1450  time: 1.0195  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1400/7110]  eta: 1:36:55  lr: 0.000010  loss: 0.0672  time: 1.0275  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1450/7110]  eta: 1:35:57  lr: 0.000010  loss: 0.2728  time: 0.9653  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1500/7110]  eta: 1:35:01  lr: 0.000010  loss: 0.2754  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1550/7110]  eta: 1:34:05  lr: 0.000010  loss: 0.2878  time: 0.9699  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1600/7110]  eta: 1:33:07  lr: 0.000010  loss: 0.1611  time: 0.9210  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1650/7110]  eta: 1:32:17  lr: 0.000010  loss: 0.3547  time: 1.0705  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1700/7110]  eta: 1:31:24  lr: 0.000010  loss: 0.0878  time: 0.9505  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1750/7110]  eta: 1:30:34  lr: 0.000010  loss: 0.1416  time: 0.9337  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1800/7110]  eta: 1:29:37  lr: 0.000010  loss: 0.0436  time: 0.9930  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1850/7110]  eta: 1:28:47  lr: 0.000010  loss: 0.3541  time: 0.9737  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1900/7110]  eta: 1:27:53  lr: 0.000010  loss: 0.0972  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [1950/7110]  eta: 1:26:59  lr: 0.000010  loss: 0.3878  time: 1.0455  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2000/7110]  eta: 1:26:10  lr: 0.000010  loss: 0.1686  time: 0.9787  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2050/7110]  eta: 1:25:19  lr: 0.000010  loss: 0.2055  time: 1.0166  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2100/7110]  eta: 1:24:20  lr: 0.000010  loss: 0.1130  time: 0.9441  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2150/7110]  eta: 1:23:28  lr: 0.000010  loss: 0.8022  time: 0.9799  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2200/7110]  eta: 1:22:39  lr: 0.000010  loss: 0.0866  time: 1.0039  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2250/7110]  eta: 1:21:45  lr: 0.000010  loss: 0.1657  time: 0.9238  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2300/7110]  eta: 1:20:52  lr: 0.000010  loss: 0.4208  time: 1.0073  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2350/7110]  eta: 1:19:59  lr: 0.000010  loss: 0.0196  time: 1.0150  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2400/7110]  eta: 1:19:06  lr: 0.000010  loss: 0.2955  time: 1.0004  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2450/7110]  eta: 1:18:16  lr: 0.000010  loss: 0.0762  time: 1.0508  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2500/7110]  eta: 1:17:24  lr: 0.000010  loss: 0.0710  time: 0.9681  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2550/7110]  eta: 1:16:34  lr: 0.000010  loss: 0.1035  time: 0.9873  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2600/7110]  eta: 1:15:42  lr: 0.000010  loss: 0.4981  time: 0.9822  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2650/7110]  eta: 1:14:51  lr: 0.000010  loss: 1.4853  time: 1.0033  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2700/7110]  eta: 1:14:02  lr: 0.000010  loss: 0.3094  time: 1.0160  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2750/7110]  eta: 1:13:13  lr: 0.000010  loss: 0.1577  time: 1.0117  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2800/7110]  eta: 1:12:25  lr: 0.000010  loss: 0.0664  time: 1.0146  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2850/7110]  eta: 1:11:35  lr: 0.000010  loss: 0.4994  time: 1.0090  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2900/7110]  eta: 1:10:44  lr: 0.000010  loss: 0.5391  time: 1.0099  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [2950/7110]  eta: 1:09:56  lr: 0.000010  loss: 0.0592  time: 1.0290  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3000/7110]  eta: 1:09:05  lr: 0.000010  loss: 0.0961  time: 1.0318  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3050/7110]  eta: 1:08:17  lr: 0.000010  loss: 0.1201  time: 1.0386  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3100/7110]  eta: 1:07:26  lr: 0.000010  loss: 0.4067  time: 0.9904  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3150/7110]  eta: 1:06:34  lr: 0.000010  loss: 0.0067  time: 1.0391  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3200/7110]  eta: 1:05:45  lr: 0.000010  loss: 0.1649  time: 1.0521  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3250/7110]  eta: 1:04:54  lr: 0.000010  loss: 0.0708  time: 1.0175  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3300/7110]  eta: 1:04:01  lr: 0.000010  loss: 0.5078  time: 0.9557  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3350/7110]  eta: 1:03:10  lr: 0.000010  loss: 0.2509  time: 1.0428  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3400/7110]  eta: 1:02:20  lr: 0.000010  loss: 0.1361  time: 0.9988  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3450/7110]  eta: 1:01:29  lr: 0.000010  loss: 0.2133  time: 0.9992  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3500/7110]  eta: 1:00:38  lr: 0.000010  loss: 0.2545  time: 1.0074  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3550/7110]  eta: 0:59:47  lr: 0.000010  loss: 0.3585  time: 0.9908  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3600/7110]  eta: 0:58:57  lr: 0.000010  loss: 0.0861  time: 1.0201  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3650/7110]  eta: 0:58:05  lr: 0.000010  loss: 0.2548  time: 0.9445  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3700/7110]  eta: 0:57:14  lr: 0.000010  loss: 0.0632  time: 0.9790  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3750/7110]  eta: 0:56:25  lr: 0.000010  loss: 1.0075  time: 1.0974  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3800/7110]  eta: 0:55:33  lr: 0.000010  loss: 0.2389  time: 0.9540  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3850/7110]  eta: 0:54:41  lr: 0.000010  loss: 0.0914  time: 0.9945  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3900/7110]  eta: 0:53:51  lr: 0.000010  loss: 0.0817  time: 1.0595  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [3950/7110]  eta: 0:53:02  lr: 0.000010  loss: 0.9968  time: 1.0317  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4000/7110]  eta: 0:52:13  lr: 0.000010  loss: 0.0752  time: 1.0245  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4050/7110]  eta: 0:51:22  lr: 0.000010  loss: 0.1416  time: 0.9988  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4100/7110]  eta: 0:50:30  lr: 0.000010  loss: 0.6458  time: 0.9510  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4150/7110]  eta: 0:49:39  lr: 0.000010  loss: 0.0825  time: 0.9506  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4200/7110]  eta: 0:48:46  lr: 0.000010  loss: 0.4676  time: 0.9664  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4250/7110]  eta: 0:47:55  lr: 0.000010  loss: 0.0656  time: 0.9780  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4300/7110]  eta: 0:47:04  lr: 0.000010  loss: 0.0856  time: 0.9905  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4350/7110]  eta: 0:46:14  lr: 0.000010  loss: 0.1434  time: 1.0061  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4400/7110]  eta: 0:45:24  lr: 0.000010  loss: 0.4774  time: 1.0035  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4450/7110]  eta: 0:44:35  lr: 0.000010  loss: 1.6671  time: 1.0166  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4500/7110]  eta: 0:43:44  lr: 0.000010  loss: 0.3085  time: 0.9109  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4550/7110]  eta: 0:42:53  lr: 0.000010  loss: 0.1665  time: 1.0473  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4600/7110]  eta: 0:42:02  lr: 0.000010  loss: 0.1491  time: 0.9553  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4650/7110]  eta: 0:41:11  lr: 0.000010  loss: 0.2053  time: 0.9566  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4700/7110]  eta: 0:40:21  lr: 0.000010  loss: 0.0447  time: 0.9865  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4750/7110]  eta: 0:39:31  lr: 0.000010  loss: 0.2691  time: 1.0861  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4800/7110]  eta: 0:38:40  lr: 0.000010  loss: 0.1942  time: 0.9242  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4850/7110]  eta: 0:37:49  lr: 0.000010  loss: 0.2797  time: 0.9820  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4900/7110]  eta: 0:36:58  lr: 0.000010  loss: 0.0920  time: 0.9343  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [4950/7110]  eta: 0:36:08  lr: 0.000010  loss: 0.1381  time: 1.0196  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5000/7110]  eta: 0:35:18  lr: 0.000010  loss: 0.1214  time: 1.0546  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5050/7110]  eta: 0:34:28  lr: 0.000010  loss: 0.2212  time: 1.0170  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5100/7110]  eta: 0:33:37  lr: 0.000010  loss: 0.1047  time: 0.9767  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5150/7110]  eta: 0:32:47  lr: 0.000010  loss: 0.1987  time: 0.9828  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5200/7110]  eta: 0:31:56  lr: 0.000010  loss: 0.0759  time: 0.9460  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5250/7110]  eta: 0:31:07  lr: 0.000010  loss: 0.1759  time: 1.0076  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5300/7110]  eta: 0:30:16  lr: 0.000010  loss: 0.5972  time: 0.9677  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5350/7110]  eta: 0:29:26  lr: 0.000010  loss: 0.2574  time: 1.0350  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5400/7110]  eta: 0:28:35  lr: 0.000010  loss: 0.2464  time: 0.9644  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5450/7110]  eta: 0:27:45  lr: 0.000010  loss: 0.4639  time: 0.9848  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5500/7110]  eta: 0:26:55  lr: 0.000010  loss: 0.0456  time: 1.0182  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5550/7110]  eta: 0:26:04  lr: 0.000010  loss: 0.1762  time: 1.0040  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5600/7110]  eta: 0:25:14  lr: 0.000010  loss: 0.0803  time: 0.9875  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5650/7110]  eta: 0:24:23  lr: 0.000010  loss: 0.2303  time: 0.9481  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5700/7110]  eta: 0:23:33  lr: 0.000010  loss: 0.0602  time: 1.0164  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5750/7110]  eta: 0:22:43  lr: 0.000010  loss: 0.5904  time: 0.9693  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5800/7110]  eta: 0:21:53  lr: 0.000010  loss: 0.9489  time: 1.0414  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.0976  time: 1.0348  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.0760  time: 1.0288  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [5950/7110]  eta: 0:19:22  lr: 0.000010  loss: 0.0647  time: 0.9709  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6000/7110]  eta: 0:18:32  lr: 0.000010  loss: 0.0934  time: 0.9587  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.6505  time: 1.0088  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6100/7110]  eta: 0:16:51  lr: 0.000010  loss: 0.0972  time: 0.9308  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6150/7110]  eta: 0:16:01  lr: 0.000010  loss: 0.1101  time: 1.0193  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6200/7110]  eta: 0:15:11  lr: 0.000010  loss: 0.1255  time: 1.0055  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6250/7110]  eta: 0:14:21  lr: 0.000010  loss: 0.2218  time: 1.0012  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6300/7110]  eta: 0:13:31  lr: 0.000010  loss: 0.1331  time: 1.0586  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.2562  time: 1.0247  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.4047  time: 0.9383  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.0778  time: 1.0357  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 1.8638  time: 1.0820  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6550/7110]  eta: 0:09:20  lr: 0.000010  loss: 0.1895  time: 0.9649  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.0401  time: 0.9795  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.3779  time: 0.9900  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.4871  time: 0.9876  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.3873  time: 1.0206  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.2423  time: 1.0741  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.5258  time: 1.0332  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.2087  time: 0.9610  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.2356  time: 1.0124  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.2667  time: 0.9670  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1601  time: 1.0026  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.3658  time: 0.9520  data: 0.0000  max mem: 66110
Train: data epoch: [29]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.1287  time: 1.1163  data: 0.0000  max mem: 66110
Train: data epoch: [29] Total time: 1:58:43 (1.0020 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:12:28    time: 20.4467  data: 19.1827  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:41    time: 3.1405  data: 1.7449  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:03    time: 1.3883  data: 0.0011  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:07    time: 1.2576  data: 0.0011  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:25    time: 1.2601  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:29:52    time: 1.3968  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:00    time: 1.2921  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:18    time: 1.3047  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:29    time: 1.3934  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:25:55    time: 1.3707  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:30    time: 1.4254  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:09    time: 1.4654  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:32    time: 1.3772  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:18    time: 1.3969  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:41    time: 1.3604  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:14    time: 1.2458  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:22:53    time: 1.3289  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:24    time: 1.2924  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:06    time: 1.3018  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:21:53    time: 1.4327  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:31    time: 1.3822  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:23    time: 1.4500  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:00    time: 1.4170  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:39    time: 1.2501  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:21    time: 1.3082  data: 0.0013  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:08    time: 1.3951  data: 0.0013  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:01    time: 1.5634  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:36    time: 1.3900  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:20    time: 1.2267  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:12    time: 1.5047  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:18:50    time: 1.4150  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:32    time: 1.2044  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:20    time: 1.3819  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:03    time: 1.4132  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:51    time: 1.4086  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:36    time: 1.4587  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:17    time: 1.3047  data: 0.0009  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:02    time: 1.2741  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:51    time: 1.4585  data: 0.0012  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:35    time: 1.4539  data: 0.0011  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:22    time: 1.4079  data: 0.0011  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:07    time: 1.4205  data: 0.0011  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:54    time: 1.4253  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:38    time: 1.4134  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:19    time: 1.2071  data: 0.0011  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:05    time: 1.2226  data: 0.0012  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:50    time: 1.3595  data: 0.0011  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:33    time: 1.2928  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:20    time: 1.3548  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:07    time: 1.4812  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:56    time: 1.5500  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:43    time: 1.5929  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:30    time: 1.5190  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:16    time: 1.4698  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:03    time: 1.4750  data: 0.0011  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:48    time: 1.4251  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:31    time: 1.2452  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:17    time: 1.2330  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:02    time: 1.3793  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:46    time: 1.2929  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:34    time: 1.3850  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:20    time: 1.5110  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:05    time: 1.3941  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:51    time: 1.3868  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:37    time: 1.3580  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:23    time: 1.4038  data: 0.0011  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:08    time: 1.3870  data: 0.0011  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:54    time: 1.3434  data: 0.0011  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:40    time: 1.4037  data: 0.0011  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:27    time: 1.4664  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:12    time: 1.4411  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:57    time: 1.2793  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:42    time: 1.2174  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:27    time: 1.1552  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:12    time: 1.1833  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:58    time: 1.3554  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:44    time: 1.3993  data: 0.0009  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:30    time: 1.3829  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:17    time: 1.4711  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:02    time: 1.3247  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:49    time: 1.3834  data: 0.0011  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:35    time: 1.5507  data: 0.0012  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:21    time: 1.4611  data: 0.0011  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:08    time: 1.5212  data: 0.0011  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:54    time: 1.4300  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:40    time: 1.3584  data: 0.0011  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:25    time: 1.3442  data: 0.0011  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:11    time: 1.3162  data: 0.0011  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:58    time: 1.4801  data: 0.0011  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:43    time: 1.3523  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:29    time: 1.2748  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:15    time: 1.4362  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:01    time: 1.4208  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.3551  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3430  data: 0.0011  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.3461  data: 0.0012  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.3887  data: 0.0011  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4374  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3615  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3856  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4840  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4552  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2966  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1563  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2614  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3683  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3842  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4338  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3772  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3346  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3122  data: 0.0484  max mem: 66110
Evaluation Total time: 0:25:21 (1.3925 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_29_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [30]  [   0/7110]  eta: 2 days, 6:11:52  lr: 0.000010  loss: 0.4739  time: 27.4420  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [  50/7110]  eta: 2:58:38  lr: 0.000010  loss: 0.2109  time: 0.9911  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 100/7110]  eta: 2:26:25  lr: 0.000010  loss: 0.1251  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 150/7110]  eta: 2:16:04  lr: 0.000010  loss: 0.0406  time: 0.9861  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 200/7110]  eta: 2:10:48  lr: 0.000010  loss: 0.3889  time: 1.0063  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 250/7110]  eta: 2:06:50  lr: 0.000010  loss: 0.2475  time: 1.0263  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 300/7110]  eta: 2:04:28  lr: 0.000010  loss: 0.1450  time: 1.0450  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 350/7110]  eta: 2:01:48  lr: 0.000010  loss: 0.0142  time: 1.0091  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 400/7110]  eta: 1:59:36  lr: 0.000010  loss: 0.3422  time: 0.9964  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 450/7110]  eta: 1:57:51  lr: 0.000010  loss: 1.7690  time: 0.9655  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 500/7110]  eta: 1:56:21  lr: 0.000010  loss: 0.0126  time: 1.0539  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 550/7110]  eta: 1:54:37  lr: 0.000010  loss: 0.2275  time: 0.9376  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 600/7110]  eta: 1:53:00  lr: 0.000010  loss: 0.2260  time: 0.9607  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 650/7110]  eta: 1:52:03  lr: 0.000010  loss: 1.3988  time: 1.0271  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 700/7110]  eta: 1:51:01  lr: 0.000010  loss: 0.0752  time: 1.0269  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 750/7110]  eta: 1:49:42  lr: 0.000010  loss: 0.0392  time: 0.9750  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 800/7110]  eta: 1:48:32  lr: 0.000010  loss: 0.1807  time: 0.9696  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 850/7110]  eta: 1:47:22  lr: 0.000010  loss: 0.0359  time: 0.9773  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 900/7110]  eta: 1:46:09  lr: 0.000010  loss: 0.3186  time: 0.9964  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [ 950/7110]  eta: 1:45:25  lr: 0.000010  loss: 0.1972  time: 1.0874  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1000/7110]  eta: 1:44:26  lr: 0.000010  loss: 0.1562  time: 0.9787  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1050/7110]  eta: 1:43:19  lr: 0.000010  loss: 0.0063  time: 0.9422  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1100/7110]  eta: 1:42:27  lr: 0.000010  loss: 0.0604  time: 1.0438  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1150/7110]  eta: 1:41:27  lr: 0.000010  loss: 0.2617  time: 1.0056  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1200/7110]  eta: 1:40:26  lr: 0.000010  loss: 0.1811  time: 0.9748  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1250/7110]  eta: 1:39:28  lr: 0.000010  loss: 0.3769  time: 0.9930  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1300/7110]  eta: 1:38:29  lr: 0.000010  loss: 0.5218  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1350/7110]  eta: 1:37:30  lr: 0.000010  loss: 0.1386  time: 0.9389  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1400/7110]  eta: 1:36:39  lr: 0.000010  loss: 0.5158  time: 1.0187  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1450/7110]  eta: 1:35:48  lr: 0.000010  loss: 0.1512  time: 1.0336  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1500/7110]  eta: 1:34:53  lr: 0.000010  loss: 0.0630  time: 0.9627  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1550/7110]  eta: 1:33:57  lr: 0.000010  loss: 0.1734  time: 1.0380  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1600/7110]  eta: 1:33:03  lr: 0.000010  loss: 0.3430  time: 0.9894  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1650/7110]  eta: 1:32:14  lr: 0.000010  loss: 1.5083  time: 1.0538  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1700/7110]  eta: 1:31:24  lr: 0.000010  loss: 0.1427  time: 1.0181  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1750/7110]  eta: 1:30:27  lr: 0.000010  loss: 0.2062  time: 0.9663  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1800/7110]  eta: 1:29:35  lr: 0.000010  loss: 0.3738  time: 0.9652  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1850/7110]  eta: 1:28:46  lr: 0.000010  loss: 0.1517  time: 1.0231  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1900/7110]  eta: 1:27:57  lr: 0.000010  loss: 0.0140  time: 0.9959  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [1950/7110]  eta: 1:27:05  lr: 0.000010  loss: 0.0606  time: 0.9483  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2000/7110]  eta: 1:26:15  lr: 0.000010  loss: 1.3596  time: 1.0301  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2050/7110]  eta: 1:25:21  lr: 0.000010  loss: 0.3771  time: 1.0315  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2100/7110]  eta: 1:24:23  lr: 0.000010  loss: 0.1966  time: 0.9527  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2150/7110]  eta: 1:23:36  lr: 0.000010  loss: 0.2529  time: 1.0088  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2200/7110]  eta: 1:22:45  lr: 0.000010  loss: 0.1642  time: 0.9954  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2250/7110]  eta: 1:21:53  lr: 0.000010  loss: 0.1302  time: 0.9635  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2300/7110]  eta: 1:21:07  lr: 0.000010  loss: 0.0268  time: 1.0450  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2350/7110]  eta: 1:20:15  lr: 0.000010  loss: 1.5260  time: 0.9540  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2400/7110]  eta: 1:19:29  lr: 0.000010  loss: 0.1841  time: 1.0667  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2450/7110]  eta: 1:18:34  lr: 0.000010  loss: 0.0974  time: 0.9982  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2500/7110]  eta: 1:17:43  lr: 0.000010  loss: 0.2038  time: 0.9761  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2550/7110]  eta: 1:16:53  lr: 0.000010  loss: 0.3520  time: 1.0145  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2600/7110]  eta: 1:16:02  lr: 0.000010  loss: 0.4869  time: 1.0166  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2650/7110]  eta: 1:15:10  lr: 0.000010  loss: 0.8943  time: 0.9993  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2700/7110]  eta: 1:14:18  lr: 0.000010  loss: 0.0494  time: 0.9899  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2750/7110]  eta: 1:13:28  lr: 0.000010  loss: 0.0884  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2800/7110]  eta: 1:12:39  lr: 0.000010  loss: 0.0898  time: 1.0450  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2850/7110]  eta: 1:11:44  lr: 0.000010  loss: 0.0386  time: 0.9718  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2900/7110]  eta: 1:10:53  lr: 0.000010  loss: 0.5545  time: 1.0433  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [2950/7110]  eta: 1:09:59  lr: 0.000010  loss: 0.1085  time: 0.9569  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3000/7110]  eta: 1:09:08  lr: 0.000010  loss: 0.1657  time: 0.9427  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3050/7110]  eta: 1:08:17  lr: 0.000010  loss: 0.1785  time: 0.9575  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3100/7110]  eta: 1:07:25  lr: 0.000010  loss: 0.3919  time: 1.0664  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3150/7110]  eta: 1:06:36  lr: 0.000010  loss: 0.1875  time: 0.9917  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3200/7110]  eta: 1:05:44  lr: 0.000010  loss: 0.7470  time: 1.0706  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3250/7110]  eta: 1:04:53  lr: 0.000010  loss: 0.1335  time: 1.0352  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3300/7110]  eta: 1:04:05  lr: 0.000010  loss: 0.0848  time: 1.0140  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3350/7110]  eta: 1:03:13  lr: 0.000010  loss: 0.1439  time: 1.0179  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3400/7110]  eta: 1:02:22  lr: 0.000010  loss: 0.1309  time: 0.9724  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3450/7110]  eta: 1:01:29  lr: 0.000010  loss: 0.1226  time: 0.9626  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3500/7110]  eta: 1:00:39  lr: 0.000010  loss: 0.0905  time: 1.0037  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3550/7110]  eta: 0:59:48  lr: 0.000010  loss: 0.5968  time: 1.0041  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3600/7110]  eta: 0:59:01  lr: 0.000010  loss: 0.0879  time: 1.1018  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3650/7110]  eta: 0:58:10  lr: 0.000010  loss: 0.1526  time: 1.0699  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3700/7110]  eta: 0:57:19  lr: 0.000010  loss: 0.1166  time: 1.0027  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3750/7110]  eta: 0:56:26  lr: 0.000010  loss: 0.1462  time: 0.9826  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3800/7110]  eta: 0:55:36  lr: 0.000010  loss: 0.0984  time: 0.9553  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3850/7110]  eta: 0:54:43  lr: 0.000010  loss: 0.0255  time: 0.9513  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3900/7110]  eta: 0:53:51  lr: 0.000010  loss: 0.1590  time: 0.9272  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [3950/7110]  eta: 0:53:01  lr: 0.000010  loss: 0.1141  time: 0.9966  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4000/7110]  eta: 0:52:10  lr: 0.000010  loss: 0.1195  time: 0.9883  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4050/7110]  eta: 0:51:18  lr: 0.000010  loss: 0.6031  time: 0.9953  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4100/7110]  eta: 0:50:28  lr: 0.000010  loss: 0.1514  time: 1.0661  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4150/7110]  eta: 0:49:37  lr: 0.000010  loss: 0.5879  time: 1.0064  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4200/7110]  eta: 0:48:46  lr: 0.000010  loss: 0.0992  time: 1.0068  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4250/7110]  eta: 0:47:56  lr: 0.000010  loss: 0.0585  time: 0.9811  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4300/7110]  eta: 0:47:04  lr: 0.000010  loss: 0.0748  time: 0.9647  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4350/7110]  eta: 0:46:14  lr: 0.000010  loss: 0.4558  time: 1.0673  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4400/7110]  eta: 0:45:23  lr: 0.000010  loss: 0.1625  time: 0.9694  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4450/7110]  eta: 0:44:32  lr: 0.000010  loss: 0.0979  time: 0.9620  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4500/7110]  eta: 0:43:41  lr: 0.000010  loss: 0.2879  time: 0.9818  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4550/7110]  eta: 0:42:52  lr: 0.000010  loss: 0.4817  time: 0.9965  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4600/7110]  eta: 0:42:01  lr: 0.000010  loss: 0.0120  time: 0.9828  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4650/7110]  eta: 0:41:10  lr: 0.000010  loss: 0.3613  time: 0.9805  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4700/7110]  eta: 0:40:20  lr: 0.000010  loss: 1.7316  time: 1.0590  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4750/7110]  eta: 0:39:30  lr: 0.000010  loss: 0.1029  time: 1.0010  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4800/7110]  eta: 0:38:40  lr: 0.000010  loss: 0.6560  time: 1.1092  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4850/7110]  eta: 0:37:51  lr: 0.000010  loss: 0.0565  time: 0.9882  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4900/7110]  eta: 0:37:00  lr: 0.000010  loss: 0.2815  time: 0.9871  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [4950/7110]  eta: 0:36:10  lr: 0.000010  loss: 0.2221  time: 1.0312  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5000/7110]  eta: 0:35:19  lr: 0.000010  loss: 0.2440  time: 1.0169  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5050/7110]  eta: 0:34:29  lr: 0.000010  loss: 0.3412  time: 1.0123  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5100/7110]  eta: 0:33:39  lr: 0.000010  loss: 0.1041  time: 1.0508  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5150/7110]  eta: 0:32:49  lr: 0.000010  loss: 0.1099  time: 1.0084  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5200/7110]  eta: 0:31:58  lr: 0.000010  loss: 0.0818  time: 0.9513  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5250/7110]  eta: 0:31:08  lr: 0.000010  loss: 0.3038  time: 1.0134  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5300/7110]  eta: 0:30:17  lr: 0.000010  loss: 0.3884  time: 0.9501  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5350/7110]  eta: 0:29:26  lr: 0.000010  loss: 0.1192  time: 0.9599  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5400/7110]  eta: 0:28:36  lr: 0.000010  loss: 0.1920  time: 0.9829  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5450/7110]  eta: 0:27:46  lr: 0.000010  loss: 0.1500  time: 1.0796  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5500/7110]  eta: 0:26:56  lr: 0.000010  loss: 0.3105  time: 0.9960  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5550/7110]  eta: 0:26:05  lr: 0.000010  loss: 0.1536  time: 1.0010  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5600/7110]  eta: 0:25:15  lr: 0.000010  loss: 0.0601  time: 0.9489  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5650/7110]  eta: 0:24:24  lr: 0.000010  loss: 0.0164  time: 0.9942  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.1994  time: 1.0362  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.2432  time: 0.9690  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.0760  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.0504  time: 1.0289  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.1713  time: 1.0972  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.3188  time: 1.0123  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.1615  time: 0.9644  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.4130  time: 0.9913  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.3469  time: 0.9746  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.2869  time: 0.9942  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.2635  time: 0.9980  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.0703  time: 0.9828  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.0466  time: 0.9098  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.1242  time: 0.9800  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.2739  time: 0.9961  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.4620  time: 0.9739  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.1225  time: 1.0164  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.1081  time: 1.0016  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.4332  time: 0.9551  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1225  time: 0.9971  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.0449  time: 1.0033  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.3779  time: 1.0040  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.1772  time: 0.9704  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.3418  time: 0.9406  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.0590  time: 1.0349  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.3140  time: 1.0347  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.1000  time: 0.9849  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.3925  time: 0.9708  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.8518  time: 1.0104  data: 0.0000  max mem: 66110
Train: data epoch: [30]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0845  time: 1.1099  data: 0.0000  max mem: 66110
Train: data epoch: [30] Total time: 1:58:50 (1.0028 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:59:20    time: 23.0198  data: 21.7602  max mem: 66110
Evaluation  [  10/1093]  eta: 1:00:57    time: 3.3775  data: 1.9793  max mem: 66110
Evaluation  [  20/1093]  eta: 0:42:55    time: 1.3696  data: 0.0012  max mem: 66110
Evaluation  [  30/1093]  eta: 0:35:25    time: 1.2414  data: 0.0012  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:22    time: 1.2619  data: 0.0012  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:37    time: 1.3945  data: 0.0011  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:25    time: 1.2540  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:37    time: 1.2598  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:45    time: 1.3816  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:05    time: 1.3462  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:25:36    time: 1.3941  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:13    time: 1.4462  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:37    time: 1.3775  data: 0.0011  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:22    time: 1.4040  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:23:50    time: 1.3937  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:24    time: 1.2954  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:04    time: 1.3633  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:35    time: 1.3090  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:17    time: 1.3125  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:04    time: 1.4500  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:42    time: 1.3983  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:38    time: 1.5114  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:14    time: 1.4789  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:52    time: 1.2504  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:31    time: 1.2648  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:18    time: 1.3672  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:11    time: 1.5792  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:49    time: 1.4451  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:30    time: 1.2451  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:22    time: 1.4787  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:00    time: 1.4150  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:40    time: 1.1893  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:28    time: 1.3667  data: 0.0011  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:10    time: 1.4079  data: 0.0012  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:58    time: 1.4027  data: 0.0011  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:43    time: 1.4614  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:24    time: 1.3092  data: 0.0011  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:07    time: 1.2466  data: 0.0012  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:56    time: 1.4301  data: 0.0011  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:40    time: 1.4520  data: 0.0013  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:26    time: 1.4025  data: 0.0013  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:09    time: 1.3613  data: 0.0011  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:56    time: 1.3541  data: 0.0012  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:40    time: 1.4027  data: 0.0012  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:21    time: 1.2045  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:06    time: 1.2225  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:51    time: 1.3607  data: 0.0011  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:35    time: 1.3116  data: 0.0013  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:22    time: 1.3719  data: 0.0013  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:09    time: 1.4794  data: 0.0011  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:58    time: 1.5577  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:45    time: 1.5686  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:31    time: 1.4883  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:17    time: 1.4543  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:03    time: 1.4387  data: 0.0011  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:48    time: 1.3975  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:31    time: 1.2397  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:17    time: 1.2369  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:02    time: 1.3639  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:46    time: 1.2791  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:34    time: 1.3944  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:20    time: 1.5271  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:06    time: 1.4104  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:52    time: 1.3873  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:37    time: 1.3522  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:23    time: 1.3897  data: 0.0011  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:09    time: 1.4012  data: 0.0011  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:55    time: 1.3965  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:41    time: 1.4218  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:27    time: 1.4481  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:13    time: 1.4207  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:57    time: 1.2306  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:42    time: 1.1798  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:26    time: 1.1369  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:12    time: 1.1670  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:58    time: 1.3319  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:44    time: 1.3756  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:30    time: 1.3842  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:16    time: 1.4612  data: 0.0012  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:01    time: 1.3134  data: 0.0012  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:48    time: 1.3821  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:35    time: 1.5612  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:21    time: 1.4661  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:08    time: 1.5140  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:53    time: 1.4087  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:39    time: 1.3102  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:25    time: 1.3338  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:11    time: 1.3818  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:58    time: 1.5246  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:43    time: 1.3528  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:29    time: 1.2698  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:15    time: 1.4432  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:01    time: 1.4621  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.3856  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3334  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.3448  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.4009  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4414  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3570  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.3749  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4855  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4580  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2785  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1330  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2514  data: 0.0009  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3299  data: 0.0009  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3797  data: 0.0009  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4668  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3817  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3306  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3040  data: 0.0439  max mem: 66110
Evaluation Total time: 0:25:21 (1.3916 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_30_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [31]  [   0/7110]  eta: 2 days, 5:55:38  lr: 0.000010  loss: 0.1825  time: 27.3050  data: 0.0001  max mem: 66110
Train: data epoch: [31]  [  50/7110]  eta: 2:55:57  lr: 0.000010  loss: 1.2960  time: 0.9879  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 100/7110]  eta: 2:24:41  lr: 0.000010  loss: 0.0756  time: 0.9747  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 150/7110]  eta: 2:14:52  lr: 0.000010  loss: 0.0996  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 200/7110]  eta: 2:08:25  lr: 0.000010  loss: 0.2252  time: 0.9632  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 250/7110]  eta: 2:04:30  lr: 0.000010  loss: 0.4711  time: 0.9794  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 300/7110]  eta: 2:01:12  lr: 0.000010  loss: 0.3759  time: 0.9482  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 350/7110]  eta: 1:59:15  lr: 0.000010  loss: 0.1285  time: 1.0426  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 400/7110]  eta: 1:57:29  lr: 0.000010  loss: 0.2697  time: 1.0279  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 450/7110]  eta: 1:56:00  lr: 0.000010  loss: 0.1200  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 500/7110]  eta: 1:54:37  lr: 0.000010  loss: 0.2790  time: 1.0059  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 550/7110]  eta: 1:53:03  lr: 0.000010  loss: 0.4337  time: 0.9615  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 600/7110]  eta: 1:51:54  lr: 0.000010  loss: 1.0599  time: 1.0308  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 650/7110]  eta: 1:50:48  lr: 0.000010  loss: 0.1129  time: 0.9714  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 700/7110]  eta: 1:50:04  lr: 0.000010  loss: 0.0914  time: 1.0449  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 750/7110]  eta: 1:49:02  lr: 0.000010  loss: 0.0393  time: 0.9861  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 800/7110]  eta: 1:48:05  lr: 0.000010  loss: 0.2803  time: 1.0637  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 850/7110]  eta: 1:47:16  lr: 0.000010  loss: 0.0204  time: 1.0356  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 900/7110]  eta: 1:46:17  lr: 0.000010  loss: 0.4381  time: 1.0372  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [ 950/7110]  eta: 1:45:29  lr: 0.000010  loss: 0.1592  time: 1.0126  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1000/7110]  eta: 1:44:25  lr: 0.000010  loss: 0.2801  time: 0.9602  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1050/7110]  eta: 1:43:19  lr: 0.000010  loss: 0.3176  time: 0.9827  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1100/7110]  eta: 1:42:23  lr: 0.000010  loss: 0.3692  time: 1.0100  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1150/7110]  eta: 1:41:30  lr: 0.000010  loss: 0.0478  time: 1.0231  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1200/7110]  eta: 1:40:33  lr: 0.000010  loss: 0.1317  time: 0.9954  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1250/7110]  eta: 1:39:30  lr: 0.000010  loss: 0.1667  time: 0.9199  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1300/7110]  eta: 1:38:26  lr: 0.000010  loss: 0.2982  time: 0.9515  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1350/7110]  eta: 1:37:41  lr: 0.000010  loss: 0.1276  time: 1.0648  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1400/7110]  eta: 1:36:49  lr: 0.000010  loss: 0.2202  time: 0.9947  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1450/7110]  eta: 1:35:58  lr: 0.000010  loss: 0.2883  time: 0.9927  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1500/7110]  eta: 1:34:59  lr: 0.000010  loss: 0.4097  time: 0.9981  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1550/7110]  eta: 1:34:05  lr: 0.000010  loss: 0.6400  time: 0.9978  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1600/7110]  eta: 1:33:11  lr: 0.000010  loss: 0.9194  time: 1.0029  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1650/7110]  eta: 1:32:17  lr: 0.000010  loss: 0.0374  time: 1.0301  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1700/7110]  eta: 1:31:31  lr: 0.000010  loss: 0.3988  time: 1.0452  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1750/7110]  eta: 1:30:39  lr: 0.000010  loss: 0.3021  time: 0.9841  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1800/7110]  eta: 1:29:41  lr: 0.000010  loss: 0.6769  time: 0.9839  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1850/7110]  eta: 1:28:43  lr: 0.000010  loss: 0.0611  time: 0.9526  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1900/7110]  eta: 1:27:46  lr: 0.000010  loss: 0.7342  time: 0.9643  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [1950/7110]  eta: 1:26:54  lr: 0.000010  loss: 0.1465  time: 0.9598  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2000/7110]  eta: 1:26:04  lr: 0.000010  loss: 0.0914  time: 0.9767  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2050/7110]  eta: 1:25:08  lr: 0.000010  loss: 0.2263  time: 0.9496  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2100/7110]  eta: 1:24:21  lr: 0.000010  loss: 0.1263  time: 1.0080  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2150/7110]  eta: 1:23:30  lr: 0.000010  loss: 0.1293  time: 1.0001  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2200/7110]  eta: 1:22:41  lr: 0.000010  loss: 0.2941  time: 0.9894  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2250/7110]  eta: 1:21:45  lr: 0.000010  loss: 0.0451  time: 0.9226  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2300/7110]  eta: 1:20:55  lr: 0.000010  loss: 0.4997  time: 0.9851  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2350/7110]  eta: 1:20:03  lr: 0.000010  loss: 0.0148  time: 0.9876  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2400/7110]  eta: 1:19:12  lr: 0.000010  loss: 0.7999  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2450/7110]  eta: 1:18:22  lr: 0.000010  loss: 0.1141  time: 0.9891  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2500/7110]  eta: 1:17:30  lr: 0.000010  loss: 0.5752  time: 0.9736  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2550/7110]  eta: 1:16:38  lr: 0.000010  loss: 0.8282  time: 1.0017  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2600/7110]  eta: 1:15:45  lr: 0.000010  loss: 0.0853  time: 0.9537  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2650/7110]  eta: 1:14:56  lr: 0.000010  loss: 0.3153  time: 1.0495  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2700/7110]  eta: 1:14:05  lr: 0.000010  loss: 0.2100  time: 0.9956  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2750/7110]  eta: 1:13:13  lr: 0.000010  loss: 0.0696  time: 0.9946  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2800/7110]  eta: 1:12:20  lr: 0.000010  loss: 0.7342  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2850/7110]  eta: 1:11:27  lr: 0.000010  loss: 0.4085  time: 0.9770  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2900/7110]  eta: 1:10:40  lr: 0.000010  loss: 0.0770  time: 1.0547  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [2950/7110]  eta: 1:09:49  lr: 0.000010  loss: 0.4192  time: 1.0043  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3000/7110]  eta: 1:08:58  lr: 0.000010  loss: 0.2555  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3050/7110]  eta: 1:08:07  lr: 0.000010  loss: 0.1799  time: 0.9665  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3100/7110]  eta: 1:07:16  lr: 0.000010  loss: 0.8555  time: 0.9668  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3150/7110]  eta: 1:06:29  lr: 0.000010  loss: 0.1188  time: 1.0368  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3200/7110]  eta: 1:05:37  lr: 0.000010  loss: 0.0533  time: 0.9798  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3250/7110]  eta: 1:04:45  lr: 0.000010  loss: 0.3808  time: 0.9784  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3300/7110]  eta: 1:03:52  lr: 0.000010  loss: 0.0817  time: 0.9584  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3350/7110]  eta: 1:03:02  lr: 0.000010  loss: 0.1070  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3400/7110]  eta: 1:02:11  lr: 0.000010  loss: 0.3306  time: 1.0421  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3450/7110]  eta: 1:01:21  lr: 0.000010  loss: 0.3904  time: 0.9920  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3500/7110]  eta: 1:00:31  lr: 0.000010  loss: 0.1497  time: 1.0113  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3550/7110]  eta: 0:59:41  lr: 0.000010  loss: 0.4890  time: 1.0346  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3600/7110]  eta: 0:58:50  lr: 0.000010  loss: 0.0977  time: 0.9509  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3650/7110]  eta: 0:57:59  lr: 0.000010  loss: 0.0790  time: 1.0166  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3700/7110]  eta: 0:57:08  lr: 0.000010  loss: 0.1925  time: 1.0274  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3750/7110]  eta: 0:56:19  lr: 0.000010  loss: 0.7841  time: 1.0420  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3800/7110]  eta: 0:55:27  lr: 0.000010  loss: 0.1062  time: 0.9797  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3850/7110]  eta: 0:54:37  lr: 0.000010  loss: 0.3201  time: 0.9943  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3900/7110]  eta: 0:53:48  lr: 0.000010  loss: 0.5631  time: 1.0122  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [3950/7110]  eta: 0:52:55  lr: 0.000010  loss: 0.1908  time: 1.0101  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4000/7110]  eta: 0:52:07  lr: 0.000010  loss: 0.1299  time: 1.0608  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4050/7110]  eta: 0:51:15  lr: 0.000010  loss: 0.0371  time: 0.9945  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4100/7110]  eta: 0:50:24  lr: 0.000010  loss: 0.4093  time: 1.0165  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4150/7110]  eta: 0:49:33  lr: 0.000010  loss: 0.0813  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4200/7110]  eta: 0:48:43  lr: 0.000010  loss: 0.1290  time: 1.0550  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4250/7110]  eta: 0:47:52  lr: 0.000010  loss: 0.3430  time: 1.0016  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4300/7110]  eta: 0:47:03  lr: 0.000010  loss: 0.0991  time: 1.0073  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4350/7110]  eta: 0:46:13  lr: 0.000010  loss: 0.1018  time: 1.0271  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4400/7110]  eta: 0:45:22  lr: 0.000010  loss: 0.0700  time: 0.9924  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4450/7110]  eta: 0:44:32  lr: 0.000010  loss: 0.0901  time: 0.9721  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4500/7110]  eta: 0:43:41  lr: 0.000010  loss: 0.1926  time: 1.0197  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4550/7110]  eta: 0:42:51  lr: 0.000010  loss: 0.0546  time: 1.0664  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4600/7110]  eta: 0:42:01  lr: 0.000010  loss: 1.4910  time: 0.9848  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4650/7110]  eta: 0:41:12  lr: 0.000010  loss: 0.1465  time: 1.0680  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4700/7110]  eta: 0:40:21  lr: 0.000010  loss: 0.3242  time: 1.0637  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4750/7110]  eta: 0:39:31  lr: 0.000010  loss: 0.2512  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4800/7110]  eta: 0:38:41  lr: 0.000010  loss: 0.5438  time: 1.0189  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4850/7110]  eta: 0:37:49  lr: 0.000010  loss: 0.4241  time: 0.9757  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4900/7110]  eta: 0:36:59  lr: 0.000010  loss: 0.2922  time: 1.0111  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [4950/7110]  eta: 0:36:08  lr: 0.000010  loss: 0.4374  time: 0.9614  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5000/7110]  eta: 0:35:18  lr: 0.000010  loss: 0.2711  time: 1.0015  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5050/7110]  eta: 0:34:27  lr: 0.000010  loss: 0.1072  time: 0.9686  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5100/7110]  eta: 0:33:37  lr: 0.000010  loss: 0.0679  time: 0.9847  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5150/7110]  eta: 0:32:46  lr: 0.000010  loss: 0.8752  time: 0.9684  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5200/7110]  eta: 0:31:56  lr: 0.000010  loss: 0.0202  time: 0.9941  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5250/7110]  eta: 0:31:06  lr: 0.000010  loss: 0.3980  time: 1.0161  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5300/7110]  eta: 0:30:16  lr: 0.000010  loss: 0.1741  time: 1.0214  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5350/7110]  eta: 0:29:26  lr: 0.000010  loss: 0.1359  time: 0.9810  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5400/7110]  eta: 0:28:36  lr: 0.000010  loss: 0.2587  time: 1.0295  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5450/7110]  eta: 0:27:45  lr: 0.000010  loss: 0.0274  time: 0.9867  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5500/7110]  eta: 0:26:54  lr: 0.000010  loss: 0.1791  time: 0.9716  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5550/7110]  eta: 0:26:04  lr: 0.000010  loss: 0.0623  time: 0.9804  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5600/7110]  eta: 0:25:14  lr: 0.000010  loss: 0.5907  time: 1.0690  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5650/7110]  eta: 0:24:24  lr: 0.000010  loss: 0.3412  time: 0.9500  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.1435  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.4689  time: 0.9631  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.3430  time: 1.0568  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.4398  time: 1.0263  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.1647  time: 0.9949  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.0445  time: 0.9404  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.0330  time: 1.0520  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.1129  time: 0.9795  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.0400  time: 0.9686  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.3497  time: 1.0564  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.2993  time: 1.0206  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.0372  time: 0.9802  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.0720  time: 1.0681  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.2603  time: 1.0780  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.4955  time: 0.9801  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.3692  time: 1.0011  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.3580  time: 0.9270  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.1412  time: 1.0239  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.2245  time: 1.0195  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.7557  time: 1.0024  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.1097  time: 1.0541  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.2563  time: 1.0416  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.1367  time: 1.0348  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.3946  time: 1.0128  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.2798  time: 0.9556  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.2670  time: 1.0459  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.1911  time: 1.0004  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1895  time: 0.9612  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1127  time: 1.0297  data: 0.0000  max mem: 66110
Train: data epoch: [31]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 1.4771  time: 1.1201  data: 0.0000  max mem: 66110
Train: data epoch: [31] Total time: 1:58:48 (1.0027 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:25:03    time: 21.1378  data: 19.9034  max mem: 66110
Evaluation  [  10/1093]  eta: 0:58:00    time: 3.2137  data: 1.8104  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:53    time: 1.4023  data: 0.0011  max mem: 66110
Evaluation  [  30/1093]  eta: 0:35:05    time: 1.3026  data: 0.0011  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:09    time: 1.2963  data: 0.0011  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:29    time: 1.4039  data: 0.0011  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:20    time: 1.2666  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:39    time: 1.2850  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:27:02    time: 1.4648  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:33    time: 1.4689  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:11    time: 1.5065  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:43    time: 1.4857  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:06    time: 1.3725  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:51    time: 1.4286  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:11    time: 1.3728  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:40    time: 1.2296  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:17    time: 1.3176  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:48    time: 1.3036  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:28    time: 1.3114  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:16    time: 1.4636  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:54    time: 1.4225  data: 0.0009  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:49    time: 1.5095  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:26    time: 1.4888  data: 0.0011  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:06    time: 1.3051  data: 0.0011  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:48    time: 1.3629  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:33    time: 1.4128  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:24    time: 1.5404  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:02    time: 1.4290  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:43    time: 1.2747  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:35    time: 1.5101  data: 0.0011  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:13    time: 1.4313  data: 0.0011  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:54    time: 1.2179  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:37    time: 1.3119  data: 0.0011  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:19    time: 1.3264  data: 0.0011  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:06    time: 1.4089  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:52    time: 1.4725  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:31    time: 1.2917  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:14    time: 1.2303  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:03    time: 1.4375  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:47    time: 1.4679  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:34    time: 1.4201  data: 0.0011  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:19    time: 1.4479  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:05    time: 1.4402  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:49    time: 1.4086  data: 0.0011  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:32    time: 1.2782  data: 0.0011  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:16    time: 1.2814  data: 0.0012  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:02    time: 1.3981  data: 0.0011  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:46    time: 1.3662  data: 0.0011  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:33    time: 1.3773  data: 0.0011  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:19    time: 1.4728  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:07    time: 1.5508  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:54    time: 1.5625  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:39    time: 1.4738  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:25    time: 1.4432  data: 0.0011  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:11    time: 1.4425  data: 0.0011  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:56    time: 1.4107  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:39    time: 1.2389  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:24    time: 1.2398  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:10    time: 1.3856  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:53    time: 1.2767  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:41    time: 1.3884  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:27    time: 1.5367  data: 0.0011  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:12    time: 1.4229  data: 0.0011  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:58    time: 1.3980  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:43    time: 1.3799  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:29    time: 1.4211  data: 0.0011  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:15    time: 1.4076  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:01    time: 1.3957  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:46    time: 1.4482  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:33    time: 1.4777  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:18    time: 1.4450  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:03    time: 1.2495  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:47    time: 1.1846  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:32    time: 1.1723  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:17    time: 1.2120  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:03    time: 1.3649  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:49    time: 1.4035  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:35    time: 1.4213  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:21    time: 1.4851  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:06    time: 1.3032  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:53    time: 1.3706  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:39    time: 1.5487  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:25    time: 1.4574  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:11    time: 1.5160  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:57    time: 1.4484  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:43    time: 1.3791  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:28    time: 1.3693  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:14    time: 1.3193  data: 0.0011  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:00    time: 1.4565  data: 0.0011  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:46    time: 1.3662  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:31    time: 1.2658  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:17    time: 1.4130  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:03    time: 1.4497  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:49    time: 1.4141  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:35    time: 1.3862  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:21    time: 1.3545  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.3735  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4229  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3607  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3841  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4753  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4502  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3185  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1780  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2645  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3562  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3965  data: 0.0012  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4584  data: 0.0012  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3882  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3388  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3158  data: 0.0479  max mem: 66110
Evaluation Total time: 0:25:34 (1.4039 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_31_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [32]  [   0/7110]  eta: 2 days, 5:32:05  lr: 0.000010  loss: 0.0621  time: 27.1062  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [  50/7110]  eta: 3:02:21  lr: 0.000010  loss: 0.2038  time: 1.0642  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 100/7110]  eta: 2:28:02  lr: 0.000010  loss: 0.1423  time: 0.9644  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 150/7110]  eta: 2:18:10  lr: 0.000010  loss: 0.0825  time: 1.0485  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 200/7110]  eta: 2:11:18  lr: 0.000010  loss: 0.3220  time: 0.9830  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 250/7110]  eta: 2:07:50  lr: 0.000010  loss: 0.3614  time: 1.0165  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 300/7110]  eta: 2:04:00  lr: 0.000010  loss: 0.2568  time: 0.9442  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 350/7110]  eta: 2:01:02  lr: 0.000010  loss: 0.1067  time: 0.9407  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 400/7110]  eta: 1:59:06  lr: 0.000010  loss: 1.0729  time: 1.0311  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 450/7110]  eta: 1:57:24  lr: 0.000010  loss: 0.3309  time: 0.9824  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 500/7110]  eta: 1:55:38  lr: 0.000010  loss: 0.3199  time: 0.9489  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 550/7110]  eta: 1:54:02  lr: 0.000010  loss: 0.1072  time: 0.9564  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 600/7110]  eta: 1:53:15  lr: 0.000010  loss: 0.4741  time: 1.0569  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 650/7110]  eta: 1:51:51  lr: 0.000010  loss: 0.0894  time: 1.0410  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 700/7110]  eta: 1:50:43  lr: 0.000010  loss: 0.5156  time: 1.0311  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 750/7110]  eta: 1:49:26  lr: 0.000010  loss: 0.3884  time: 0.9998  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 800/7110]  eta: 1:48:13  lr: 0.000010  loss: 0.0798  time: 0.9330  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 850/7110]  eta: 1:47:16  lr: 0.000010  loss: 0.0261  time: 0.9755  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 900/7110]  eta: 1:46:17  lr: 0.000010  loss: 0.2283  time: 1.0592  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [ 950/7110]  eta: 1:45:14  lr: 0.000010  loss: 0.0580  time: 0.9790  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1000/7110]  eta: 1:44:18  lr: 0.000010  loss: 0.2977  time: 1.0271  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1050/7110]  eta: 1:43:15  lr: 0.000010  loss: 0.0932  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1100/7110]  eta: 1:42:26  lr: 0.000010  loss: 0.2916  time: 1.0187  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1150/7110]  eta: 1:41:24  lr: 0.000010  loss: 0.2156  time: 1.0128  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1200/7110]  eta: 1:40:36  lr: 0.000010  loss: 0.3608  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1250/7110]  eta: 1:39:39  lr: 0.000010  loss: 0.0781  time: 0.9983  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1300/7110]  eta: 1:38:43  lr: 0.000010  loss: 0.0956  time: 0.9607  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1350/7110]  eta: 1:37:34  lr: 0.000010  loss: 1.1229  time: 0.9402  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1400/7110]  eta: 1:36:41  lr: 0.000010  loss: 0.3129  time: 0.9893  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1450/7110]  eta: 1:35:50  lr: 0.000010  loss: 0.0244  time: 1.0433  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1500/7110]  eta: 1:34:59  lr: 0.000010  loss: 0.1843  time: 1.0945  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1550/7110]  eta: 1:33:59  lr: 0.000010  loss: 0.3753  time: 0.9480  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1600/7110]  eta: 1:33:08  lr: 0.000010  loss: 0.0558  time: 1.0019  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1650/7110]  eta: 1:32:14  lr: 0.000010  loss: 0.1987  time: 1.0090  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1700/7110]  eta: 1:31:25  lr: 0.000010  loss: 0.4562  time: 1.0696  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1750/7110]  eta: 1:30:34  lr: 0.000010  loss: 0.1327  time: 1.0634  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1800/7110]  eta: 1:29:43  lr: 0.000010  loss: 0.6965  time: 1.0520  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1850/7110]  eta: 1:28:52  lr: 0.000010  loss: 0.2107  time: 1.0632  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1900/7110]  eta: 1:27:56  lr: 0.000010  loss: 0.5995  time: 0.9762  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [1950/7110]  eta: 1:26:58  lr: 0.000010  loss: 0.6204  time: 0.9658  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2000/7110]  eta: 1:26:03  lr: 0.000010  loss: 0.6035  time: 0.9695  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2050/7110]  eta: 1:25:13  lr: 0.000010  loss: 0.5692  time: 0.9729  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2100/7110]  eta: 1:24:19  lr: 0.000010  loss: 0.3635  time: 0.9812  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2150/7110]  eta: 1:23:24  lr: 0.000010  loss: 0.1978  time: 0.9647  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2200/7110]  eta: 1:22:34  lr: 0.000010  loss: 0.2291  time: 0.9737  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2250/7110]  eta: 1:21:41  lr: 0.000010  loss: 0.2014  time: 0.9620  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2300/7110]  eta: 1:20:51  lr: 0.000010  loss: 0.4141  time: 1.0125  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2350/7110]  eta: 1:20:04  lr: 0.000010  loss: 0.1337  time: 1.0504  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2400/7110]  eta: 1:19:15  lr: 0.000010  loss: 0.0654  time: 1.0296  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2450/7110]  eta: 1:18:24  lr: 0.000010  loss: 1.3768  time: 0.9985  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2500/7110]  eta: 1:17:31  lr: 0.000010  loss: 0.4330  time: 0.9737  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2550/7110]  eta: 1:16:40  lr: 0.000010  loss: 0.0791  time: 0.9627  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2600/7110]  eta: 1:15:46  lr: 0.000010  loss: 0.2524  time: 0.9730  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2650/7110]  eta: 1:14:52  lr: 0.000010  loss: 0.3262  time: 0.9415  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2700/7110]  eta: 1:14:04  lr: 0.000010  loss: 0.1008  time: 1.0411  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2750/7110]  eta: 1:13:14  lr: 0.000010  loss: 0.3683  time: 1.0658  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2800/7110]  eta: 1:12:22  lr: 0.000010  loss: 0.2730  time: 0.9741  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2850/7110]  eta: 1:11:30  lr: 0.000010  loss: 0.0400  time: 0.9403  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2900/7110]  eta: 1:10:40  lr: 0.000010  loss: 0.3760  time: 0.9978  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [2950/7110]  eta: 1:09:49  lr: 0.000010  loss: 0.3104  time: 0.9759  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3000/7110]  eta: 1:09:00  lr: 0.000010  loss: 0.5183  time: 0.9770  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3050/7110]  eta: 1:08:08  lr: 0.000010  loss: 0.2299  time: 0.9012  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3100/7110]  eta: 1:07:15  lr: 0.000010  loss: 0.5047  time: 0.9961  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3150/7110]  eta: 1:06:27  lr: 0.000010  loss: 0.1317  time: 1.0101  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3200/7110]  eta: 1:05:36  lr: 0.000010  loss: 0.0622  time: 0.9700  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3250/7110]  eta: 1:04:44  lr: 0.000010  loss: 0.2941  time: 0.9760  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3300/7110]  eta: 1:03:53  lr: 0.000010  loss: 0.0282  time: 1.0311  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3350/7110]  eta: 1:03:04  lr: 0.000010  loss: 0.3753  time: 1.0458  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3400/7110]  eta: 1:02:12  lr: 0.000010  loss: 0.2774  time: 0.9886  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3450/7110]  eta: 1:01:22  lr: 0.000010  loss: 0.0269  time: 1.0366  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3500/7110]  eta: 1:00:32  lr: 0.000010  loss: 0.1475  time: 0.9983  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3550/7110]  eta: 0:59:40  lr: 0.000010  loss: 0.3677  time: 1.0040  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3600/7110]  eta: 0:58:50  lr: 0.000010  loss: 0.2034  time: 1.0084  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3650/7110]  eta: 0:57:59  lr: 0.000010  loss: 0.1310  time: 1.0566  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3700/7110]  eta: 0:57:07  lr: 0.000010  loss: 0.1381  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3750/7110]  eta: 0:56:16  lr: 0.000010  loss: 0.1214  time: 0.9897  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3800/7110]  eta: 0:55:26  lr: 0.000010  loss: 0.1256  time: 1.0027  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3850/7110]  eta: 0:54:34  lr: 0.000010  loss: 0.0780  time: 0.9567  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3900/7110]  eta: 0:53:44  lr: 0.000010  loss: 0.3359  time: 1.0169  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [3950/7110]  eta: 0:52:52  lr: 0.000010  loss: 0.2200  time: 0.9283  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4000/7110]  eta: 0:52:03  lr: 0.000010  loss: 0.3928  time: 1.0508  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4050/7110]  eta: 0:51:15  lr: 0.000010  loss: 0.9189  time: 1.0150  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4100/7110]  eta: 0:50:23  lr: 0.000010  loss: 0.1158  time: 0.9735  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4150/7110]  eta: 0:49:34  lr: 0.000010  loss: 0.2211  time: 1.0292  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4200/7110]  eta: 0:48:42  lr: 0.000010  loss: 0.2777  time: 0.9683  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4250/7110]  eta: 0:47:52  lr: 0.000010  loss: 0.4251  time: 1.0452  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4300/7110]  eta: 0:47:03  lr: 0.000010  loss: 0.3659  time: 1.0276  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4350/7110]  eta: 0:46:12  lr: 0.000010  loss: 0.1230  time: 0.9108  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4400/7110]  eta: 0:45:22  lr: 0.000010  loss: 0.5042  time: 0.9887  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4450/7110]  eta: 0:44:30  lr: 0.000010  loss: 0.1452  time: 0.9732  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4500/7110]  eta: 0:43:39  lr: 0.000010  loss: 0.7525  time: 0.9723  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4550/7110]  eta: 0:42:48  lr: 0.000010  loss: 0.3015  time: 0.9872  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4600/7110]  eta: 0:41:58  lr: 0.000010  loss: 0.3747  time: 0.9346  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4650/7110]  eta: 0:41:08  lr: 0.000010  loss: 0.1204  time: 0.9695  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4700/7110]  eta: 0:40:17  lr: 0.000010  loss: 0.0724  time: 0.9846  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4750/7110]  eta: 0:39:27  lr: 0.000010  loss: 0.6421  time: 0.9527  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4800/7110]  eta: 0:38:37  lr: 0.000010  loss: 0.4631  time: 0.9844  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4850/7110]  eta: 0:37:46  lr: 0.000010  loss: 0.3683  time: 0.9599  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4900/7110]  eta: 0:36:57  lr: 0.000010  loss: 0.2530  time: 1.0079  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [4950/7110]  eta: 0:36:07  lr: 0.000010  loss: 0.6115  time: 0.9628  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5000/7110]  eta: 0:35:17  lr: 0.000010  loss: 0.3242  time: 1.0362  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5050/7110]  eta: 0:34:27  lr: 0.000010  loss: 0.5105  time: 1.0171  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5100/7110]  eta: 0:33:36  lr: 0.000010  loss: 0.1707  time: 0.9575  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5150/7110]  eta: 0:32:45  lr: 0.000010  loss: 0.0328  time: 1.0091  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5200/7110]  eta: 0:31:56  lr: 0.000010  loss: 0.1102  time: 1.0110  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5250/7110]  eta: 0:31:05  lr: 0.000010  loss: 0.0767  time: 1.0035  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5300/7110]  eta: 0:30:15  lr: 0.000010  loss: 0.4372  time: 0.9311  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5350/7110]  eta: 0:29:25  lr: 0.000010  loss: 0.4624  time: 1.0353  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5400/7110]  eta: 0:28:34  lr: 0.000010  loss: 0.1872  time: 0.9181  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5450/7110]  eta: 0:27:44  lr: 0.000010  loss: 0.3937  time: 0.9998  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5500/7110]  eta: 0:26:54  lr: 0.000010  loss: 0.0456  time: 0.9691  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5550/7110]  eta: 0:26:04  lr: 0.000010  loss: 0.1096  time: 1.0301  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5600/7110]  eta: 0:25:14  lr: 0.000010  loss: 0.0784  time: 0.9857  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5650/7110]  eta: 0:24:23  lr: 0.000010  loss: 0.0624  time: 1.0362  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.0093  time: 1.0552  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.0597  time: 0.9872  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5800/7110]  eta: 0:21:53  lr: 0.000010  loss: 0.1028  time: 1.0016  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.1235  time: 1.0138  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.0360  time: 1.0170  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.1317  time: 0.9897  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.1055  time: 0.9987  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.2512  time: 1.0431  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.1960  time: 0.9735  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.1122  time: 1.0716  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.6459  time: 0.9575  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.0188  time: 0.9405  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.1251  time: 1.0713  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.0726  time: 0.9974  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.0469  time: 0.9466  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.8150  time: 0.9637  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.0642  time: 0.9975  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.1249  time: 0.9997  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.0933  time: 1.0810  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.2643  time: 0.9923  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.0482  time: 1.0058  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.0492  time: 0.9760  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.4229  time: 0.9852  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.6110  time: 0.9888  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.4425  time: 0.9779  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.3193  time: 0.9580  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.4082  time: 0.9866  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.0683  time: 0.9840  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.3213  time: 0.9905  data: 0.0000  max mem: 66110
Train: data epoch: [32]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.1499  time: 1.0864  data: 0.0000  max mem: 66110
Train: data epoch: [32] Total time: 1:58:48 (1.0025 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:18:31    time: 20.7787  data: 19.5314  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:18    time: 3.1749  data: 1.7765  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:26    time: 1.3946  data: 0.0011  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:53    time: 1.3066  data: 0.0011  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:08    time: 1.3210  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:29    time: 1.4196  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:18    time: 1.2611  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:42    time: 1.2953  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:27:08    time: 1.4942  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:37    time: 1.4802  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:21    time: 1.5345  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:55    time: 1.5317  data: 0.0011  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:15    time: 1.3800  data: 0.0011  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:58    time: 1.4149  data: 0.0011  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:23    time: 1.4022  data: 0.0011  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:54    time: 1.2882  data: 0.0011  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:29    time: 1.3330  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:58    time: 1.2884  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:40    time: 1.3213  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:27    time: 1.4785  data: 0.0011  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:04    time: 1.4197  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:59    time: 1.5106  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:36    time: 1.5087  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:13    time: 1.2795  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:54    time: 1.3115  data: 0.0009  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:39    time: 1.4089  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:31    time: 1.5756  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:08    time: 1.4474  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:48    time: 1.2379  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:39    time: 1.4803  data: 0.0011  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:17    time: 1.4347  data: 0.0011  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:59    time: 1.2479  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:42    time: 1.3451  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:24    time: 1.3305  data: 0.0009  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:11    time: 1.3945  data: 0.0009  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:56    time: 1.4645  data: 0.0009  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:36    time: 1.2978  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:19    time: 1.2418  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:06    time: 1.4230  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:51    time: 1.4676  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:38    time: 1.4557  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:23    time: 1.4772  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:09    time: 1.4490  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:54    time: 1.4104  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:35    time: 1.2471  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:20    time: 1.2509  data: 0.0011  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:06    time: 1.4104  data: 0.0011  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:49    time: 1.3743  data: 0.0011  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:36    time: 1.3877  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:23    time: 1.5062  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:11    time: 1.5786  data: 0.0011  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:58    time: 1.5862  data: 0.0011  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:44    time: 1.5084  data: 0.0011  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:30    time: 1.4703  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:15    time: 1.4651  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:01    time: 1.4245  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:43    time: 1.2333  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:28    time: 1.2295  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:13    time: 1.3929  data: 0.0011  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:56    time: 1.2503  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:44    time: 1.3579  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:29    time: 1.5268  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:15    time: 1.4162  data: 0.0011  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:01    time: 1.4072  data: 0.0011  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:45    time: 1.3617  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:32    time: 1.4368  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:17    time: 1.4227  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:03    time: 1.3499  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:48    time: 1.4322  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:35    time: 1.5116  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:20    time: 1.4538  data: 0.0011  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:05    time: 1.2650  data: 0.0012  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:49    time: 1.2063  data: 0.0012  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:33    time: 1.1398  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:19    time: 1.1899  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:05    time: 1.3729  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:51    time: 1.4235  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:36    time: 1.4003  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:23    time: 1.4797  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:07    time: 1.3265  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:54    time: 1.3665  data: 0.0011  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:40    time: 1.5498  data: 0.0011  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:26    time: 1.4543  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:12    time: 1.5148  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:58    time: 1.4348  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:44    time: 1.4167  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:29    time: 1.4141  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:15    time: 1.3414  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:02    time: 1.4907  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:47    time: 1.3634  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:33    time: 1.2818  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:18    time: 1.4332  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:04    time: 1.4635  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:50    time: 1.3917  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:36    time: 1.3302  data: 0.0011  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:22    time: 1.3367  data: 0.0011  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.3721  data: 0.0011  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4200  data: 0.0012  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3478  data: 0.0011  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3406  data: 0.0009  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4418  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.4938  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:43    time: 1.3421  data: 0.0009  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1566  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2586  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3558  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.4098  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4638  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3754  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3421  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3144  data: 0.0434  max mem: 66110
Evaluation Total time: 0:25:38 (1.4074 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_32_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [33]  [   0/7110]  eta: 2 days, 6:21:06  lr: 0.000010  loss: 0.2523  time: 27.5198  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [  50/7110]  eta: 2:57:58  lr: 0.000010  loss: 0.1381  time: 1.0369  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 100/7110]  eta: 2:25:44  lr: 0.000010  loss: 0.6111  time: 1.0105  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 150/7110]  eta: 2:14:28  lr: 0.000010  loss: 0.3210  time: 0.9823  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 200/7110]  eta: 2:08:18  lr: 0.000010  loss: 0.0947  time: 0.9469  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 250/7110]  eta: 2:03:53  lr: 0.000010  loss: 0.1258  time: 0.9691  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 300/7110]  eta: 2:01:05  lr: 0.000010  loss: 0.3038  time: 0.9852  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 350/7110]  eta: 1:59:26  lr: 0.000010  loss: 0.1705  time: 1.0366  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 400/7110]  eta: 1:57:17  lr: 0.000010  loss: 0.4527  time: 0.9520  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 450/7110]  eta: 1:55:59  lr: 0.000010  loss: 0.2180  time: 0.9862  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 500/7110]  eta: 1:54:10  lr: 0.000010  loss: 0.1252  time: 0.9384  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 550/7110]  eta: 1:52:47  lr: 0.000010  loss: 0.0512  time: 1.0057  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 600/7110]  eta: 1:51:23  lr: 0.000010  loss: 0.0283  time: 0.9521  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 650/7110]  eta: 1:50:25  lr: 0.000010  loss: 0.5695  time: 0.9763  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 700/7110]  eta: 1:49:02  lr: 0.000010  loss: 0.0933  time: 0.9763  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 750/7110]  eta: 1:47:52  lr: 0.000010  loss: 0.1683  time: 1.0153  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 800/7110]  eta: 1:47:00  lr: 0.000010  loss: 0.2177  time: 1.0111  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 850/7110]  eta: 1:46:01  lr: 0.000010  loss: 0.1025  time: 0.9719  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 900/7110]  eta: 1:45:09  lr: 0.000010  loss: 0.3109  time: 1.0008  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [ 950/7110]  eta: 1:44:04  lr: 0.000010  loss: 0.2188  time: 0.9819  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1000/7110]  eta: 1:43:07  lr: 0.000010  loss: 0.1719  time: 1.0106  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1050/7110]  eta: 1:42:16  lr: 0.000010  loss: 0.1136  time: 0.9953  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1100/7110]  eta: 1:41:24  lr: 0.000010  loss: 0.3362  time: 1.0247  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1150/7110]  eta: 1:40:36  lr: 0.000010  loss: 0.0358  time: 0.9945  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1200/7110]  eta: 1:39:49  lr: 0.000010  loss: 0.1204  time: 1.0198  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1250/7110]  eta: 1:38:59  lr: 0.000010  loss: 0.2289  time: 0.9913  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1300/7110]  eta: 1:38:11  lr: 0.000010  loss: 0.2350  time: 1.0163  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1350/7110]  eta: 1:37:18  lr: 0.000010  loss: 0.4427  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1400/7110]  eta: 1:36:31  lr: 0.000010  loss: 0.0271  time: 1.0044  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1450/7110]  eta: 1:35:36  lr: 0.000010  loss: 0.1169  time: 0.9773  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1500/7110]  eta: 1:34:42  lr: 0.000010  loss: 0.5550  time: 0.9948  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1550/7110]  eta: 1:33:53  lr: 0.000010  loss: 0.3601  time: 1.0705  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1600/7110]  eta: 1:32:59  lr: 0.000010  loss: 0.4458  time: 1.0059  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1650/7110]  eta: 1:32:04  lr: 0.000010  loss: 0.4364  time: 1.0462  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1700/7110]  eta: 1:31:05  lr: 0.000010  loss: 0.3561  time: 0.9644  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1750/7110]  eta: 1:30:15  lr: 0.000010  loss: 0.1812  time: 1.0217  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1800/7110]  eta: 1:29:24  lr: 0.000010  loss: 0.0888  time: 1.0046  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1850/7110]  eta: 1:28:30  lr: 0.000010  loss: 0.0263  time: 1.0289  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1900/7110]  eta: 1:27:41  lr: 0.000010  loss: 0.0559  time: 1.0765  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [1950/7110]  eta: 1:26:47  lr: 0.000010  loss: 0.1321  time: 0.9834  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2000/7110]  eta: 1:26:00  lr: 0.000010  loss: 0.2388  time: 1.0330  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2050/7110]  eta: 1:25:07  lr: 0.000010  loss: 0.5688  time: 0.9582  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2100/7110]  eta: 1:24:19  lr: 0.000010  loss: 0.1042  time: 1.0482  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2150/7110]  eta: 1:23:30  lr: 0.000010  loss: 0.5580  time: 1.0488  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2200/7110]  eta: 1:22:39  lr: 0.000010  loss: 0.2764  time: 0.9812  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2250/7110]  eta: 1:21:45  lr: 0.000010  loss: 0.7482  time: 0.9649  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2300/7110]  eta: 1:20:55  lr: 0.000010  loss: 0.2109  time: 1.0297  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2350/7110]  eta: 1:20:05  lr: 0.000010  loss: 0.3816  time: 1.0329  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2400/7110]  eta: 1:19:13  lr: 0.000010  loss: 0.0616  time: 0.9903  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2450/7110]  eta: 1:18:21  lr: 0.000010  loss: 0.0936  time: 0.9498  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2500/7110]  eta: 1:17:29  lr: 0.000010  loss: 0.4789  time: 0.9839  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2550/7110]  eta: 1:16:40  lr: 0.000010  loss: 0.1266  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2600/7110]  eta: 1:15:49  lr: 0.000010  loss: 0.1287  time: 0.9949  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2650/7110]  eta: 1:14:57  lr: 0.000010  loss: 0.1755  time: 1.0286  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2700/7110]  eta: 1:14:03  lr: 0.000010  loss: 0.1654  time: 0.9443  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2750/7110]  eta: 1:13:10  lr: 0.000010  loss: 0.1362  time: 0.9453  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2800/7110]  eta: 1:12:19  lr: 0.000010  loss: 0.0839  time: 1.0222  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2850/7110]  eta: 1:11:27  lr: 0.000010  loss: 0.2476  time: 0.9640  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2900/7110]  eta: 1:10:35  lr: 0.000010  loss: 1.5620  time: 0.9693  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [2950/7110]  eta: 1:09:45  lr: 0.000010  loss: 0.1147  time: 1.0048  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3000/7110]  eta: 1:08:54  lr: 0.000010  loss: 0.0511  time: 0.9668  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3050/7110]  eta: 1:08:00  lr: 0.000010  loss: 0.3167  time: 0.9770  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3100/7110]  eta: 1:07:06  lr: 0.000010  loss: 0.3479  time: 0.9467  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3150/7110]  eta: 1:06:14  lr: 0.000010  loss: 0.0145  time: 0.9633  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3200/7110]  eta: 1:05:24  lr: 0.000010  loss: 0.3160  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3250/7110]  eta: 1:04:32  lr: 0.000010  loss: 0.7967  time: 0.9522  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3300/7110]  eta: 1:03:42  lr: 0.000010  loss: 0.0584  time: 1.0267  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3350/7110]  eta: 1:02:50  lr: 0.000010  loss: 0.1871  time: 0.9598  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3400/7110]  eta: 1:02:02  lr: 0.000010  loss: 0.1287  time: 0.9865  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3450/7110]  eta: 1:01:12  lr: 0.000010  loss: 0.0394  time: 1.0319  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3500/7110]  eta: 1:00:21  lr: 0.000010  loss: 0.4952  time: 0.9973  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3550/7110]  eta: 0:59:31  lr: 0.000010  loss: 0.6262  time: 1.0295  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3600/7110]  eta: 0:58:41  lr: 0.000010  loss: 0.0995  time: 0.9929  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3650/7110]  eta: 0:57:51  lr: 0.000010  loss: 0.4148  time: 1.0015  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3700/7110]  eta: 0:57:00  lr: 0.000010  loss: 0.0907  time: 1.0242  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3750/7110]  eta: 0:56:12  lr: 0.000010  loss: 0.0960  time: 1.0691  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3800/7110]  eta: 0:55:22  lr: 0.000010  loss: 0.1046  time: 0.9885  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3850/7110]  eta: 0:54:32  lr: 0.000010  loss: 0.1643  time: 0.9446  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3900/7110]  eta: 0:53:42  lr: 0.000010  loss: 0.2070  time: 1.0219  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [3950/7110]  eta: 0:52:53  lr: 0.000010  loss: 0.0717  time: 1.0590  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4000/7110]  eta: 0:52:02  lr: 0.000010  loss: 0.1288  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4050/7110]  eta: 0:51:12  lr: 0.000010  loss: 0.3470  time: 0.9741  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4100/7110]  eta: 0:50:21  lr: 0.000010  loss: 0.1571  time: 0.9845  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4150/7110]  eta: 0:49:32  lr: 0.000010  loss: 0.1280  time: 1.0441  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4200/7110]  eta: 0:48:42  lr: 0.000010  loss: 0.1209  time: 1.0687  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4250/7110]  eta: 0:47:53  lr: 0.000010  loss: 0.1252  time: 0.9956  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4300/7110]  eta: 0:47:01  lr: 0.000010  loss: 0.3140  time: 0.9538  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4350/7110]  eta: 0:46:13  lr: 0.000010  loss: 0.1609  time: 1.0460  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4400/7110]  eta: 0:45:22  lr: 0.000010  loss: 0.3339  time: 1.0030  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4450/7110]  eta: 0:44:33  lr: 0.000010  loss: 0.4141  time: 0.9631  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4500/7110]  eta: 0:43:43  lr: 0.000010  loss: 0.4372  time: 1.0015  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4550/7110]  eta: 0:42:53  lr: 0.000010  loss: 0.5474  time: 1.0406  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4600/7110]  eta: 0:42:03  lr: 0.000010  loss: 0.2428  time: 0.9529  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4650/7110]  eta: 0:41:12  lr: 0.000010  loss: 0.0807  time: 0.9844  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4700/7110]  eta: 0:40:22  lr: 0.000010  loss: 0.2089  time: 1.0650  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4750/7110]  eta: 0:39:31  lr: 0.000010  loss: 0.1254  time: 0.9944  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4800/7110]  eta: 0:38:40  lr: 0.000010  loss: 0.1139  time: 0.9755  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4850/7110]  eta: 0:37:50  lr: 0.000010  loss: 0.3739  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4900/7110]  eta: 0:36:59  lr: 0.000010  loss: 0.2485  time: 0.9855  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [4950/7110]  eta: 0:36:08  lr: 0.000010  loss: 0.3946  time: 0.9628  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5000/7110]  eta: 0:35:18  lr: 0.000010  loss: 0.0432  time: 0.9946  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5050/7110]  eta: 0:34:28  lr: 0.000010  loss: 0.1561  time: 1.0056  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5100/7110]  eta: 0:33:38  lr: 0.000010  loss: 1.3966  time: 1.0097  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5150/7110]  eta: 0:32:48  lr: 0.000010  loss: 0.2455  time: 1.0012  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5200/7110]  eta: 0:31:58  lr: 0.000010  loss: 0.3047  time: 0.9867  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5250/7110]  eta: 0:31:08  lr: 0.000010  loss: 0.0951  time: 1.0379  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5300/7110]  eta: 0:30:18  lr: 0.000010  loss: 0.3820  time: 0.9870  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5350/7110]  eta: 0:29:27  lr: 0.000010  loss: 0.2872  time: 1.0408  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5400/7110]  eta: 0:28:37  lr: 0.000010  loss: 0.0357  time: 1.0424  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5450/7110]  eta: 0:27:47  lr: 0.000010  loss: 0.1416  time: 1.0040  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5500/7110]  eta: 0:26:56  lr: 0.000010  loss: 0.1349  time: 0.9379  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5550/7110]  eta: 0:26:06  lr: 0.000010  loss: 0.2245  time: 1.0163  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5600/7110]  eta: 0:25:16  lr: 0.000010  loss: 0.1292  time: 1.0394  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5650/7110]  eta: 0:24:26  lr: 0.000010  loss: 0.1836  time: 1.0363  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5700/7110]  eta: 0:23:36  lr: 0.000010  loss: 0.1100  time: 0.9497  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5750/7110]  eta: 0:22:45  lr: 0.000010  loss: 0.1493  time: 0.9454  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5800/7110]  eta: 0:21:55  lr: 0.000010  loss: 0.3320  time: 0.9911  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5850/7110]  eta: 0:21:05  lr: 0.000010  loss: 0.3151  time: 0.9955  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.2367  time: 1.0002  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [5950/7110]  eta: 0:19:24  lr: 0.000010  loss: 0.1047  time: 1.0470  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6000/7110]  eta: 0:18:34  lr: 0.000010  loss: 0.3386  time: 1.0031  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6050/7110]  eta: 0:17:44  lr: 0.000010  loss: 0.2054  time: 0.9828  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6100/7110]  eta: 0:16:54  lr: 0.000010  loss: 0.2390  time: 1.0060  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.2311  time: 1.0058  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.2535  time: 0.9443  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.0602  time: 1.0167  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6300/7110]  eta: 0:13:33  lr: 0.000010  loss: 0.2746  time: 0.9620  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.5200  time: 0.9709  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.1924  time: 0.9979  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.2581  time: 1.0421  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.1004  time: 0.9910  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.2271  time: 0.9609  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.1787  time: 0.9416  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1394  time: 0.9643  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.0866  time: 0.9764  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.4406  time: 1.0586  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.3159  time: 0.9545  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.0989  time: 0.9914  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.0836  time: 0.9492  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.2351  time: 0.9458  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 1.5917  time: 1.0300  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.6600  time: 1.0025  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 1.6179  time: 1.0442  data: 0.0000  max mem: 66110
Train: data epoch: [33]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.2230  time: 1.1190  data: 0.0000  max mem: 66110
Train: data epoch: [33] Total time: 1:58:47 (1.0024 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:20:05    time: 20.8648  data: 19.6182  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:29    time: 3.1855  data: 1.7844  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:08    time: 1.3721  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:31    time: 1.2681  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:41    time: 1.2869  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:01    time: 1.3838  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:27:55    time: 1.2434  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:18    time: 1.2811  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:40    time: 1.4541  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:09    time: 1.4358  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:02    time: 1.5442  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:35    time: 1.5479  data: 0.0011  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:57    time: 1.3693  data: 0.0011  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:40    time: 1.4028  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:01    time: 1.3512  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:31    time: 1.2279  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:11    time: 1.3406  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:42    time: 1.3244  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:23    time: 1.3147  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:13    time: 1.4792  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:51    time: 1.4366  data: 0.0009  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:41    time: 1.4531  data: 0.0009  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:20    time: 1.4514  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:56    time: 1.2542  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:36    time: 1.2641  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:20    time: 1.3671  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:13    time: 1.5410  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:51    time: 1.4359  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:33    time: 1.2586  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:25    time: 1.5080  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:02    time: 1.4061  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:42    time: 1.1795  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:25    time: 1.2777  data: 0.0011  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:08    time: 1.3028  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:55    time: 1.3932  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:41    time: 1.4600  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:21    time: 1.3084  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:04    time: 1.2213  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:53    time: 1.4132  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:38    time: 1.4714  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:24    time: 1.4239  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:08    time: 1.3864  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:55    time: 1.3781  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:39    time: 1.4107  data: 0.0012  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:20    time: 1.2213  data: 0.0012  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:06    time: 1.2266  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:52    time: 1.4033  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:36    time: 1.3497  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:23    time: 1.3624  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:09    time: 1.4749  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:13:58    time: 1.5510  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:45    time: 1.5592  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:31    time: 1.4691  data: 0.0009  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:17    time: 1.4323  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:03    time: 1.4328  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:48    time: 1.3996  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:30    time: 1.1688  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:15    time: 1.1644  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:01    time: 1.3563  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:44    time: 1.2456  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:32    time: 1.3466  data: 0.0011  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:18    time: 1.4932  data: 0.0011  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:03    time: 1.3912  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:49    time: 1.3864  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:35    time: 1.3655  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:21    time: 1.4011  data: 0.0011  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:06    time: 1.3649  data: 0.0012  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:52    time: 1.3342  data: 0.0011  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:38    time: 1.4200  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:26    time: 1.5188  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:11    time: 1.4621  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:56    time: 1.2519  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:41    time: 1.1724  data: 0.0011  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:25    time: 1.1012  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:10    time: 1.1543  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:56    time: 1.3224  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:43    time: 1.3935  data: 0.0011  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:29    time: 1.3995  data: 0.0011  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:15    time: 1.4573  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:00    time: 1.2958  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:47    time: 1.3251  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:33    time: 1.5074  data: 0.0009  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:20    time: 1.4602  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:06    time: 1.5180  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:52    time: 1.4326  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:38    time: 1.4143  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:24    time: 1.4000  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:10    time: 1.3348  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:57    time: 1.4908  data: 0.0009  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:42    time: 1.3393  data: 0.0009  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:28    time: 1.2542  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:14    time: 1.4225  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:00    time: 1.4103  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:46    time: 1.3386  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:32    time: 1.3571  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:18    time: 1.3536  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.3526  data: 0.0009  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4176  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:36    time: 1.2953  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.2770  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4313  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4455  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2967  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1550  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2312  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3112  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3592  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:31    time: 1.4377  data: 0.0009  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:17    time: 1.3708  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3218  data: 0.0011  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2937  data: 0.0427  max mem: 66110
Evaluation Total time: 0:25:13 (1.3843 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_33_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [34]  [   0/7110]  eta: 2 days, 6:25:40  lr: 0.000010  loss: 0.0768  time: 27.5584  data: 0.0001  max mem: 66110
Train: data epoch: [34]  [  50/7110]  eta: 2:56:33  lr: 0.000010  loss: 0.2859  time: 0.9807  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 100/7110]  eta: 2:27:51  lr: 0.000010  loss: 0.0781  time: 1.0643  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 150/7110]  eta: 2:17:11  lr: 0.000010  loss: 0.1869  time: 1.0518  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 200/7110]  eta: 2:10:27  lr: 0.000010  loss: 0.7114  time: 0.9780  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 250/7110]  eta: 2:06:35  lr: 0.000010  loss: 0.3498  time: 0.9821  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 300/7110]  eta: 2:03:28  lr: 0.000010  loss: 0.3240  time: 1.0305  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 350/7110]  eta: 2:00:49  lr: 0.000010  loss: 0.3167  time: 1.0201  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 400/7110]  eta: 1:58:52  lr: 0.000010  loss: 0.2017  time: 0.9802  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 450/7110]  eta: 1:57:36  lr: 0.000010  loss: 0.1306  time: 1.0184  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 500/7110]  eta: 1:55:27  lr: 0.000010  loss: 0.1527  time: 0.9251  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 550/7110]  eta: 1:54:06  lr: 0.000010  loss: 0.2646  time: 1.0026  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 600/7110]  eta: 1:52:55  lr: 0.000010  loss: 0.1244  time: 1.0038  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 650/7110]  eta: 1:51:31  lr: 0.000010  loss: 0.1660  time: 0.9943  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 700/7110]  eta: 1:50:31  lr: 0.000010  loss: 0.0734  time: 0.9532  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 750/7110]  eta: 1:49:28  lr: 0.000010  loss: 0.6641  time: 1.0469  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 800/7110]  eta: 1:48:18  lr: 0.000010  loss: 0.2251  time: 0.9506  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 850/7110]  eta: 1:47:31  lr: 0.000010  loss: 0.1036  time: 1.0791  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 900/7110]  eta: 1:46:26  lr: 0.000010  loss: 0.3527  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [ 950/7110]  eta: 1:45:28  lr: 0.000010  loss: 0.4089  time: 1.0212  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1000/7110]  eta: 1:44:20  lr: 0.000010  loss: 0.2725  time: 0.9579  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1050/7110]  eta: 1:43:24  lr: 0.000010  loss: 0.2003  time: 0.9962  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1100/7110]  eta: 1:42:27  lr: 0.000010  loss: 0.1290  time: 0.9913  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1150/7110]  eta: 1:41:40  lr: 0.000010  loss: 0.2006  time: 1.0366  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1200/7110]  eta: 1:40:35  lr: 0.000010  loss: 0.0828  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1250/7110]  eta: 1:39:46  lr: 0.000010  loss: 0.1406  time: 1.0474  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1300/7110]  eta: 1:38:52  lr: 0.000010  loss: 0.1050  time: 0.9650  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1350/7110]  eta: 1:38:03  lr: 0.000010  loss: 0.2794  time: 0.9912  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1400/7110]  eta: 1:37:02  lr: 0.000010  loss: 0.1924  time: 0.9940  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1450/7110]  eta: 1:36:11  lr: 0.000010  loss: 0.1157  time: 1.0337  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1500/7110]  eta: 1:35:12  lr: 0.000010  loss: 0.4621  time: 1.0132  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1550/7110]  eta: 1:34:18  lr: 0.000010  loss: 0.4021  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1600/7110]  eta: 1:33:25  lr: 0.000010  loss: 0.1160  time: 1.0168  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1650/7110]  eta: 1:32:30  lr: 0.000010  loss: 0.1786  time: 0.9811  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1700/7110]  eta: 1:31:35  lr: 0.000010  loss: 0.1357  time: 0.9843  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1750/7110]  eta: 1:30:42  lr: 0.000010  loss: 0.2108  time: 0.9305  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1800/7110]  eta: 1:29:43  lr: 0.000010  loss: 0.0269  time: 0.9416  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1850/7110]  eta: 1:28:51  lr: 0.000010  loss: 0.0380  time: 1.0079  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1900/7110]  eta: 1:27:58  lr: 0.000010  loss: 0.0783  time: 0.9859  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [1950/7110]  eta: 1:27:11  lr: 0.000010  loss: 0.4884  time: 1.0534  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2000/7110]  eta: 1:26:19  lr: 0.000010  loss: 0.0248  time: 1.0190  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2050/7110]  eta: 1:25:22  lr: 0.000010  loss: 0.1862  time: 0.9589  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2100/7110]  eta: 1:24:29  lr: 0.000010  loss: 0.1707  time: 0.9774  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2150/7110]  eta: 1:23:37  lr: 0.000010  loss: 0.4720  time: 1.0535  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2200/7110]  eta: 1:22:42  lr: 0.000010  loss: 0.2286  time: 0.9667  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2250/7110]  eta: 1:21:48  lr: 0.000010  loss: 0.2583  time: 1.0122  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2300/7110]  eta: 1:20:59  lr: 0.000010  loss: 0.6028  time: 1.0455  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2350/7110]  eta: 1:20:09  lr: 0.000010  loss: 0.2573  time: 0.9759  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2400/7110]  eta: 1:19:19  lr: 0.000010  loss: 0.7110  time: 0.9575  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2450/7110]  eta: 1:18:27  lr: 0.000010  loss: 0.3976  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2500/7110]  eta: 1:17:38  lr: 0.000010  loss: 0.3318  time: 1.0531  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2550/7110]  eta: 1:16:51  lr: 0.000010  loss: 0.0496  time: 1.0283  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2600/7110]  eta: 1:15:59  lr: 0.000010  loss: 0.4822  time: 1.0267  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2650/7110]  eta: 1:15:06  lr: 0.000010  loss: 0.0598  time: 0.9910  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2700/7110]  eta: 1:14:11  lr: 0.000010  loss: 0.2815  time: 0.9251  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2750/7110]  eta: 1:13:20  lr: 0.000010  loss: 0.3621  time: 0.9536  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2800/7110]  eta: 1:12:28  lr: 0.000010  loss: 0.3005  time: 0.9611  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2850/7110]  eta: 1:11:39  lr: 0.000010  loss: 0.0870  time: 0.9975  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2900/7110]  eta: 1:10:47  lr: 0.000010  loss: 0.2092  time: 1.0451  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [2950/7110]  eta: 1:09:58  lr: 0.000010  loss: 0.4173  time: 0.9865  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3000/7110]  eta: 1:09:08  lr: 0.000010  loss: 0.2413  time: 1.0473  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3050/7110]  eta: 1:08:19  lr: 0.000010  loss: 0.2699  time: 0.9760  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3100/7110]  eta: 1:07:29  lr: 0.000010  loss: 0.4732  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3150/7110]  eta: 1:06:36  lr: 0.000010  loss: 0.4025  time: 0.9388  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3200/7110]  eta: 1:05:45  lr: 0.000010  loss: 0.3562  time: 1.0003  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3250/7110]  eta: 1:04:54  lr: 0.000010  loss: 0.1620  time: 1.0298  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3300/7110]  eta: 1:04:02  lr: 0.000010  loss: 0.0698  time: 1.0013  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3350/7110]  eta: 1:03:12  lr: 0.000010  loss: 0.4419  time: 1.0050  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3400/7110]  eta: 1:02:21  lr: 0.000010  loss: 0.0728  time: 0.9252  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3450/7110]  eta: 1:01:29  lr: 0.000010  loss: 0.1895  time: 0.9739  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3500/7110]  eta: 1:00:39  lr: 0.000010  loss: 0.0279  time: 0.9960  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3550/7110]  eta: 0:59:46  lr: 0.000010  loss: 0.3224  time: 0.9073  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3600/7110]  eta: 0:58:54  lr: 0.000010  loss: 1.7005  time: 0.9675  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3650/7110]  eta: 0:58:02  lr: 0.000010  loss: 0.9410  time: 0.9817  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3700/7110]  eta: 0:57:12  lr: 0.000010  loss: 0.3195  time: 0.9936  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3750/7110]  eta: 0:56:21  lr: 0.000010  loss: 0.0858  time: 0.9687  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3800/7110]  eta: 0:55:29  lr: 0.000010  loss: 0.1003  time: 0.9737  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3850/7110]  eta: 0:54:39  lr: 0.000010  loss: 0.2995  time: 1.0223  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3900/7110]  eta: 0:53:48  lr: 0.000010  loss: 0.1658  time: 0.9891  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [3950/7110]  eta: 0:52:57  lr: 0.000010  loss: 0.9432  time: 0.9728  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4000/7110]  eta: 0:52:08  lr: 0.000010  loss: 0.0383  time: 1.0626  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4050/7110]  eta: 0:51:16  lr: 0.000010  loss: 0.1579  time: 0.9432  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4100/7110]  eta: 0:50:27  lr: 0.000010  loss: 0.3010  time: 1.0317  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4150/7110]  eta: 0:49:37  lr: 0.000010  loss: 0.0834  time: 1.0063  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4200/7110]  eta: 0:48:45  lr: 0.000010  loss: 0.2627  time: 0.9395  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4250/7110]  eta: 0:47:54  lr: 0.000010  loss: 0.1656  time: 0.9525  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4300/7110]  eta: 0:47:04  lr: 0.000010  loss: 0.1281  time: 0.9965  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4350/7110]  eta: 0:46:13  lr: 0.000010  loss: 0.2244  time: 1.0077  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4400/7110]  eta: 0:45:24  lr: 0.000010  loss: 0.0373  time: 1.0109  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4450/7110]  eta: 0:44:32  lr: 0.000010  loss: 0.5671  time: 0.9847  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4500/7110]  eta: 0:43:42  lr: 0.000010  loss: 0.3022  time: 0.9978  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4550/7110]  eta: 0:42:52  lr: 0.000010  loss: 0.5488  time: 1.0452  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4600/7110]  eta: 0:42:02  lr: 0.000010  loss: 0.2310  time: 0.9881  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4650/7110]  eta: 0:41:12  lr: 0.000010  loss: 0.0834  time: 1.0232  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4700/7110]  eta: 0:40:22  lr: 0.000010  loss: 0.3210  time: 0.9266  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4750/7110]  eta: 0:39:32  lr: 0.000010  loss: 0.2924  time: 1.0509  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4800/7110]  eta: 0:38:41  lr: 0.000010  loss: 0.6842  time: 1.0338  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4850/7110]  eta: 0:37:51  lr: 0.000010  loss: 0.1981  time: 0.9995  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4900/7110]  eta: 0:37:01  lr: 0.000010  loss: 0.0869  time: 1.0177  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [4950/7110]  eta: 0:36:10  lr: 0.000010  loss: 0.4994  time: 1.0367  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5000/7110]  eta: 0:35:20  lr: 0.000010  loss: 0.2932  time: 1.0216  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5050/7110]  eta: 0:34:30  lr: 0.000010  loss: 0.0423  time: 0.9972  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5100/7110]  eta: 0:33:40  lr: 0.000010  loss: 0.0237  time: 0.9846  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5150/7110]  eta: 0:32:50  lr: 0.000010  loss: 0.0962  time: 0.9703  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5200/7110]  eta: 0:31:59  lr: 0.000010  loss: 0.0362  time: 1.0143  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5250/7110]  eta: 0:31:09  lr: 0.000010  loss: 0.1680  time: 0.9874  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5300/7110]  eta: 0:30:19  lr: 0.000010  loss: 0.0681  time: 0.9852  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5350/7110]  eta: 0:29:28  lr: 0.000010  loss: 0.2700  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5400/7110]  eta: 0:28:38  lr: 0.000010  loss: 0.1538  time: 1.0181  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5450/7110]  eta: 0:27:48  lr: 0.000010  loss: 0.4669  time: 1.0339  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5500/7110]  eta: 0:26:58  lr: 0.000010  loss: 0.0517  time: 1.0198  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5550/7110]  eta: 0:26:08  lr: 0.000010  loss: 0.3315  time: 1.0336  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5600/7110]  eta: 0:25:18  lr: 0.000010  loss: 0.5571  time: 1.0317  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5650/7110]  eta: 0:24:27  lr: 0.000010  loss: 0.1267  time: 0.9817  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5700/7110]  eta: 0:23:37  lr: 0.000010  loss: 0.2159  time: 1.0562  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5750/7110]  eta: 0:22:46  lr: 0.000010  loss: 0.0125  time: 0.9164  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5800/7110]  eta: 0:21:55  lr: 0.000010  loss: 0.0633  time: 0.9613  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5850/7110]  eta: 0:21:05  lr: 0.000010  loss: 0.2401  time: 1.0542  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5900/7110]  eta: 0:20:15  lr: 0.000010  loss: 0.4773  time: 1.0107  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [5950/7110]  eta: 0:19:25  lr: 0.000010  loss: 0.1635  time: 1.0290  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6000/7110]  eta: 0:18:34  lr: 0.000010  loss: 0.3994  time: 0.9847  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6050/7110]  eta: 0:17:44  lr: 0.000010  loss: 0.1168  time: 0.9712  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6100/7110]  eta: 0:16:54  lr: 0.000010  loss: 0.0551  time: 1.0590  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6150/7110]  eta: 0:16:04  lr: 0.000010  loss: 0.5009  time: 0.9854  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6200/7110]  eta: 0:15:14  lr: 0.000010  loss: 0.0809  time: 0.9299  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.1175  time: 1.0006  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6300/7110]  eta: 0:13:33  lr: 0.000010  loss: 0.1370  time: 1.0393  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6350/7110]  eta: 0:12:43  lr: 0.000010  loss: 0.0620  time: 0.9793  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.0844  time: 0.9776  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.0954  time: 1.0527  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.1250  time: 1.0437  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.4191  time: 0.9845  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6600/7110]  eta: 0:08:32  lr: 0.000010  loss: 0.0287  time: 0.9815  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1024  time: 0.9811  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.1604  time: 0.9692  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.0740  time: 1.0176  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.1769  time: 0.9696  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.5124  time: 0.9731  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1390  time: 0.9828  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1091  time: 0.9605  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.2278  time: 0.9397  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.4551  time: 1.0378  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.0710  time: 0.9356  data: 0.0000  max mem: 66110
Train: data epoch: [34]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0554  time: 1.1009  data: 0.0000  max mem: 66110
Train: data epoch: [34] Total time: 1:58:53 (1.0033 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:18:12    time: 20.7616  data: 19.5129  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:10    time: 3.1675  data: 1.7748  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:08    time: 1.3776  data: 0.0009  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:43    time: 1.2952  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:57    time: 1.3169  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:17    time: 1.4061  data: 0.0011  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:09    time: 1.2567  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:30    time: 1.2836  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:52    time: 1.4593  data: 0.0009  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:23    time: 1.4570  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:12    time: 1.5501  data: 0.0009  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:42    time: 1.5263  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:05    time: 1.3631  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:46    time: 1.4000  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:08    time: 1.3533  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:41    time: 1.2716  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:18    time: 1.3494  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:49    time: 1.3054  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:30    time: 1.3209  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:20    time: 1.4945  data: 0.0009  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:59    time: 1.4676  data: 0.0009  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:55    time: 1.5379  data: 0.0009  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:34    time: 1.5318  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:12    time: 1.3055  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:53    time: 1.3224  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:38    time: 1.4060  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:30    time: 1.5708  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:07    time: 1.4455  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:46    time: 1.2187  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:37    time: 1.4558  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:14    time: 1.4106  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:54    time: 1.1953  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:40    time: 1.3520  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:21    time: 1.3624  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:08    time: 1.3848  data: 0.0009  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:53    time: 1.4687  data: 0.0009  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:35    time: 1.3462  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:18    time: 1.2836  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:06    time: 1.4377  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:51    time: 1.4834  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:37    time: 1.4370  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:22    time: 1.4340  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:07    time: 1.4021  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:51    time: 1.3762  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:34    time: 1.2765  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:19    time: 1.2954  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:04    time: 1.3903  data: 0.0009  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:48    time: 1.3706  data: 0.0009  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:34    time: 1.3925  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:21    time: 1.4682  data: 0.0009  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:09    time: 1.5481  data: 0.0009  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:56    time: 1.5676  data: 0.0009  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:42    time: 1.5192  data: 0.0009  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:28    time: 1.4815  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:14    time: 1.4499  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:59    time: 1.4062  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:41    time: 1.2405  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:26    time: 1.2406  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:12    time: 1.3840  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:55    time: 1.2817  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:42    time: 1.3777  data: 0.0009  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:28    time: 1.5310  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:14    time: 1.4253  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:59    time: 1.3911  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:44    time: 1.3540  data: 0.0009  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:31    time: 1.4094  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:15    time: 1.3916  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:01    time: 1.3501  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:47    time: 1.4277  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:34    time: 1.5210  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:19    time: 1.4635  data: 0.0009  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:04    time: 1.3033  data: 0.0009  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:49    time: 1.2444  data: 0.0009  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:33    time: 1.1396  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:18    time: 1.1888  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:04    time: 1.3583  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:50    time: 1.4232  data: 0.0009  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:36    time: 1.4040  data: 0.0009  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:22    time: 1.4494  data: 0.0009  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:07    time: 1.3213  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:54    time: 1.3869  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:40    time: 1.5479  data: 0.0009  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:26    time: 1.4586  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:12    time: 1.5200  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:58    time: 1.4575  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:44    time: 1.4370  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:29    time: 1.4094  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:15    time: 1.3316  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:01    time: 1.4769  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:46    time: 1.3503  data: 0.0009  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:32    time: 1.2737  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:18    time: 1.4280  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:04    time: 1.4545  data: 0.0009  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:50    time: 1.3953  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:36    time: 1.3288  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:21    time: 1.3108  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.3508  data: 0.0009  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4331  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3663  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3660  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4747  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.4998  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3362  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1839  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2915  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3688  data: 0.0009  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3755  data: 0.0009  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4273  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3804  data: 0.0009  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3337  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3073  data: 0.0444  max mem: 66110
Evaluation Total time: 0:25:37 (1.4064 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_34_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [35]  [   0/7110]  eta: 2 days, 6:46:01  lr: 0.000010  loss: 0.2595  time: 27.7301  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [  50/7110]  eta: 2:59:35  lr: 0.000010  loss: 0.4273  time: 1.0108  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 100/7110]  eta: 2:29:03  lr: 0.000010  loss: 0.0275  time: 0.9940  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 150/7110]  eta: 2:17:34  lr: 0.000010  loss: 0.5252  time: 0.9637  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 200/7110]  eta: 2:12:14  lr: 0.000010  loss: 0.2028  time: 0.9624  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 250/7110]  eta: 2:06:41  lr: 0.000010  loss: 0.0886  time: 0.9552  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 300/7110]  eta: 2:04:11  lr: 0.000010  loss: 0.1233  time: 1.0148  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 350/7110]  eta: 2:01:41  lr: 0.000010  loss: 0.2065  time: 0.9720  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 400/7110]  eta: 2:00:11  lr: 0.000010  loss: 0.0673  time: 1.0815  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 450/7110]  eta: 1:58:40  lr: 0.000010  loss: 0.1881  time: 1.0366  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 500/7110]  eta: 1:56:57  lr: 0.000010  loss: 0.1212  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 550/7110]  eta: 1:55:34  lr: 0.000010  loss: 0.3548  time: 1.0138  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 600/7110]  eta: 1:54:19  lr: 0.000010  loss: 0.2299  time: 0.9851  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 650/7110]  eta: 1:52:58  lr: 0.000010  loss: 0.2475  time: 0.9982  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 700/7110]  eta: 1:51:46  lr: 0.000010  loss: 0.0629  time: 1.0008  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 750/7110]  eta: 1:50:48  lr: 0.000010  loss: 1.6487  time: 1.0596  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 800/7110]  eta: 1:49:30  lr: 0.000010  loss: 0.0493  time: 0.9979  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 850/7110]  eta: 1:48:25  lr: 0.000010  loss: 0.6028  time: 1.0088  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 900/7110]  eta: 1:47:34  lr: 0.000010  loss: 0.2240  time: 1.0450  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [ 950/7110]  eta: 1:46:15  lr: 0.000010  loss: 0.0802  time: 0.9622  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1000/7110]  eta: 1:45:10  lr: 0.000010  loss: 0.2473  time: 1.0470  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1050/7110]  eta: 1:44:16  lr: 0.000010  loss: 0.1007  time: 0.9820  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1100/7110]  eta: 1:43:06  lr: 0.000010  loss: 0.6555  time: 0.9794  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1150/7110]  eta: 1:42:08  lr: 0.000010  loss: 0.0555  time: 0.9749  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1200/7110]  eta: 1:41:15  lr: 0.000010  loss: 0.0913  time: 1.0484  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1250/7110]  eta: 1:40:14  lr: 0.000010  loss: 0.1377  time: 0.9496  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1300/7110]  eta: 1:39:23  lr: 0.000010  loss: 0.5247  time: 0.9955  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1350/7110]  eta: 1:38:30  lr: 0.000010  loss: 0.5122  time: 1.0313  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1400/7110]  eta: 1:37:42  lr: 0.000010  loss: 0.2039  time: 0.9878  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1450/7110]  eta: 1:36:40  lr: 0.000010  loss: 0.3964  time: 1.0017  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1500/7110]  eta: 1:35:47  lr: 0.000010  loss: 0.5065  time: 0.9840  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1550/7110]  eta: 1:34:54  lr: 0.000010  loss: 0.0681  time: 1.0155  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1600/7110]  eta: 1:34:05  lr: 0.000010  loss: 0.0848  time: 1.0454  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1650/7110]  eta: 1:33:06  lr: 0.000010  loss: 0.1657  time: 0.9965  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1700/7110]  eta: 1:32:14  lr: 0.000010  loss: 0.1069  time: 1.0657  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1750/7110]  eta: 1:31:21  lr: 0.000010  loss: 0.3456  time: 1.0550  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1800/7110]  eta: 1:30:22  lr: 0.000010  loss: 0.5155  time: 0.9720  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1850/7110]  eta: 1:29:28  lr: 0.000010  loss: 0.4647  time: 0.9915  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1900/7110]  eta: 1:28:32  lr: 0.000010  loss: 0.4138  time: 0.9721  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [1950/7110]  eta: 1:27:39  lr: 0.000010  loss: 0.1999  time: 0.9755  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2000/7110]  eta: 1:26:45  lr: 0.000010  loss: 0.1016  time: 1.0025  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2050/7110]  eta: 1:25:54  lr: 0.000010  loss: 0.2232  time: 1.0587  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2100/7110]  eta: 1:25:00  lr: 0.000010  loss: 0.1927  time: 0.9798  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2150/7110]  eta: 1:24:06  lr: 0.000010  loss: 0.2456  time: 1.0293  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2200/7110]  eta: 1:23:08  lr: 0.000010  loss: 0.2260  time: 0.9272  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2250/7110]  eta: 1:22:19  lr: 0.000010  loss: 0.1247  time: 1.0418  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2300/7110]  eta: 1:21:27  lr: 0.000010  loss: 0.1376  time: 1.0438  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2350/7110]  eta: 1:20:32  lr: 0.000010  loss: 0.9082  time: 0.9794  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2400/7110]  eta: 1:19:35  lr: 0.000010  loss: 0.1425  time: 0.9630  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2450/7110]  eta: 1:18:39  lr: 0.000010  loss: 0.3329  time: 1.0034  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2500/7110]  eta: 1:17:43  lr: 0.000010  loss: 0.2686  time: 0.9514  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2550/7110]  eta: 1:16:51  lr: 0.000010  loss: 0.2421  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2600/7110]  eta: 1:16:01  lr: 0.000010  loss: 0.2069  time: 1.0103  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2650/7110]  eta: 1:15:09  lr: 0.000010  loss: 1.5130  time: 1.0065  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2700/7110]  eta: 1:14:15  lr: 0.000010  loss: 0.1652  time: 0.9594  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2750/7110]  eta: 1:13:24  lr: 0.000010  loss: 0.2077  time: 1.0576  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2800/7110]  eta: 1:12:35  lr: 0.000010  loss: 0.0816  time: 1.0286  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2850/7110]  eta: 1:11:43  lr: 0.000010  loss: 0.0074  time: 0.9663  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2900/7110]  eta: 1:10:50  lr: 0.000010  loss: 0.0488  time: 0.9848  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [2950/7110]  eta: 1:09:59  lr: 0.000010  loss: 0.3220  time: 1.0379  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3000/7110]  eta: 1:09:08  lr: 0.000010  loss: 0.4416  time: 0.9600  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3050/7110]  eta: 1:08:19  lr: 0.000010  loss: 0.2137  time: 1.0790  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3100/7110]  eta: 1:07:29  lr: 0.000010  loss: 0.2595  time: 1.0559  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3150/7110]  eta: 1:06:39  lr: 0.000010  loss: 0.1125  time: 1.0705  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3200/7110]  eta: 1:05:50  lr: 0.000010  loss: 0.1819  time: 1.0255  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3250/7110]  eta: 1:04:59  lr: 0.000010  loss: 0.1924  time: 1.0189  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3300/7110]  eta: 1:04:08  lr: 0.000010  loss: 0.0393  time: 1.0414  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3350/7110]  eta: 1:03:18  lr: 0.000010  loss: 0.3931  time: 0.9743  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3400/7110]  eta: 1:02:26  lr: 0.000010  loss: 0.2933  time: 0.9499  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3450/7110]  eta: 1:01:33  lr: 0.000010  loss: 0.2748  time: 1.0052  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3500/7110]  eta: 1:00:42  lr: 0.000010  loss: 0.6125  time: 0.9822  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3550/7110]  eta: 0:59:51  lr: 0.000010  loss: 0.2102  time: 0.9539  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3600/7110]  eta: 0:59:00  lr: 0.000010  loss: 0.0904  time: 0.9637  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3650/7110]  eta: 0:58:10  lr: 0.000010  loss: 0.4278  time: 0.9768  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3700/7110]  eta: 0:57:18  lr: 0.000010  loss: 0.1003  time: 0.9874  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3750/7110]  eta: 0:56:28  lr: 0.000010  loss: 0.3006  time: 0.9875  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3800/7110]  eta: 0:55:36  lr: 0.000010  loss: 0.3533  time: 0.9837  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3850/7110]  eta: 0:54:46  lr: 0.000010  loss: 0.2960  time: 0.9842  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3900/7110]  eta: 0:53:54  lr: 0.000010  loss: 0.1173  time: 0.9578  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [3950/7110]  eta: 0:53:02  lr: 0.000010  loss: 0.3240  time: 0.9738  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4000/7110]  eta: 0:52:11  lr: 0.000010  loss: 0.0246  time: 0.9852  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4050/7110]  eta: 0:51:20  lr: 0.000010  loss: 0.2010  time: 0.9863  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4100/7110]  eta: 0:50:30  lr: 0.000010  loss: 0.4032  time: 0.9746  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4150/7110]  eta: 0:49:39  lr: 0.000010  loss: 0.2318  time: 0.9432  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4200/7110]  eta: 0:48:48  lr: 0.000010  loss: 0.0198  time: 0.9650  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4250/7110]  eta: 0:47:57  lr: 0.000010  loss: 0.5145  time: 1.0046  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4300/7110]  eta: 0:47:06  lr: 0.000010  loss: 0.3454  time: 0.9813  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4350/7110]  eta: 0:46:15  lr: 0.000010  loss: 0.1673  time: 0.9635  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4400/7110]  eta: 0:45:24  lr: 0.000010  loss: 0.0263  time: 0.9666  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4450/7110]  eta: 0:44:33  lr: 0.000010  loss: 0.0575  time: 0.9901  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4500/7110]  eta: 0:43:42  lr: 0.000010  loss: 0.4137  time: 1.0468  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4550/7110]  eta: 0:42:52  lr: 0.000010  loss: 0.5825  time: 1.0252  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4600/7110]  eta: 0:42:03  lr: 0.000010  loss: 0.2266  time: 1.0019  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4650/7110]  eta: 0:41:12  lr: 0.000010  loss: 0.0919  time: 0.9954  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4700/7110]  eta: 0:40:22  lr: 0.000010  loss: 0.2613  time: 1.0647  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4750/7110]  eta: 0:39:31  lr: 0.000010  loss: 0.6067  time: 0.9517  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4800/7110]  eta: 0:38:40  lr: 0.000010  loss: 0.0734  time: 0.9992  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4850/7110]  eta: 0:37:50  lr: 0.000010  loss: 0.0549  time: 0.9925  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4900/7110]  eta: 0:37:01  lr: 0.000010  loss: 0.0361  time: 1.0729  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [4950/7110]  eta: 0:36:10  lr: 0.000010  loss: 0.3935  time: 1.0775  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5000/7110]  eta: 0:35:20  lr: 0.000010  loss: 1.3952  time: 1.0598  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5050/7110]  eta: 0:34:30  lr: 0.000010  loss: 0.1121  time: 0.9889  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5100/7110]  eta: 0:33:39  lr: 0.000010  loss: 0.1440  time: 0.9632  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5150/7110]  eta: 0:32:49  lr: 0.000010  loss: 0.0768  time: 0.9981  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5200/7110]  eta: 0:31:58  lr: 0.000010  loss: 0.1187  time: 0.9915  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5250/7110]  eta: 0:31:09  lr: 0.000010  loss: 0.1857  time: 0.9887  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5300/7110]  eta: 0:30:18  lr: 0.000010  loss: 0.4164  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5350/7110]  eta: 0:29:28  lr: 0.000010  loss: 0.1625  time: 1.0155  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5400/7110]  eta: 0:28:38  lr: 0.000010  loss: 0.6125  time: 0.9648  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5450/7110]  eta: 0:27:47  lr: 0.000010  loss: 0.3646  time: 0.9972  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5500/7110]  eta: 0:26:56  lr: 0.000010  loss: 0.1630  time: 0.8825  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5550/7110]  eta: 0:26:06  lr: 0.000010  loss: 0.6227  time: 0.9920  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5600/7110]  eta: 0:25:15  lr: 0.000010  loss: 0.2175  time: 1.0428  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5650/7110]  eta: 0:24:25  lr: 0.000010  loss: 0.1559  time: 0.9756  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5700/7110]  eta: 0:23:35  lr: 0.000010  loss: 0.0308  time: 1.0363  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5750/7110]  eta: 0:22:45  lr: 0.000010  loss: 0.1210  time: 1.0097  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5800/7110]  eta: 0:21:55  lr: 0.000010  loss: 0.1204  time: 1.0298  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 0.4647  time: 0.9653  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.1325  time: 1.0089  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [5950/7110]  eta: 0:19:24  lr: 0.000010  loss: 0.1126  time: 1.0178  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6000/7110]  eta: 0:18:34  lr: 0.000010  loss: 0.0181  time: 0.9575  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.3556  time: 0.9553  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.4291  time: 0.9356  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.2198  time: 1.0210  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.0386  time: 0.9759  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.1871  time: 0.9915  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.3741  time: 1.0005  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.2618  time: 0.9908  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.1843  time: 1.0033  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.1283  time: 0.9890  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.1410  time: 1.0604  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.0525  time: 0.9703  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.0229  time: 0.9583  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.3024  time: 0.9841  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.2457  time: 1.0830  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.0756  time: 1.0068  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.3527  time: 0.9688  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.4889  time: 1.0412  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.0826  time: 1.0275  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1244  time: 0.9547  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.2132  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.0219  time: 0.9794  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1368  time: 1.0449  data: 0.0000  max mem: 66110
Train: data epoch: [35]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0513  time: 1.0894  data: 0.0000  max mem: 66110
Train: data epoch: [35] Total time: 1:58:57 (1.0039 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:01:56    time: 19.8692  data: 18.6187  max mem: 66110
Evaluation  [  10/1093]  eta: 0:55:42    time: 3.0867  data: 1.6935  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:34    time: 1.4479  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:48    time: 1.3482  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:03    time: 1.3031  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:26    time: 1.4186  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:30    time: 1.3056  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:48    time: 1.3240  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:27:07    time: 1.4571  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:38    time: 1.4627  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:25    time: 1.5565  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:58    time: 1.5496  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:20    time: 1.3890  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:25:11    time: 1.4801  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:31    time: 1.4309  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:24:00    time: 1.2487  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:37    time: 1.3500  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:23:07    time: 1.3206  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:47    time: 1.3172  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:38    time: 1.5127  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:16    time: 1.4878  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:22:10    time: 1.5334  data: 0.0009  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:47    time: 1.5146  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:23    time: 1.2863  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:21:03    time: 1.3022  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:47    time: 1.3960  data: 0.0009  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:40    time: 1.5711  data: 0.0009  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:16    time: 1.4466  data: 0.0009  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:54    time: 1.2187  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:45    time: 1.4605  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:22    time: 1.4168  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:19:02    time: 1.1978  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:48    time: 1.3750  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:30    time: 1.4018  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:17    time: 1.4026  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:18:01    time: 1.4624  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:41    time: 1.3101  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:25    time: 1.2766  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:13    time: 1.4613  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:58    time: 1.4960  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:44    time: 1.4602  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:30    time: 1.4856  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:16    time: 1.4614  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:16:00    time: 1.4031  data: 0.0011  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:42    time: 1.2780  data: 0.0011  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:26    time: 1.2911  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:12    time: 1.4065  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:55    time: 1.3597  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:41    time: 1.3705  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:28    time: 1.5067  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:16    time: 1.5847  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:14:03    time: 1.6176  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:50    time: 1.5654  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:35    time: 1.4907  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:21    time: 1.4537  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:05    time: 1.3970  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:48    time: 1.2426  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:33    time: 1.2682  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:18    time: 1.3963  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:12:01    time: 1.2653  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:48    time: 1.3648  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:34    time: 1.5056  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:19    time: 1.3894  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:04    time: 1.3816  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:49    time: 1.3728  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:36    time: 1.4602  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:20    time: 1.4178  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:07    time: 1.4097  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:52    time: 1.4958  data: 0.0011  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:39    time: 1.5236  data: 0.0012  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:24    time: 1.4960  data: 0.0012  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:09    time: 1.3462  data: 0.0011  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:54    time: 1.2658  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:38    time: 1.1507  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:23    time: 1.1968  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:09    time: 1.3559  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:54    time: 1.4002  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:40    time: 1.4221  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:26    time: 1.5005  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:10    time: 1.2785  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:57    time: 1.3298  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:43    time: 1.5439  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:29    time: 1.4466  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:15    time: 1.5096  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:06:00    time: 1.4621  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:46    time: 1.4232  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:32    time: 1.3999  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:17    time: 1.3634  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:04    time: 1.5260  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:49    time: 1.3844  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:34    time: 1.2738  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:20    time: 1.4206  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:06    time: 1.3979  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:51    time: 1.3418  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:37    time: 1.4092  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:23    time: 1.4138  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:09    time: 1.3802  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:55    time: 1.4353  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:40    time: 1.3572  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:26    time: 1.3471  data: 0.0009  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:12    time: 1.4581  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:58    time: 1.4946  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:43    time: 1.3390  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:29    time: 1.1731  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:15    time: 1.2678  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3565  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.4242  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4686  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3709  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3411  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3136  data: 0.0434  max mem: 66110
Evaluation Total time: 0:25:48 (1.4164 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_35_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [36]  [   0/7110]  eta: 2 days, 6:40:05  lr: 0.000010  loss: 0.0614  time: 27.6801  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [  50/7110]  eta: 2:58:01  lr: 0.000010  loss: 0.0699  time: 0.9886  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 100/7110]  eta: 2:25:50  lr: 0.000010  loss: 0.1467  time: 0.9803  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 150/7110]  eta: 2:15:48  lr: 0.000010  loss: 0.2923  time: 0.9931  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 200/7110]  eta: 2:09:56  lr: 0.000010  loss: 0.1352  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 250/7110]  eta: 2:05:24  lr: 0.000010  loss: 0.0316  time: 0.9174  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 300/7110]  eta: 2:02:59  lr: 0.000010  loss: 0.5398  time: 1.0066  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 350/7110]  eta: 2:00:35  lr: 0.000010  loss: 0.1163  time: 1.0090  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 400/7110]  eta: 1:58:34  lr: 0.000010  loss: 0.3285  time: 0.9808  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 450/7110]  eta: 1:56:30  lr: 0.000010  loss: 0.2970  time: 0.9297  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 500/7110]  eta: 1:54:57  lr: 0.000010  loss: 0.3907  time: 1.0065  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 550/7110]  eta: 1:54:05  lr: 0.000010  loss: 0.1389  time: 1.0439  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 600/7110]  eta: 1:52:36  lr: 0.000010  loss: 0.2250  time: 0.9679  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 650/7110]  eta: 1:51:50  lr: 0.000010  loss: 0.0311  time: 1.0085  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 700/7110]  eta: 1:50:50  lr: 0.000010  loss: 0.3319  time: 1.0370  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 750/7110]  eta: 1:49:37  lr: 0.000010  loss: 0.0897  time: 1.0218  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 800/7110]  eta: 1:48:25  lr: 0.000010  loss: 0.4785  time: 0.9992  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 850/7110]  eta: 1:47:26  lr: 0.000010  loss: 0.5621  time: 0.9843  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 900/7110]  eta: 1:46:35  lr: 0.000010  loss: 0.2429  time: 1.0129  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [ 950/7110]  eta: 1:45:38  lr: 0.000010  loss: 0.5038  time: 0.9637  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1000/7110]  eta: 1:44:31  lr: 0.000010  loss: 0.3009  time: 0.9893  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1050/7110]  eta: 1:43:32  lr: 0.000010  loss: 0.7786  time: 1.0100  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1100/7110]  eta: 1:42:28  lr: 0.000010  loss: 0.6872  time: 1.0098  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1150/7110]  eta: 1:41:42  lr: 0.000010  loss: 0.0198  time: 1.1165  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1200/7110]  eta: 1:40:33  lr: 0.000010  loss: 0.2589  time: 0.9738  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1250/7110]  eta: 1:39:36  lr: 0.000010  loss: 0.1490  time: 0.9661  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1300/7110]  eta: 1:38:46  lr: 0.000010  loss: 0.1165  time: 1.0103  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1350/7110]  eta: 1:37:55  lr: 0.000010  loss: 0.4153  time: 0.9984  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1400/7110]  eta: 1:37:02  lr: 0.000010  loss: 0.0868  time: 0.9917  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1450/7110]  eta: 1:36:14  lr: 0.000010  loss: 0.6308  time: 1.0554  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1500/7110]  eta: 1:35:21  lr: 0.000010  loss: 0.0081  time: 1.0091  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1550/7110]  eta: 1:34:27  lr: 0.000010  loss: 0.6003  time: 1.0065  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1600/7110]  eta: 1:33:35  lr: 0.000010  loss: 0.0365  time: 1.0290  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1650/7110]  eta: 1:32:42  lr: 0.000010  loss: 0.5515  time: 1.0051  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1700/7110]  eta: 1:31:46  lr: 0.000010  loss: 0.0147  time: 1.0053  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1750/7110]  eta: 1:30:48  lr: 0.000010  loss: 0.5391  time: 0.9758  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1800/7110]  eta: 1:29:58  lr: 0.000010  loss: 0.0508  time: 1.0072  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1850/7110]  eta: 1:29:02  lr: 0.000010  loss: 0.3415  time: 0.9376  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1900/7110]  eta: 1:28:06  lr: 0.000010  loss: 0.0639  time: 0.9794  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [1950/7110]  eta: 1:27:18  lr: 0.000010  loss: 0.7558  time: 1.0057  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2000/7110]  eta: 1:26:25  lr: 0.000010  loss: 0.4377  time: 0.9717  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2050/7110]  eta: 1:25:32  lr: 0.000010  loss: 0.2746  time: 1.0163  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2100/7110]  eta: 1:24:41  lr: 0.000010  loss: 0.1409  time: 1.0361  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2150/7110]  eta: 1:23:48  lr: 0.000010  loss: 0.2123  time: 0.9550  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2200/7110]  eta: 1:22:57  lr: 0.000010  loss: 0.0452  time: 1.0172  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2250/7110]  eta: 1:22:03  lr: 0.000010  loss: 0.2223  time: 0.9368  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2300/7110]  eta: 1:21:12  lr: 0.000010  loss: 0.3932  time: 0.9651  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2350/7110]  eta: 1:20:20  lr: 0.000010  loss: 0.3372  time: 0.9675  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2400/7110]  eta: 1:19:26  lr: 0.000010  loss: 0.0230  time: 0.9906  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2450/7110]  eta: 1:18:35  lr: 0.000010  loss: 0.1071  time: 0.9895  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2500/7110]  eta: 1:17:42  lr: 0.000010  loss: 0.3399  time: 1.0356  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2550/7110]  eta: 1:16:54  lr: 0.000010  loss: 0.2493  time: 1.0304  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2600/7110]  eta: 1:16:04  lr: 0.000010  loss: 0.3565  time: 0.9975  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2650/7110]  eta: 1:15:11  lr: 0.000010  loss: 0.5643  time: 0.9729  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2700/7110]  eta: 1:14:20  lr: 0.000010  loss: 0.5687  time: 1.0208  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2750/7110]  eta: 1:13:30  lr: 0.000010  loss: 0.5572  time: 1.0287  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2800/7110]  eta: 1:12:40  lr: 0.000010  loss: 0.1897  time: 1.0139  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2850/7110]  eta: 1:11:51  lr: 0.000010  loss: 0.1430  time: 1.0312  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2900/7110]  eta: 1:10:58  lr: 0.000010  loss: 0.2527  time: 0.9468  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [2950/7110]  eta: 1:10:06  lr: 0.000010  loss: 0.5380  time: 0.9250  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3000/7110]  eta: 1:09:15  lr: 0.000010  loss: 0.1053  time: 1.0303  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3050/7110]  eta: 1:08:23  lr: 0.000010  loss: 0.2453  time: 1.0315  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3100/7110]  eta: 1:07:34  lr: 0.000010  loss: 0.1425  time: 1.0831  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3150/7110]  eta: 1:06:42  lr: 0.000010  loss: 0.0643  time: 0.9857  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3200/7110]  eta: 1:05:52  lr: 0.000010  loss: 0.1645  time: 1.0710  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3250/7110]  eta: 1:05:00  lr: 0.000010  loss: 1.5069  time: 1.0287  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3300/7110]  eta: 1:04:06  lr: 0.000010  loss: 0.2674  time: 0.9542  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3350/7110]  eta: 1:03:13  lr: 0.000010  loss: 0.2183  time: 0.9701  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3400/7110]  eta: 1:02:21  lr: 0.000010  loss: 0.1048  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3450/7110]  eta: 1:01:31  lr: 0.000010  loss: 0.0598  time: 1.0172  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3500/7110]  eta: 1:00:40  lr: 0.000010  loss: 0.0450  time: 0.9525  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3550/7110]  eta: 0:59:48  lr: 0.000010  loss: 0.0479  time: 0.9745  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3600/7110]  eta: 0:58:57  lr: 0.000010  loss: 0.1130  time: 0.9911  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3650/7110]  eta: 0:58:06  lr: 0.000010  loss: 0.2412  time: 1.0022  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3700/7110]  eta: 0:57:15  lr: 0.000010  loss: 0.3670  time: 0.9921  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3750/7110]  eta: 0:56:25  lr: 0.000010  loss: 0.1561  time: 0.9743  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3800/7110]  eta: 0:55:33  lr: 0.000010  loss: 0.2994  time: 0.9607  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3850/7110]  eta: 0:54:41  lr: 0.000010  loss: 0.2046  time: 0.9865  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3900/7110]  eta: 0:53:51  lr: 0.000010  loss: 0.3939  time: 1.0123  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [3950/7110]  eta: 0:53:01  lr: 0.000010  loss: 0.1595  time: 1.0325  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4000/7110]  eta: 0:52:11  lr: 0.000010  loss: 0.0740  time: 1.0638  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4050/7110]  eta: 0:51:20  lr: 0.000010  loss: 0.0398  time: 0.9700  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4100/7110]  eta: 0:50:29  lr: 0.000010  loss: 0.0405  time: 0.9777  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4150/7110]  eta: 0:49:38  lr: 0.000010  loss: 0.3434  time: 0.9840  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4200/7110]  eta: 0:48:49  lr: 0.000010  loss: 0.0572  time: 1.0563  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4250/7110]  eta: 0:47:58  lr: 0.000010  loss: 0.0930  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4300/7110]  eta: 0:47:07  lr: 0.000010  loss: 0.7018  time: 1.0022  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4350/7110]  eta: 0:46:16  lr: 0.000010  loss: 0.0333  time: 1.0042  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4400/7110]  eta: 0:45:25  lr: 0.000010  loss: 0.3548  time: 1.0000  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4450/7110]  eta: 0:44:34  lr: 0.000010  loss: 0.1668  time: 1.0283  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4500/7110]  eta: 0:43:43  lr: 0.000010  loss: 0.3876  time: 0.9632  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4550/7110]  eta: 0:42:53  lr: 0.000010  loss: 0.0485  time: 1.0149  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4600/7110]  eta: 0:42:02  lr: 0.000010  loss: 0.2971  time: 1.0320  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4650/7110]  eta: 0:41:11  lr: 0.000010  loss: 0.1549  time: 1.0123  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4700/7110]  eta: 0:40:20  lr: 0.000010  loss: 0.1301  time: 1.0073  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4750/7110]  eta: 0:39:29  lr: 0.000010  loss: 0.2663  time: 0.9698  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4800/7110]  eta: 0:38:39  lr: 0.000010  loss: 0.6512  time: 1.0033  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4850/7110]  eta: 0:37:49  lr: 0.000010  loss: 0.1714  time: 1.0655  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4900/7110]  eta: 0:36:59  lr: 0.000010  loss: 0.1071  time: 1.0663  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [4950/7110]  eta: 0:36:09  lr: 0.000010  loss: 0.4431  time: 1.0490  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5000/7110]  eta: 0:35:18  lr: 0.000010  loss: 0.1181  time: 0.9691  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5050/7110]  eta: 0:34:27  lr: 0.000010  loss: 0.3260  time: 0.9276  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5100/7110]  eta: 0:33:37  lr: 0.000010  loss: 0.0617  time: 1.0002  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5150/7110]  eta: 0:32:47  lr: 0.000010  loss: 0.2795  time: 0.9672  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5200/7110]  eta: 0:31:56  lr: 0.000010  loss: 0.0544  time: 0.9887  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5250/7110]  eta: 0:31:06  lr: 0.000010  loss: 0.4436  time: 1.0175  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5300/7110]  eta: 0:30:16  lr: 0.000010  loss: 0.1453  time: 1.0501  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5350/7110]  eta: 0:29:26  lr: 0.000010  loss: 0.0127  time: 1.0172  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5400/7110]  eta: 0:28:35  lr: 0.000010  loss: 0.1005  time: 0.9540  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5450/7110]  eta: 0:27:45  lr: 0.000010  loss: 0.1095  time: 1.0176  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5500/7110]  eta: 0:26:54  lr: 0.000010  loss: 0.1674  time: 0.9223  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5550/7110]  eta: 0:26:04  lr: 0.000010  loss: 0.6313  time: 0.9544  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5600/7110]  eta: 0:25:13  lr: 0.000010  loss: 0.3813  time: 0.9918  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5650/7110]  eta: 0:24:23  lr: 0.000010  loss: 0.2425  time: 1.0434  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5700/7110]  eta: 0:23:33  lr: 0.000010  loss: 0.1115  time: 0.9307  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5750/7110]  eta: 0:22:43  lr: 0.000010  loss: 1.6518  time: 1.0918  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5800/7110]  eta: 0:21:53  lr: 0.000010  loss: 0.1101  time: 0.9751  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.1506  time: 0.9959  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.5146  time: 0.9747  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.1405  time: 1.0280  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6000/7110]  eta: 0:18:32  lr: 0.000010  loss: 0.4623  time: 0.9860  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.4378  time: 0.9782  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.2251  time: 1.0893  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.2640  time: 0.9827  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 1.0871  time: 0.9986  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.2510  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.0314  time: 1.0778  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.3118  time: 1.0226  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.2030  time: 0.9997  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.3083  time: 0.9996  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.0541  time: 0.9673  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.0094  time: 1.0487  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.1278  time: 1.0145  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.0473  time: 1.0817  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.6222  time: 0.9851  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.2275  time: 1.0490  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.1207  time: 0.9258  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.3621  time: 0.9469  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.0648  time: 0.9558  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.2568  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.6389  time: 0.9220  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.6833  time: 0.9656  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 1.0918  time: 1.0135  data: 0.0000  max mem: 66110
Train: data epoch: [36]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.3038  time: 1.0907  data: 0.0000  max mem: 66110
Train: data epoch: [36] Total time: 1:58:51 (1.0031 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:22:35    time: 21.0020  data: 19.7690  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:30    time: 3.1861  data: 1.7981  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:47    time: 1.4036  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:35:02    time: 1.3133  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:06    time: 1.2972  data: 0.0009  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:17    time: 1.3772  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:22    time: 1.2770  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:39    time: 1.3145  data: 0.0009  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:54    time: 1.4287  data: 0.0009  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:25    time: 1.4361  data: 0.0009  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:12    time: 1.5388  data: 0.0009  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:47    time: 1.5425  data: 0.0009  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:06    time: 1.3727  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:48    time: 1.3894  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:09    time: 1.3551  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:40    time: 1.2509  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:19    time: 1.3501  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:50    time: 1.3163  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:30    time: 1.3145  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:20    time: 1.4844  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:58    time: 1.4529  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:54    time: 1.5248  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:33    time: 1.5351  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:10    time: 1.3044  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:51    time: 1.2986  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:35    time: 1.3899  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:26    time: 1.5374  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:01    time: 1.3878  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:43    time: 1.2343  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:34    time: 1.4985  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:11    time: 1.4117  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:52    time: 1.2181  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:36    time: 1.3256  data: 0.0009  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:17    time: 1.3157  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:04    time: 1.3810  data: 0.0009  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:49    time: 1.4571  data: 0.0009  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:31    time: 1.3219  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:14    time: 1.2721  data: 0.0011  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:02    time: 1.4447  data: 0.0011  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:47    time: 1.4738  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:33    time: 1.4241  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:17    time: 1.3990  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:03    time: 1.3913  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:48    time: 1.4014  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:29    time: 1.2311  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:14    time: 1.2421  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:00    time: 1.4004  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:44    time: 1.3559  data: 0.0009  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:30    time: 1.3666  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:16    time: 1.4731  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:05    time: 1.5513  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:52    time: 1.5950  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:39    time: 1.5454  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:25    time: 1.4951  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:11    time: 1.4758  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:56    time: 1.4324  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:39    time: 1.2568  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:24    time: 1.2336  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:10    time: 1.3726  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:53    time: 1.2885  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:40    time: 1.3608  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:26    time: 1.5044  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:12    time: 1.4167  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:57    time: 1.3798  data: 0.0009  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:42    time: 1.3487  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:29    time: 1.4134  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:14    time: 1.4104  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:00    time: 1.3882  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:46    time: 1.4462  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:33    time: 1.5176  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:18    time: 1.4644  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:03    time: 1.2811  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:47    time: 1.2133  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:31    time: 1.1316  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:16    time: 1.1372  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:02    time: 1.3069  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:48    time: 1.3878  data: 0.0009  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:34    time: 1.4046  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:20    time: 1.4835  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:05    time: 1.3251  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:52    time: 1.3852  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:38    time: 1.5318  data: 0.0009  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:24    time: 1.4478  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:11    time: 1.5148  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:56    time: 1.4555  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:42    time: 1.4182  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:28    time: 1.4000  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:14    time: 1.3497  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:00    time: 1.4910  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:46    time: 1.3644  data: 0.0009  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:31    time: 1.2505  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:17    time: 1.3949  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:03    time: 1.4249  data: 0.0009  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:49    time: 1.3644  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:35    time: 1.3994  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:21    time: 1.3969  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.3653  data: 0.0009  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4316  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:38    time: 1.3581  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.3606  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:10    time: 1.4744  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4950  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3384  data: 0.0011  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1698  data: 0.0011  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2593  data: 0.0009  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3619  data: 0.0009  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.4003  data: 0.0009  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4424  data: 0.0009  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3720  data: 0.0009  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3213  data: 0.0009  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2951  data: 0.0446  max mem: 66110
Evaluation Total time: 0:25:33 (1.4027 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_36_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [37]  [   0/7110]  eta: 2 days, 5:43:32  lr: 0.000010  loss: 0.4417  time: 27.2029  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [  50/7110]  eta: 2:56:31  lr: 0.000010  loss: 0.0222  time: 1.0257  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 100/7110]  eta: 2:25:46  lr: 0.000010  loss: 0.6861  time: 0.9758  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 150/7110]  eta: 2:15:25  lr: 0.000010  loss: 0.0897  time: 1.0055  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 200/7110]  eta: 2:08:59  lr: 0.000010  loss: 0.3955  time: 0.9980  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 250/7110]  eta: 2:05:36  lr: 0.000010  loss: 0.2880  time: 1.0205  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 300/7110]  eta: 2:02:28  lr: 0.000010  loss: 0.0974  time: 0.9868  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 350/7110]  eta: 2:00:40  lr: 0.000010  loss: 0.0444  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 400/7110]  eta: 1:58:56  lr: 0.000010  loss: 0.2988  time: 1.0299  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 450/7110]  eta: 1:57:26  lr: 0.000010  loss: 0.0659  time: 1.0105  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 500/7110]  eta: 1:55:38  lr: 0.000010  loss: 0.0630  time: 0.9791  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 550/7110]  eta: 1:54:28  lr: 0.000010  loss: 0.6164  time: 1.0269  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 600/7110]  eta: 1:53:11  lr: 0.000010  loss: 0.3631  time: 1.0047  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 650/7110]  eta: 1:52:02  lr: 0.000010  loss: 0.7495  time: 1.0090  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 700/7110]  eta: 1:50:50  lr: 0.000010  loss: 0.2181  time: 1.0025  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 750/7110]  eta: 1:49:44  lr: 0.000010  loss: 0.1719  time: 1.0351  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 800/7110]  eta: 1:48:47  lr: 0.000010  loss: 0.0370  time: 0.9806  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 850/7110]  eta: 1:47:43  lr: 0.000010  loss: 0.3030  time: 0.9740  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 900/7110]  eta: 1:46:36  lr: 0.000010  loss: 0.0652  time: 0.9634  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [ 950/7110]  eta: 1:45:37  lr: 0.000010  loss: 0.3362  time: 0.9845  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1000/7110]  eta: 1:44:21  lr: 0.000010  loss: 0.3833  time: 0.9980  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1050/7110]  eta: 1:43:27  lr: 0.000010  loss: 1.2145  time: 1.0593  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1100/7110]  eta: 1:42:24  lr: 0.000010  loss: 0.0613  time: 0.9867  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1150/7110]  eta: 1:41:18  lr: 0.000010  loss: 0.0392  time: 0.9965  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1200/7110]  eta: 1:40:26  lr: 0.000010  loss: 1.1887  time: 1.0301  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1250/7110]  eta: 1:39:30  lr: 0.000010  loss: 0.4095  time: 1.0267  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1300/7110]  eta: 1:38:29  lr: 0.000010  loss: 0.0159  time: 1.0014  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1350/7110]  eta: 1:37:29  lr: 0.000010  loss: 0.3316  time: 1.0183  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1400/7110]  eta: 1:36:33  lr: 0.000010  loss: 0.1479  time: 1.0171  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1450/7110]  eta: 1:35:42  lr: 0.000010  loss: 1.5011  time: 1.0261  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1500/7110]  eta: 1:34:50  lr: 0.000010  loss: 0.2110  time: 1.0240  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1550/7110]  eta: 1:33:54  lr: 0.000010  loss: 0.1688  time: 0.9704  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1600/7110]  eta: 1:32:55  lr: 0.000010  loss: 0.0756  time: 0.9463  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1650/7110]  eta: 1:32:03  lr: 0.000010  loss: 0.2489  time: 0.9888  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1700/7110]  eta: 1:31:13  lr: 0.000010  loss: 0.0692  time: 1.0133  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1750/7110]  eta: 1:30:25  lr: 0.000010  loss: 0.5316  time: 1.0908  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1800/7110]  eta: 1:29:35  lr: 0.000010  loss: 0.0458  time: 0.9864  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1850/7110]  eta: 1:28:38  lr: 0.000010  loss: 0.0548  time: 0.9970  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1900/7110]  eta: 1:27:46  lr: 0.000010  loss: 0.1654  time: 0.9899  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [1950/7110]  eta: 1:26:58  lr: 0.000010  loss: 0.0274  time: 1.0267  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2000/7110]  eta: 1:26:05  lr: 0.000010  loss: 0.1966  time: 1.0039  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2050/7110]  eta: 1:25:14  lr: 0.000010  loss: 0.0833  time: 0.9591  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2100/7110]  eta: 1:24:22  lr: 0.000010  loss: 0.2116  time: 1.0226  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2150/7110]  eta: 1:23:29  lr: 0.000010  loss: 0.3444  time: 1.0220  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2200/7110]  eta: 1:22:35  lr: 0.000010  loss: 0.3684  time: 1.0007  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2250/7110]  eta: 1:21:45  lr: 0.000010  loss: 0.3051  time: 1.0586  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2300/7110]  eta: 1:20:57  lr: 0.000010  loss: 0.1135  time: 1.0610  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2350/7110]  eta: 1:20:06  lr: 0.000010  loss: 0.4814  time: 0.9916  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2400/7110]  eta: 1:19:12  lr: 0.000010  loss: 0.6519  time: 0.9201  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2450/7110]  eta: 1:18:20  lr: 0.000010  loss: 0.0330  time: 0.9583  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2500/7110]  eta: 1:17:32  lr: 0.000010  loss: 0.5179  time: 1.0353  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2550/7110]  eta: 1:16:38  lr: 0.000010  loss: 0.0647  time: 0.9907  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2600/7110]  eta: 1:15:47  lr: 0.000010  loss: 0.1148  time: 0.9927  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2650/7110]  eta: 1:14:56  lr: 0.000010  loss: 0.1248  time: 0.9990  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2700/7110]  eta: 1:14:04  lr: 0.000010  loss: 0.3006  time: 0.9910  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2750/7110]  eta: 1:13:13  lr: 0.000010  loss: 0.4353  time: 0.9738  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2800/7110]  eta: 1:12:21  lr: 0.000010  loss: 0.1517  time: 0.9727  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2850/7110]  eta: 1:11:32  lr: 0.000010  loss: 0.5829  time: 1.0228  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2900/7110]  eta: 1:10:39  lr: 0.000010  loss: 0.1572  time: 0.9703  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [2950/7110]  eta: 1:09:44  lr: 0.000010  loss: 0.3270  time: 0.9603  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3000/7110]  eta: 1:08:51  lr: 0.000010  loss: 0.3745  time: 0.9947  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3050/7110]  eta: 1:08:01  lr: 0.000010  loss: 0.2695  time: 0.9789  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3100/7110]  eta: 1:07:10  lr: 0.000010  loss: 0.1076  time: 0.9804  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3150/7110]  eta: 1:06:19  lr: 0.000010  loss: 0.2642  time: 0.9895  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3200/7110]  eta: 1:05:30  lr: 0.000010  loss: 0.1935  time: 1.0491  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3250/7110]  eta: 1:04:41  lr: 0.000010  loss: 0.2777  time: 1.0240  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3300/7110]  eta: 1:03:49  lr: 0.000010  loss: 0.2830  time: 0.9926  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3350/7110]  eta: 1:02:58  lr: 0.000010  loss: 0.2926  time: 1.0122  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3400/7110]  eta: 1:02:07  lr: 0.000010  loss: 0.1379  time: 1.0479  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3450/7110]  eta: 1:01:16  lr: 0.000010  loss: 0.1886  time: 0.9745  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3500/7110]  eta: 1:00:26  lr: 0.000010  loss: 0.3831  time: 1.0081  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3550/7110]  eta: 0:59:36  lr: 0.000010  loss: 0.1123  time: 0.9708  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3600/7110]  eta: 0:58:47  lr: 0.000010  loss: 0.0188  time: 1.0222  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3650/7110]  eta: 0:57:56  lr: 0.000010  loss: 0.4131  time: 0.9850  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3700/7110]  eta: 0:57:04  lr: 0.000010  loss: 0.4333  time: 0.9803  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3750/7110]  eta: 0:56:15  lr: 0.000010  loss: 0.2248  time: 0.9980  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3800/7110]  eta: 0:55:23  lr: 0.000010  loss: 0.1420  time: 0.9561  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3850/7110]  eta: 0:54:32  lr: 0.000010  loss: 0.1376  time: 0.9793  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3900/7110]  eta: 0:53:43  lr: 0.000010  loss: 0.1967  time: 1.0245  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [3950/7110]  eta: 0:52:53  lr: 0.000010  loss: 0.0480  time: 1.0574  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4000/7110]  eta: 0:52:02  lr: 0.000010  loss: 0.1844  time: 1.0028  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4050/7110]  eta: 0:51:12  lr: 0.000010  loss: 0.1403  time: 0.9420  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4100/7110]  eta: 0:50:21  lr: 0.000010  loss: 0.0576  time: 0.9264  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4150/7110]  eta: 0:49:31  lr: 0.000010  loss: 0.4859  time: 1.0414  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4200/7110]  eta: 0:48:42  lr: 0.000010  loss: 0.2010  time: 1.0594  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4250/7110]  eta: 0:47:52  lr: 0.000010  loss: 0.0366  time: 0.9639  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4300/7110]  eta: 0:47:01  lr: 0.000010  loss: 0.8076  time: 0.9875  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4350/7110]  eta: 0:46:11  lr: 0.000010  loss: 0.1417  time: 1.0008  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4400/7110]  eta: 0:45:21  lr: 0.000010  loss: 0.1875  time: 1.0356  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4450/7110]  eta: 0:44:32  lr: 0.000010  loss: 0.4233  time: 1.0504  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4500/7110]  eta: 0:43:43  lr: 0.000010  loss: 0.3491  time: 0.9987  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4550/7110]  eta: 0:42:53  lr: 0.000010  loss: 0.3060  time: 1.0439  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4600/7110]  eta: 0:42:02  lr: 0.000010  loss: 0.3661  time: 0.9427  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4650/7110]  eta: 0:41:11  lr: 0.000010  loss: 0.4351  time: 0.9743  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4700/7110]  eta: 0:40:21  lr: 0.000010  loss: 0.1176  time: 1.0188  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4750/7110]  eta: 0:39:31  lr: 0.000010  loss: 0.3040  time: 0.9653  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4800/7110]  eta: 0:38:40  lr: 0.000010  loss: 0.0162  time: 0.9874  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4850/7110]  eta: 0:37:50  lr: 0.000010  loss: 0.0691  time: 1.0188  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4900/7110]  eta: 0:36:59  lr: 0.000010  loss: 0.1415  time: 0.9619  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [4950/7110]  eta: 0:36:08  lr: 0.000010  loss: 0.2010  time: 0.9651  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5000/7110]  eta: 0:35:18  lr: 0.000010  loss: 0.0811  time: 1.0366  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5050/7110]  eta: 0:34:27  lr: 0.000010  loss: 0.3066  time: 1.0001  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5100/7110]  eta: 0:33:38  lr: 0.000010  loss: 0.2381  time: 0.9868  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5150/7110]  eta: 0:32:47  lr: 0.000010  loss: 0.3643  time: 1.0195  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5200/7110]  eta: 0:31:57  lr: 0.000010  loss: 1.5130  time: 1.0736  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5250/7110]  eta: 0:31:08  lr: 0.000010  loss: 1.4374  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5300/7110]  eta: 0:30:17  lr: 0.000010  loss: 0.1113  time: 1.0078  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5350/7110]  eta: 0:29:27  lr: 0.000010  loss: 0.2873  time: 0.9928  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5400/7110]  eta: 0:28:36  lr: 0.000010  loss: 0.0854  time: 0.9871  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5450/7110]  eta: 0:27:46  lr: 0.000010  loss: 0.2265  time: 1.0098  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5500/7110]  eta: 0:26:55  lr: 0.000010  loss: 0.1511  time: 0.9491  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5550/7110]  eta: 0:26:05  lr: 0.000010  loss: 0.0561  time: 0.9586  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5600/7110]  eta: 0:25:14  lr: 0.000010  loss: 0.1553  time: 1.0243  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5650/7110]  eta: 0:24:24  lr: 0.000010  loss: 0.1292  time: 0.9923  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.0287  time: 0.9998  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5750/7110]  eta: 0:22:43  lr: 0.000010  loss: 0.0425  time: 0.9279  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5800/7110]  eta: 0:21:53  lr: 0.000010  loss: 0.2850  time: 0.9384  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5850/7110]  eta: 0:21:02  lr: 0.000010  loss: 0.4290  time: 0.9928  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.3985  time: 1.0320  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [5950/7110]  eta: 0:19:22  lr: 0.000010  loss: 0.2349  time: 0.9474  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6000/7110]  eta: 0:18:32  lr: 0.000010  loss: 0.0666  time: 0.9651  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.6666  time: 0.9970  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.4187  time: 1.0018  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6150/7110]  eta: 0:16:01  lr: 0.000010  loss: 0.6872  time: 0.9858  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 1.6674  time: 1.0436  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.0844  time: 1.0373  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.1084  time: 1.0113  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.1152  time: 0.9396  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.3533  time: 0.9982  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.3178  time: 0.9252  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.3338  time: 1.0190  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.0296  time: 0.9498  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.1864  time: 1.0235  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.5995  time: 0.9793  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.1716  time: 0.9826  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.2512  time: 0.9569  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.3367  time: 1.0799  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.5389  time: 0.9814  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1605  time: 1.0361  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.2136  time: 0.9634  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.2904  time: 0.9605  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1081  time: 1.0432  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.2450  time: 1.0047  data: 0.0000  max mem: 66110
Train: data epoch: [37]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0796  time: 1.1076  data: 0.0000  max mem: 66110
Train: data epoch: [37] Total time: 1:58:49 (1.0027 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:16:28    time: 20.6669  data: 19.4259  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:59    time: 3.1574  data: 1.7669  max mem: 66110
Evaluation  [  20/1093]  eta: 0:40:55    time: 1.3691  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:19    time: 1.2671  data: 0.0011  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:49    time: 1.3146  data: 0.0011  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:14    time: 1.4316  data: 0.0011  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:03    time: 1.2544  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:34    time: 1.3066  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:51    time: 1.4702  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:20    time: 1.4300  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:08    time: 1.5358  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:39    time: 1.5226  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:03    time: 1.3683  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:54    time: 1.4676  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:15    time: 1.4184  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:45    time: 1.2549  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:22    time: 1.3373  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:51    time: 1.2912  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:33    time: 1.3129  data: 0.0011  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:21    time: 1.4779  data: 0.0012  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:00    time: 1.4507  data: 0.0011  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:56    time: 1.5406  data: 0.0011  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:35    time: 1.5364  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:13    time: 1.3119  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:53    time: 1.3059  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:38    time: 1.3972  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:30    time: 1.5757  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:09    time: 1.4806  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:49    time: 1.2817  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:40    time: 1.4815  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:17    time: 1.4087  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:58    time: 1.2195  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:41    time: 1.3173  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:23    time: 1.3110  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:09    time: 1.3894  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:54    time: 1.4628  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:34    time: 1.2961  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:18    time: 1.2450  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:06    time: 1.4465  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:49    time: 1.4511  data: 0.0011  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:36    time: 1.4063  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:23    time: 1.5046  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:08    time: 1.4789  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:53    time: 1.3892  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:34    time: 1.2363  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:19    time: 1.2699  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:05    time: 1.4246  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:49    time: 1.3812  data: 0.0011  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:35    time: 1.3852  data: 0.0011  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:22    time: 1.4711  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:10    time: 1.5578  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:58    time: 1.6588  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:44    time: 1.5574  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:29    time: 1.4325  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:15    time: 1.4628  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:01    time: 1.4421  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:43    time: 1.2240  data: 0.0012  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:28    time: 1.2033  data: 0.0011  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:13    time: 1.3803  data: 0.0011  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:56    time: 1.2684  data: 0.0012  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:43    time: 1.3519  data: 0.0011  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:29    time: 1.5149  data: 0.0009  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:14    time: 1.4112  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:00    time: 1.3975  data: 0.0009  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:45    time: 1.3674  data: 0.0009  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:31    time: 1.4074  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:16    time: 1.3891  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:02    time: 1.3981  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:48    time: 1.4750  data: 0.0009  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:35    time: 1.4981  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:20    time: 1.4664  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:05    time: 1.2803  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:49    time: 1.2084  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:33    time: 1.1379  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:18    time: 1.1549  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:04    time: 1.3484  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:50    time: 1.4198  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:36    time: 1.4217  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:23    time: 1.4937  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:07    time: 1.3105  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:54    time: 1.3923  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:40    time: 1.5627  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:26    time: 1.4517  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:12    time: 1.5188  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:58    time: 1.4586  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:44    time: 1.4390  data: 0.0011  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:30    time: 1.4420  data: 0.0012  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:15    time: 1.3680  data: 0.0011  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:02    time: 1.5099  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:47    time: 1.3679  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:33    time: 1.2600  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:19    time: 1.4254  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:05    time: 1.4587  data: 0.0011  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:50    time: 1.3961  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:36    time: 1.4296  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:22    time: 1.4378  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:08    time: 1.3964  data: 0.0009  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:54    time: 1.4388  data: 0.0009  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3649  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.4064  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4940  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.4431  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:43    time: 1.2858  data: 0.0009  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1493  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2596  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3643  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3907  data: 0.0009  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4461  data: 0.0009  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.4442  data: 0.0009  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.4093  data: 0.0009  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3814  data: 0.0426  max mem: 66110
Evaluation Total time: 0:25:42 (1.4116 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_37_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [38]  [   0/7110]  eta: 704 days, 22:38:17  lr: 0.000010  loss: 0.1609  time: 8566.3994  data: 0.0001  max mem: 66110
Train: data epoch: [38]  [  50/7110]  eta: 13 days, 19:17:27  lr: 0.000010  loss: 0.0662  time: 1.0081  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 100/7110]  eta: 6 days, 23:01:39  lr: 0.000010  loss: 0.1645  time: 0.9562  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 150/7110]  eta: 4 days, 15:34:17  lr: 0.000010  loss: 0.3070  time: 1.0455  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 200/7110]  eta: 3 days, 11:41:23  lr: 0.000010  loss: 1.6377  time: 1.0194  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 250/7110]  eta: 2 days, 18:55:09  lr: 0.000010  loss: 0.3041  time: 1.0473  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 300/7110]  eta: 2 days, 7:42:39  lr: 0.000010  loss: 0.0821  time: 1.0108  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 350/7110]  eta: 1 day, 23:41:38  lr: 0.000010  loss: 0.4057  time: 1.0007  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 400/7110]  eta: 1 day, 17:40:22  lr: 0.000010  loss: 0.1329  time: 1.0249  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 450/7110]  eta: 1 day, 12:59:01  lr: 0.000010  loss: 0.8275  time: 1.0123  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 500/7110]  eta: 1 day, 9:13:33  lr: 0.000010  loss: 0.4501  time: 0.9750  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 550/7110]  eta: 1 day, 6:08:47  lr: 0.000010  loss: 0.0446  time: 0.9387  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 600/7110]  eta: 1 day, 3:34:47  lr: 0.000010  loss: 0.1125  time: 1.0107  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 650/7110]  eta: 1 day, 1:24:28  lr: 0.000010  loss: 0.1099  time: 1.0684  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 700/7110]  eta: 23:32:21  lr: 0.000010  loss: 0.2475  time: 1.0098  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 750/7110]  eta: 21:55:29  lr: 0.000010  loss: 0.0239  time: 1.0486  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 800/7110]  eta: 20:30:13  lr: 0.000010  loss: 0.2009  time: 0.9514  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 850/7110]  eta: 19:14:52  lr: 0.000010  loss: 0.2218  time: 1.0026  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 900/7110]  eta: 18:07:58  lr: 0.000010  loss: 0.3048  time: 1.0607  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [ 950/7110]  eta: 17:08:11  lr: 0.000010  loss: 0.3313  time: 1.0353  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1000/7110]  eta: 16:13:54  lr: 0.000010  loss: 0.3814  time: 1.0321  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1050/7110]  eta: 15:24:48  lr: 0.000010  loss: 0.1751  time: 0.9927  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1100/7110]  eta: 14:39:47  lr: 0.000010  loss: 0.6392  time: 0.9323  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1150/7110]  eta: 13:58:47  lr: 0.000010  loss: 0.0806  time: 1.0021  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1200/7110]  eta: 13:21:18  lr: 0.000010  loss: 0.3746  time: 1.0164  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1250/7110]  eta: 12:46:42  lr: 0.000010  loss: 0.2236  time: 1.0069  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1300/7110]  eta: 12:14:44  lr: 0.000010  loss: 0.2967  time: 1.0460  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1350/7110]  eta: 11:44:57  lr: 0.000010  loss: 1.7227  time: 1.0099  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1400/7110]  eta: 11:17:11  lr: 0.000010  loss: 0.2232  time: 0.9484  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1450/7110]  eta: 10:51:25  lr: 0.000010  loss: 0.7005  time: 0.9634  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1500/7110]  eta: 10:27:20  lr: 0.000010  loss: 0.0411  time: 1.0084  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1550/7110]  eta: 10:04:41  lr: 0.000010  loss: 0.2770  time: 0.9791  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1600/7110]  eta: 9:43:26  lr: 0.000010  loss: 0.1719  time: 1.0233  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1650/7110]  eta: 9:23:23  lr: 0.000010  loss: 0.1998  time: 0.9450  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1700/7110]  eta: 9:04:26  lr: 0.000010  loss: 0.4043  time: 0.9658  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1750/7110]  eta: 8:46:30  lr: 0.000010  loss: 0.7702  time: 0.9899  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1800/7110]  eta: 8:29:36  lr: 0.000010  loss: 0.2878  time: 1.0440  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1850/7110]  eta: 8:13:35  lr: 0.000010  loss: 0.4388  time: 1.0442  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1900/7110]  eta: 7:58:26  lr: 0.000010  loss: 0.1423  time: 1.0458  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [1950/7110]  eta: 7:43:59  lr: 0.000010  loss: 0.5663  time: 1.0268  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2000/7110]  eta: 7:30:07  lr: 0.000010  loss: 0.3020  time: 1.0097  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2050/7110]  eta: 7:16:54  lr: 0.000010  loss: 0.3881  time: 1.0088  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2100/7110]  eta: 7:04:18  lr: 0.000010  loss: 0.5357  time: 0.9999  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2150/7110]  eta: 6:52:13  lr: 0.000010  loss: 0.1658  time: 0.9888  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2200/7110]  eta: 6:40:37  lr: 0.000010  loss: 0.2050  time: 0.9522  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2250/7110]  eta: 6:29:30  lr: 0.000010  loss: 0.2936  time: 0.9492  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2300/7110]  eta: 6:18:53  lr: 0.000010  loss: 0.0708  time: 0.9918  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2350/7110]  eta: 6:08:40  lr: 0.000010  loss: 0.1237  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2400/7110]  eta: 5:58:46  lr: 0.000010  loss: 0.0858  time: 0.9443  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2450/7110]  eta: 5:49:17  lr: 0.000010  loss: 0.1813  time: 0.9689  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2500/7110]  eta: 5:40:12  lr: 0.000010  loss: 0.2034  time: 1.0108  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2550/7110]  eta: 5:31:21  lr: 0.000010  loss: 0.0923  time: 0.9446  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2600/7110]  eta: 5:22:53  lr: 0.000010  loss: 0.0998  time: 1.0081  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2650/7110]  eta: 5:14:40  lr: 0.000010  loss: 0.4362  time: 0.9944  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2700/7110]  eta: 5:06:47  lr: 0.000010  loss: 0.0979  time: 1.0630  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2750/7110]  eta: 4:59:06  lr: 0.000010  loss: 0.0518  time: 0.9729  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2800/7110]  eta: 4:51:39  lr: 0.000010  loss: 0.1142  time: 1.0092  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2850/7110]  eta: 4:44:28  lr: 0.000010  loss: 1.3802  time: 0.9894  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2900/7110]  eta: 4:37:29  lr: 0.000010  loss: 0.1168  time: 0.9810  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [2950/7110]  eta: 4:30:43  lr: 0.000010  loss: 0.0409  time: 0.9543  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3000/7110]  eta: 4:24:10  lr: 0.000010  loss: 0.2690  time: 1.0491  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3050/7110]  eta: 4:17:47  lr: 0.000010  loss: 1.8141  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3100/7110]  eta: 4:11:35  lr: 0.000010  loss: 0.3097  time: 0.9681  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3150/7110]  eta: 4:05:34  lr: 0.000010  loss: 0.2829  time: 1.0202  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3200/7110]  eta: 3:59:40  lr: 0.000010  loss: 0.1772  time: 1.0139  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3250/7110]  eta: 3:53:57  lr: 0.000010  loss: 0.2397  time: 1.0000  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3300/7110]  eta: 3:48:22  lr: 0.000010  loss: 0.1083  time: 1.0053  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3350/7110]  eta: 3:42:56  lr: 0.000010  loss: 0.1149  time: 1.0271  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3400/7110]  eta: 3:37:38  lr: 0.000010  loss: 0.4708  time: 0.9640  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3450/7110]  eta: 3:32:27  lr: 0.000010  loss: 0.1217  time: 0.9881  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3500/7110]  eta: 3:27:24  lr: 0.000010  loss: 0.1359  time: 1.0211  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3550/7110]  eta: 3:22:29  lr: 0.000010  loss: 0.1363  time: 0.9797  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3600/7110]  eta: 3:17:42  lr: 0.000010  loss: 0.2513  time: 1.0134  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3650/7110]  eta: 3:13:00  lr: 0.000010  loss: 0.2704  time: 1.0027  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3700/7110]  eta: 3:08:25  lr: 0.000010  loss: 0.5007  time: 1.0222  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3750/7110]  eta: 3:03:56  lr: 0.000010  loss: 0.3577  time: 1.0293  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3800/7110]  eta: 2:59:33  lr: 0.000010  loss: 0.3337  time: 0.9670  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3850/7110]  eta: 2:55:14  lr: 0.000010  loss: 0.1977  time: 0.9779  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3900/7110]  eta: 2:51:02  lr: 0.000010  loss: 0.4339  time: 1.0022  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [3950/7110]  eta: 2:46:54  lr: 0.000010  loss: 0.3292  time: 0.9664  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4000/7110]  eta: 2:42:53  lr: 0.000010  loss: 0.0479  time: 1.0616  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4050/7110]  eta: 2:38:54  lr: 0.000010  loss: 0.1938  time: 0.9791  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4100/7110]  eta: 2:35:00  lr: 0.000010  loss: 0.0472  time: 0.9402  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4150/7110]  eta: 2:31:11  lr: 0.000010  loss: 0.3484  time: 1.0357  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4200/7110]  eta: 2:27:27  lr: 0.000010  loss: 0.3718  time: 0.9606  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4250/7110]  eta: 2:23:47  lr: 0.000010  loss: 0.4801  time: 1.0381  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4300/7110]  eta: 2:20:11  lr: 0.000010  loss: 0.1830  time: 1.0243  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4350/7110]  eta: 2:16:38  lr: 0.000010  loss: 0.4750  time: 0.9526  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4400/7110]  eta: 2:13:09  lr: 0.000010  loss: 0.3013  time: 1.0494  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4450/7110]  eta: 2:09:44  lr: 0.000010  loss: 0.3518  time: 1.0215  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4500/7110]  eta: 2:06:22  lr: 0.000010  loss: 0.0257  time: 1.0381  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4550/7110]  eta: 2:03:02  lr: 0.000010  loss: 0.1252  time: 0.9440  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4600/7110]  eta: 1:59:45  lr: 0.000010  loss: 0.0459  time: 0.9665  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4650/7110]  eta: 1:56:34  lr: 0.000010  loss: 0.0787  time: 1.0540  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4700/7110]  eta: 1:53:25  lr: 0.000010  loss: 0.0948  time: 0.9745  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4750/7110]  eta: 1:50:18  lr: 0.000010  loss: 0.3430  time: 0.9900  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4800/7110]  eta: 1:47:14  lr: 0.000010  loss: 0.4221  time: 1.0423  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4850/7110]  eta: 1:44:13  lr: 0.000010  loss: 0.0250  time: 1.0410  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4900/7110]  eta: 1:41:15  lr: 0.000010  loss: 0.1830  time: 1.0019  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [4950/7110]  eta: 1:38:19  lr: 0.000010  loss: 0.0768  time: 0.9811  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5000/7110]  eta: 1:35:26  lr: 0.000010  loss: 0.0707  time: 1.0017  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5050/7110]  eta: 1:32:36  lr: 0.000010  loss: 0.1587  time: 0.9767  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5100/7110]  eta: 1:29:46  lr: 0.000010  loss: 0.1073  time: 0.9607  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5150/7110]  eta: 1:27:01  lr: 0.000010  loss: 0.3632  time: 1.0039  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5200/7110]  eta: 1:24:17  lr: 0.000010  loss: 0.0722  time: 1.0157  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5250/7110]  eta: 1:21:36  lr: 0.000010  loss: 0.0974  time: 1.0065  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5300/7110]  eta: 1:18:57  lr: 0.000010  loss: 0.4002  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5350/7110]  eta: 1:16:19  lr: 0.000010  loss: 0.2137  time: 1.0135  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5400/7110]  eta: 1:13:44  lr: 0.000010  loss: 0.5107  time: 1.0276  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5450/7110]  eta: 1:11:11  lr: 0.000010  loss: 0.1540  time: 0.9981  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5500/7110]  eta: 1:08:39  lr: 0.000010  loss: 0.1767  time: 1.0325  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5550/7110]  eta: 1:06:09  lr: 0.000010  loss: 0.1376  time: 0.9673  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5600/7110]  eta: 1:03:41  lr: 0.000010  loss: 0.0375  time: 1.0076  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5650/7110]  eta: 1:01:15  lr: 0.000010  loss: 0.2223  time: 1.0091  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5700/7110]  eta: 0:58:51  lr: 0.000010  loss: 1.7227  time: 1.0019  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5750/7110]  eta: 0:56:27  lr: 0.000010  loss: 0.2132  time: 0.9709  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5800/7110]  eta: 0:54:06  lr: 0.000010  loss: 0.3519  time: 1.0741  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5850/7110]  eta: 0:51:46  lr: 0.000010  loss: 0.3954  time: 1.0173  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5900/7110]  eta: 0:49:28  lr: 0.000010  loss: 0.2137  time: 0.9842  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [5950/7110]  eta: 0:47:11  lr: 0.000010  loss: 0.1014  time: 0.9670  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6000/7110]  eta: 0:44:55  lr: 0.000010  loss: 1.2216  time: 0.9461  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6050/7110]  eta: 0:42:42  lr: 0.000010  loss: 0.1882  time: 0.9727  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6100/7110]  eta: 0:40:29  lr: 0.000010  loss: 0.5229  time: 0.9831  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6150/7110]  eta: 0:38:18  lr: 0.000010  loss: 0.0890  time: 1.0590  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6200/7110]  eta: 0:36:08  lr: 0.000010  loss: 1.6446  time: 0.9956  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6250/7110]  eta: 0:33:59  lr: 0.000010  loss: 0.7213  time: 1.0128  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6300/7110]  eta: 0:31:52  lr: 0.000010  loss: 0.5000  time: 0.9948  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6350/7110]  eta: 0:29:46  lr: 0.000010  loss: 0.4380  time: 1.0303  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6400/7110]  eta: 0:27:41  lr: 0.000010  loss: 0.1725  time: 1.0302  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6450/7110]  eta: 0:25:37  lr: 0.000010  loss: 1.2836  time: 1.0790  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6500/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.3802  time: 1.0314  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6550/7110]  eta: 0:21:33  lr: 0.000010  loss: 0.2749  time: 1.0265  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6600/7110]  eta: 0:19:32  lr: 0.000010  loss: 0.1351  time: 1.0248  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6650/7110]  eta: 0:17:33  lr: 0.000010  loss: 1.6507  time: 0.9421  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6700/7110]  eta: 0:15:34  lr: 0.000010  loss: 0.2455  time: 0.9840  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6750/7110]  eta: 0:13:37  lr: 0.000010  loss: 0.2354  time: 0.9631  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6800/7110]  eta: 0:11:40  lr: 0.000010  loss: 0.2146  time: 0.9959  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6850/7110]  eta: 0:09:45  lr: 0.000010  loss: 0.1455  time: 0.9527  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6900/7110]  eta: 0:07:50  lr: 0.000010  loss: 0.3731  time: 0.9987  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [6950/7110]  eta: 0:05:57  lr: 0.000010  loss: 0.0895  time: 0.9935  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [7000/7110]  eta: 0:04:04  lr: 0.000010  loss: 0.1984  time: 0.9767  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [7050/7110]  eta: 0:02:12  lr: 0.000010  loss: 0.2941  time: 1.0571  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [7100/7110]  eta: 0:00:22  lr: 0.000010  loss: 0.3680  time: 1.0035  data: 0.0000  max mem: 66110
Train: data epoch: [38]  [7109/7110]  eta: 0:00:02  lr: 0.000010  loss: 0.8429  time: 1.0859  data: 0.0000  max mem: 66110
Train: data epoch: [38] Total time: 4:21:23 (2.2058 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 8:06:01    time: 26.6807  data: 25.4158  max mem: 66110
Evaluation  [  10/1093]  eta: 1:06:57    time: 3.7098  data: 2.3114  max mem: 66110
Evaluation  [  20/1093]  eta: 0:47:07    time: 1.4331  data: 0.0009  max mem: 66110
Evaluation  [  30/1093]  eta: 0:38:41    time: 1.3451  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:34:57    time: 1.3167  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:32:43    time: 1.4158  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:30:24    time: 1.3023  data: 0.0009  max mem: 66110
Evaluation  [  70/1093]  eta: 0:29:34    time: 1.3566  data: 0.0009  max mem: 66110
Evaluation  [  80/1093]  eta: 0:28:35    time: 1.4734  data: 0.0009  max mem: 66110
Evaluation  [  90/1093]  eta: 0:27:52    time: 1.4284  data: 0.0009  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:27:28    time: 1.5248  data: 0.0009  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:26:56    time: 1.5383  data: 0.0009  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:26:15    time: 1.4107  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:25:57    time: 1.4718  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:25:13    time: 1.4001  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:24:40    time: 1.2542  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:24:13    time: 1.3418  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:23:43    time: 1.3338  data: 0.0009  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:23:21    time: 1.3515  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:23:07    time: 1.4896  data: 0.0009  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:44    time: 1.4725  data: 0.0009  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:22:37    time: 1.5439  data: 0.0009  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:22:14    time: 1.5279  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:49    time: 1.3100  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:21:27    time: 1.3056  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:21:12    time: 1.4090  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:21:00    time: 1.5587  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:35    time: 1.4115  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:20:15    time: 1.2604  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:20:05    time: 1.4974  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:41    time: 1.4128  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:19:20    time: 1.2012  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:19:04    time: 1.3429  data: 0.0009  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:46    time: 1.3789  data: 0.0009  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:32    time: 1.4106  data: 0.0009  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:18:15    time: 1.4615  data: 0.0009  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:55    time: 1.2939  data: 0.0009  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:38    time: 1.2624  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:25    time: 1.4611  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:17:09    time: 1.4829  data: 0.0009  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:55    time: 1.4343  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:40    time: 1.4797  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:24    time: 1.4322  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:16:08    time: 1.3650  data: 0.0009  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:47    time: 1.2063  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:32    time: 1.2363  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:17    time: 1.4038  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:15:01    time: 1.3656  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:47    time: 1.4143  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:33    time: 1.4992  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:21    time: 1.5612  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:14:09    time: 1.6547  data: 0.0011  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:55    time: 1.6109  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:41    time: 1.4897  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:26    time: 1.4610  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:11    time: 1.4157  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:53    time: 1.2481  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:37    time: 1.2328  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:23    time: 1.3818  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:12:06    time: 1.2882  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:53    time: 1.3773  data: 0.0009  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:38    time: 1.5357  data: 0.0009  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:23    time: 1.4067  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:08    time: 1.3612  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:52    time: 1.3271  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:38    time: 1.3675  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:23    time: 1.3619  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:08    time: 1.3473  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:54    time: 1.4147  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:40    time: 1.5148  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:26    time: 1.5083  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:10    time: 1.2866  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:54    time: 1.1930  data: 0.0009  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:38    time: 1.1410  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:23    time: 1.1873  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:09    time: 1.3590  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:54    time: 1.3856  data: 0.0009  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:40    time: 1.3739  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:26    time: 1.4509  data: 0.0009  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:10    time: 1.2931  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:57    time: 1.3640  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:43    time: 1.5498  data: 0.0009  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:29    time: 1.4583  data: 0.0009  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:15    time: 1.5195  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:06:00    time: 1.4564  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:46    time: 1.4366  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:32    time: 1.4211  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:17    time: 1.3569  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:04    time: 1.4946  data: 0.0009  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:49    time: 1.3484  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:34    time: 1.2629  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:20    time: 1.4233  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:06    time: 1.4444  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:52    time: 1.3818  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:37    time: 1.4092  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:23    time: 1.4225  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:09    time: 1.4012  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:55    time: 1.4434  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:40    time: 1.3612  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:26    time: 1.4241  data: 0.0009  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:12    time: 1.5170  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:58    time: 1.4801  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:43    time: 1.3555  data: 0.0009  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:29    time: 1.2059  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:15    time: 1.2795  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:01    time: 1.3562  data: 0.0009  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3894  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4403  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3853  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3509  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3226  data: 0.0483  max mem: 66110
Evaluation Total time: 0:25:50 (1.4186 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_38_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [39]  [   0/7110]  eta: 2 days, 5:19:35  lr: 0.000010  loss: 0.3250  time: 27.0008  data: 0.0001  max mem: 66110
Train: data epoch: [39]  [  50/7110]  eta: 3:01:20  lr: 0.000010  loss: 0.2029  time: 1.0252  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 100/7110]  eta: 2:27:55  lr: 0.000010  loss: 0.1688  time: 0.9830  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 150/7110]  eta: 2:15:04  lr: 0.000010  loss: 0.0767  time: 0.9225  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 200/7110]  eta: 2:09:20  lr: 0.000010  loss: 0.1811  time: 0.9332  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 250/7110]  eta: 2:05:15  lr: 0.000010  loss: 0.2482  time: 0.9579  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 300/7110]  eta: 2:02:09  lr: 0.000010  loss: 0.2160  time: 0.9589  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 350/7110]  eta: 1:59:57  lr: 0.000010  loss: 0.0181  time: 0.9997  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 400/7110]  eta: 1:57:56  lr: 0.000010  loss: 0.3325  time: 1.0383  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 450/7110]  eta: 1:56:19  lr: 0.000010  loss: 0.0691  time: 0.9761  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 500/7110]  eta: 1:54:59  lr: 0.000010  loss: 0.4892  time: 1.0085  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 550/7110]  eta: 1:53:40  lr: 0.000010  loss: 0.3494  time: 1.0434  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 600/7110]  eta: 1:52:36  lr: 0.000010  loss: 0.3087  time: 1.0344  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 650/7110]  eta: 1:51:30  lr: 0.000010  loss: 0.0718  time: 1.0014  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 700/7110]  eta: 1:50:16  lr: 0.000010  loss: 0.0547  time: 1.0258  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 750/7110]  eta: 1:49:04  lr: 0.000010  loss: 0.1007  time: 0.9564  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 800/7110]  eta: 1:48:03  lr: 0.000010  loss: 0.1453  time: 1.0171  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 850/7110]  eta: 1:47:06  lr: 0.000010  loss: 0.0630  time: 1.0018  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 900/7110]  eta: 1:46:10  lr: 0.000010  loss: 0.4429  time: 1.0104  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [ 950/7110]  eta: 1:45:14  lr: 0.000010  loss: 0.2153  time: 0.9671  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1000/7110]  eta: 1:44:13  lr: 0.000010  loss: 0.0601  time: 0.9518  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1050/7110]  eta: 1:43:18  lr: 0.000010  loss: 0.0701  time: 1.0288  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1100/7110]  eta: 1:42:25  lr: 0.000010  loss: 0.3160  time: 1.0275  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1150/7110]  eta: 1:41:32  lr: 0.000010  loss: 0.3248  time: 0.9940  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1200/7110]  eta: 1:40:34  lr: 0.000010  loss: 0.3329  time: 1.0187  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1250/7110]  eta: 1:39:30  lr: 0.000010  loss: 0.0618  time: 0.9488  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1300/7110]  eta: 1:38:20  lr: 0.000010  loss: 0.1524  time: 0.9751  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1350/7110]  eta: 1:37:21  lr: 0.000010  loss: 0.1619  time: 0.9845  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1400/7110]  eta: 1:36:29  lr: 0.000010  loss: 0.6288  time: 1.0391  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1450/7110]  eta: 1:35:38  lr: 0.000010  loss: 0.2141  time: 0.9600  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1500/7110]  eta: 1:34:42  lr: 0.000010  loss: 0.7543  time: 0.9462  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1550/7110]  eta: 1:33:49  lr: 0.000010  loss: 0.1109  time: 1.0438  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1600/7110]  eta: 1:33:01  lr: 0.000010  loss: 0.2050  time: 0.9937  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1650/7110]  eta: 1:32:12  lr: 0.000010  loss: 0.0881  time: 1.0314  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1700/7110]  eta: 1:31:32  lr: 0.000010  loss: 0.0295  time: 1.0568  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1750/7110]  eta: 1:30:37  lr: 0.000010  loss: 0.3289  time: 0.9670  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1800/7110]  eta: 1:29:46  lr: 0.000010  loss: 0.0322  time: 0.9850  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1850/7110]  eta: 1:28:54  lr: 0.000010  loss: 0.2026  time: 0.9891  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1900/7110]  eta: 1:27:58  lr: 0.000010  loss: 0.1604  time: 1.0301  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [1950/7110]  eta: 1:27:11  lr: 0.000010  loss: 0.0989  time: 1.0029  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2000/7110]  eta: 1:26:16  lr: 0.000010  loss: 0.1192  time: 0.9820  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2050/7110]  eta: 1:25:20  lr: 0.000010  loss: 0.0432  time: 0.9691  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2100/7110]  eta: 1:24:22  lr: 0.000010  loss: 0.1145  time: 0.9054  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2150/7110]  eta: 1:23:25  lr: 0.000010  loss: 0.0882  time: 1.0047  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2200/7110]  eta: 1:22:29  lr: 0.000010  loss: 0.6195  time: 0.9869  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2250/7110]  eta: 1:21:36  lr: 0.000010  loss: 0.4260  time: 0.9738  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2300/7110]  eta: 1:20:44  lr: 0.000010  loss: 1.6459  time: 1.0383  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2350/7110]  eta: 1:19:53  lr: 0.000010  loss: 0.3189  time: 0.9685  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2400/7110]  eta: 1:19:05  lr: 0.000010  loss: 0.2027  time: 1.0613  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2450/7110]  eta: 1:18:14  lr: 0.000010  loss: 0.0898  time: 1.0222  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2500/7110]  eta: 1:17:24  lr: 0.000010  loss: 0.1224  time: 1.0555  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2550/7110]  eta: 1:16:33  lr: 0.000010  loss: 0.4757  time: 1.0012  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2600/7110]  eta: 1:15:43  lr: 0.000010  loss: 0.7515  time: 1.0288  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2650/7110]  eta: 1:14:51  lr: 0.000010  loss: 0.1170  time: 0.9803  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2700/7110]  eta: 1:14:00  lr: 0.000010  loss: 0.3672  time: 0.9720  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2750/7110]  eta: 1:13:07  lr: 0.000010  loss: 0.1552  time: 0.9270  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2800/7110]  eta: 1:12:18  lr: 0.000010  loss: 0.7444  time: 0.9876  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2850/7110]  eta: 1:11:30  lr: 0.000010  loss: 0.2826  time: 1.0271  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2900/7110]  eta: 1:10:39  lr: 0.000010  loss: 0.0442  time: 0.9929  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [2950/7110]  eta: 1:09:49  lr: 0.000010  loss: 1.1767  time: 0.9717  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3000/7110]  eta: 1:09:00  lr: 0.000010  loss: 0.5095  time: 1.0234  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3050/7110]  eta: 1:08:07  lr: 0.000010  loss: 0.1107  time: 0.9875  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3100/7110]  eta: 1:07:15  lr: 0.000010  loss: 0.3296  time: 0.9817  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3150/7110]  eta: 1:06:25  lr: 0.000010  loss: 0.2048  time: 1.0446  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3200/7110]  eta: 1:05:32  lr: 0.000010  loss: 0.4389  time: 0.9854  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3250/7110]  eta: 1:04:40  lr: 0.000010  loss: 0.5081  time: 0.9808  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3300/7110]  eta: 1:03:50  lr: 0.000010  loss: 0.7337  time: 1.0487  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3350/7110]  eta: 1:03:00  lr: 0.000010  loss: 0.4547  time: 0.9818  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3400/7110]  eta: 1:02:11  lr: 0.000010  loss: 0.7754  time: 1.0421  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3450/7110]  eta: 1:01:21  lr: 0.000010  loss: 0.4446  time: 1.0079  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3500/7110]  eta: 1:00:31  lr: 0.000010  loss: 0.2050  time: 1.0354  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3550/7110]  eta: 0:59:42  lr: 0.000010  loss: 0.3336  time: 1.0530  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3600/7110]  eta: 0:58:51  lr: 0.000010  loss: 0.2311  time: 0.9489  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3650/7110]  eta: 0:58:01  lr: 0.000010  loss: 0.1437  time: 1.0159  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3700/7110]  eta: 0:57:11  lr: 0.000010  loss: 0.0435  time: 0.9928  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3750/7110]  eta: 0:56:19  lr: 0.000010  loss: 0.2764  time: 0.9770  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3800/7110]  eta: 0:55:28  lr: 0.000010  loss: 0.2933  time: 1.0071  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3850/7110]  eta: 0:54:37  lr: 0.000010  loss: 0.2276  time: 0.9790  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3900/7110]  eta: 0:53:46  lr: 0.000010  loss: 0.1107  time: 0.9878  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [3950/7110]  eta: 0:52:55  lr: 0.000010  loss: 0.2398  time: 0.9954  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4000/7110]  eta: 0:52:04  lr: 0.000010  loss: 0.1833  time: 0.9604  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4050/7110]  eta: 0:51:15  lr: 0.000010  loss: 0.1121  time: 1.0662  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4100/7110]  eta: 0:50:24  lr: 0.000010  loss: 0.2311  time: 0.9417  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4150/7110]  eta: 0:49:33  lr: 0.000010  loss: 0.9198  time: 1.0018  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4200/7110]  eta: 0:48:42  lr: 0.000010  loss: 0.2072  time: 1.0193  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4250/7110]  eta: 0:47:52  lr: 0.000010  loss: 1.5464  time: 1.0022  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4300/7110]  eta: 0:47:00  lr: 0.000010  loss: 0.1951  time: 0.9410  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4350/7110]  eta: 0:46:11  lr: 0.000010  loss: 0.4846  time: 0.9943  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4400/7110]  eta: 0:45:21  lr: 0.000010  loss: 0.2292  time: 0.9674  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4450/7110]  eta: 0:44:31  lr: 0.000010  loss: 0.0556  time: 0.9738  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4500/7110]  eta: 0:43:40  lr: 0.000010  loss: 0.1861  time: 0.9356  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4550/7110]  eta: 0:42:49  lr: 0.000010  loss: 0.1550  time: 1.0569  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4600/7110]  eta: 0:41:59  lr: 0.000010  loss: 0.0547  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4650/7110]  eta: 0:41:08  lr: 0.000010  loss: 0.0316  time: 0.9744  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4700/7110]  eta: 0:40:17  lr: 0.000010  loss: 0.1419  time: 0.9801  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4750/7110]  eta: 0:39:27  lr: 0.000010  loss: 0.1875  time: 0.9651  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4800/7110]  eta: 0:38:38  lr: 0.000010  loss: 0.1341  time: 1.0444  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4850/7110]  eta: 0:37:47  lr: 0.000010  loss: 0.2156  time: 0.9475  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4900/7110]  eta: 0:36:58  lr: 0.000010  loss: 0.4439  time: 1.0523  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [4950/7110]  eta: 0:36:07  lr: 0.000010  loss: 0.1670  time: 0.9892  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5000/7110]  eta: 0:35:17  lr: 0.000010  loss: 0.0167  time: 0.9734  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5050/7110]  eta: 0:34:26  lr: 0.000010  loss: 0.2512  time: 0.9511  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5100/7110]  eta: 0:33:36  lr: 0.000010  loss: 0.5945  time: 0.9355  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5150/7110]  eta: 0:32:46  lr: 0.000010  loss: 0.0432  time: 1.0458  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5200/7110]  eta: 0:31:56  lr: 0.000010  loss: 0.2278  time: 1.0174  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5250/7110]  eta: 0:31:06  lr: 0.000010  loss: 0.0242  time: 1.0485  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5300/7110]  eta: 0:30:16  lr: 0.000010  loss: 0.2653  time: 1.0545  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5350/7110]  eta: 0:29:26  lr: 0.000010  loss: 0.4727  time: 1.0338  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5400/7110]  eta: 0:28:36  lr: 0.000010  loss: 0.2879  time: 1.0192  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5450/7110]  eta: 0:27:46  lr: 0.000010  loss: 0.4594  time: 0.9861  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5500/7110]  eta: 0:26:55  lr: 0.000010  loss: 0.0588  time: 0.9845  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5550/7110]  eta: 0:26:05  lr: 0.000010  loss: 0.0983  time: 0.9930  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5600/7110]  eta: 0:25:15  lr: 0.000010  loss: 0.0573  time: 0.9272  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5650/7110]  eta: 0:24:24  lr: 0.000010  loss: 0.2527  time: 0.9626  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.1588  time: 1.0301  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.0205  time: 1.0051  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.0413  time: 1.0564  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 0.4092  time: 1.0393  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.1915  time: 1.0631  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [5950/7110]  eta: 0:19:24  lr: 0.000010  loss: 0.1223  time: 1.0135  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.2040  time: 0.9288  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.3513  time: 0.9858  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.0600  time: 1.0049  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.1046  time: 0.9792  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.0264  time: 0.9804  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.1896  time: 0.9855  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.1461  time: 1.0083  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.1586  time: 1.0331  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.0697  time: 0.9854  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.2024  time: 0.9429  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.6761  time: 1.0167  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.3616  time: 0.9887  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.2522  time: 1.0436  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1602  time: 0.9655  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.7843  time: 0.9901  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.0585  time: 1.0807  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.4414  time: 0.9983  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.3212  time: 1.0180  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.6804  time: 1.0098  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1175  time: 0.9803  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.0314  time: 1.0452  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.5875  time: 0.9630  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.0519  time: 1.0334  data: 0.0000  max mem: 66110
Train: data epoch: [39]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.6574  time: 1.1871  data: 0.0000  max mem: 66110
Train: data epoch: [39] Total time: 1:58:57 (1.0039 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 8:02:54    time: 26.5089  data: 25.2552  max mem: 66110
Evaluation  [  10/1093]  eta: 1:06:37    time: 3.6914  data: 2.2968  max mem: 66110
Evaluation  [  20/1093]  eta: 0:46:37    time: 1.4118  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:38:11    time: 1.3104  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:34:32    time: 1.2983  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:32:18    time: 1.3988  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:29:46    time: 1.2403  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:28:51    time: 1.2702  data: 0.0009  max mem: 66110
Evaluation  [  80/1093]  eta: 0:28:05    time: 1.4644  data: 0.0009  max mem: 66110
Evaluation  [  90/1093]  eta: 0:27:28    time: 1.4668  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:27:07    time: 1.5357  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:26:35    time: 1.5315  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:52    time: 1.3759  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:25:31    time: 1.4080  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:53    time: 1.3994  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:24:21    time: 1.2826  data: 0.0011  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:57    time: 1.3513  data: 0.0011  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:23:25    time: 1.3189  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:23:04    time: 1.3227  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:55    time: 1.5272  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:32    time: 1.5004  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:22:26    time: 1.5398  data: 0.0011  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:22:03    time: 1.5254  data: 0.0011  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:38    time: 1.2886  data: 0.0011  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:21:15    time: 1.2612  data: 0.0012  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:58    time: 1.3667  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:50    time: 1.5758  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:27    time: 1.4754  data: 0.0009  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:20:05    time: 1.2384  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:55    time: 1.4476  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:31    time: 1.4124  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:19:11    time: 1.1973  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:51    time: 1.2650  data: 0.0009  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:32    time: 1.2767  data: 0.0009  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:19    time: 1.3868  data: 0.0009  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:18:03    time: 1.4620  data: 0.0009  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:43    time: 1.2910  data: 0.0009  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:25    time: 1.2221  data: 0.0009  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:13    time: 1.4249  data: 0.0009  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:57    time: 1.4842  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:43    time: 1.4378  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:27    time: 1.4026  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:12    time: 1.3787  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:57    time: 1.4012  data: 0.0018  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:38    time: 1.2636  data: 0.0016  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:22    time: 1.2558  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:08    time: 1.3951  data: 0.0009  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:52    time: 1.3662  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:38    time: 1.3702  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:24    time: 1.4700  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:13    time: 1.5833  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:14:00    time: 1.6131  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:45    time: 1.4992  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:31    time: 1.4465  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:16    time: 1.4367  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:01    time: 1.4029  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:43    time: 1.2115  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:27    time: 1.1499  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:12    time: 1.3187  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:55    time: 1.2424  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:42    time: 1.3450  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:28    time: 1.5217  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:14    time: 1.4164  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:59    time: 1.3854  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:44    time: 1.3488  data: 0.0009  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:30    time: 1.3689  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:15    time: 1.3866  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:02    time: 1.4377  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:48    time: 1.4949  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:34    time: 1.5198  data: 0.0012  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:20    time: 1.4602  data: 0.0011  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:05    time: 1.2980  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:49    time: 1.2236  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:33    time: 1.1345  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:19    time: 1.2044  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:05    time: 1.3827  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:50    time: 1.4122  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:36    time: 1.3917  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:22    time: 1.4718  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:07    time: 1.2874  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:54    time: 1.3474  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:40    time: 1.5491  data: 0.0009  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:26    time: 1.4498  data: 0.0009  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:12    time: 1.5176  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:58    time: 1.4745  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:44    time: 1.4291  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:29    time: 1.4088  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:15    time: 1.3829  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:01    time: 1.5041  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:47    time: 1.3681  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:33    time: 1.2789  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:18    time: 1.4186  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:04    time: 1.4455  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:50    time: 1.3763  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:36    time: 1.4084  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:22    time: 1.4367  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:08    time: 1.4095  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:54    time: 1.4499  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3590  data: 0.0009  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3593  data: 0.0009  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4567  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.4733  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:43    time: 1.3366  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1577  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2530  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3546  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3869  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4431  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3743  data: 0.0009  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3428  data: 0.0009  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3168  data: 0.0419  max mem: 66110
Evaluation Total time: 0:25:39 (1.4088 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_39_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [40]  [   0/7110]  eta: 2 days, 7:30:24  lr: 0.000010  loss: 0.1461  time: 28.1047  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [  50/7110]  eta: 2:58:38  lr: 0.000010  loss: 0.1448  time: 1.0091  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 100/7110]  eta: 2:26:23  lr: 0.000010  loss: 0.0500  time: 0.9931  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 150/7110]  eta: 2:16:12  lr: 0.000010  loss: 0.1764  time: 1.0315  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 200/7110]  eta: 2:10:37  lr: 0.000010  loss: 0.3564  time: 1.0114  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 250/7110]  eta: 2:07:13  lr: 0.000010  loss: 0.2063  time: 0.9886  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 300/7110]  eta: 2:03:54  lr: 0.000010  loss: 0.2610  time: 1.0419  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 350/7110]  eta: 2:01:39  lr: 0.000010  loss: 0.0671  time: 1.0758  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 400/7110]  eta: 1:59:39  lr: 0.000010  loss: 0.1903  time: 1.0606  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 450/7110]  eta: 1:57:52  lr: 0.000010  loss: 0.3867  time: 0.9895  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 500/7110]  eta: 1:56:35  lr: 0.000010  loss: 0.4863  time: 1.0073  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 550/7110]  eta: 1:55:00  lr: 0.000010  loss: 0.3851  time: 0.9746  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 600/7110]  eta: 1:53:55  lr: 0.000010  loss: 0.2356  time: 1.0321  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 650/7110]  eta: 1:52:51  lr: 0.000010  loss: 0.4699  time: 1.0722  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 700/7110]  eta: 1:51:43  lr: 0.000010  loss: 0.0121  time: 0.9877  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 750/7110]  eta: 1:50:34  lr: 0.000010  loss: 0.7058  time: 1.0115  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 800/7110]  eta: 1:49:23  lr: 0.000010  loss: 0.0203  time: 0.9665  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 850/7110]  eta: 1:48:06  lr: 0.000010  loss: 0.0605  time: 0.9423  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 900/7110]  eta: 1:47:04  lr: 0.000010  loss: 0.1373  time: 0.9752  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [ 950/7110]  eta: 1:46:00  lr: 0.000010  loss: 0.1309  time: 1.0200  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1000/7110]  eta: 1:45:00  lr: 0.000010  loss: 0.1597  time: 1.0236  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1050/7110]  eta: 1:43:49  lr: 0.000010  loss: 0.1838  time: 0.9710  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1100/7110]  eta: 1:42:48  lr: 0.000010  loss: 0.2451  time: 1.0063  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1150/7110]  eta: 1:42:01  lr: 0.000010  loss: 0.1553  time: 1.0275  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1200/7110]  eta: 1:40:53  lr: 0.000010  loss: 0.0644  time: 0.9208  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1250/7110]  eta: 1:39:51  lr: 0.000010  loss: 0.0375  time: 1.0086  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1300/7110]  eta: 1:38:46  lr: 0.000010  loss: 0.1290  time: 0.9803  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1350/7110]  eta: 1:37:47  lr: 0.000010  loss: 0.0577  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1400/7110]  eta: 1:36:55  lr: 0.000010  loss: 0.0521  time: 1.0168  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1450/7110]  eta: 1:35:58  lr: 0.000010  loss: 0.0241  time: 0.9404  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1500/7110]  eta: 1:35:10  lr: 0.000010  loss: 0.1323  time: 1.0133  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1550/7110]  eta: 1:34:22  lr: 0.000010  loss: 0.3346  time: 1.0021  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1600/7110]  eta: 1:33:30  lr: 0.000010  loss: 0.1852  time: 0.9801  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1650/7110]  eta: 1:32:38  lr: 0.000010  loss: 0.2139  time: 1.0472  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1700/7110]  eta: 1:31:52  lr: 0.000010  loss: 0.0517  time: 1.0812  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1750/7110]  eta: 1:30:59  lr: 0.000010  loss: 0.6899  time: 1.0172  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1800/7110]  eta: 1:30:02  lr: 0.000010  loss: 0.2088  time: 0.9588  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1850/7110]  eta: 1:29:10  lr: 0.000010  loss: 0.1842  time: 0.9774  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1900/7110]  eta: 1:28:16  lr: 0.000010  loss: 0.0036  time: 1.0132  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [1950/7110]  eta: 1:27:23  lr: 0.000010  loss: 0.0837  time: 1.0026  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2000/7110]  eta: 1:26:32  lr: 0.000010  loss: 0.2840  time: 0.9984  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2050/7110]  eta: 1:25:34  lr: 0.000010  loss: 0.2318  time: 0.9602  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2100/7110]  eta: 1:24:42  lr: 0.000010  loss: 0.2961  time: 0.9665  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2150/7110]  eta: 1:23:46  lr: 0.000010  loss: 0.0925  time: 0.9413  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2200/7110]  eta: 1:22:58  lr: 0.000010  loss: 0.1816  time: 1.0336  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2250/7110]  eta: 1:22:14  lr: 0.000010  loss: 0.4933  time: 1.0754  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2300/7110]  eta: 1:21:24  lr: 0.000010  loss: 0.0578  time: 1.0320  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2350/7110]  eta: 1:20:31  lr: 0.000010  loss: 0.1834  time: 0.9648  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2400/7110]  eta: 1:19:36  lr: 0.000010  loss: 0.3013  time: 0.9691  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2450/7110]  eta: 1:18:44  lr: 0.000010  loss: 0.0247  time: 0.9894  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2500/7110]  eta: 1:17:49  lr: 0.000010  loss: 0.1195  time: 0.9705  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2550/7110]  eta: 1:16:55  lr: 0.000010  loss: 0.1299  time: 0.9958  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2600/7110]  eta: 1:16:09  lr: 0.000010  loss: 0.1595  time: 1.0100  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2650/7110]  eta: 1:15:14  lr: 0.000010  loss: 0.2336  time: 0.9760  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2700/7110]  eta: 1:14:19  lr: 0.000010  loss: 0.0398  time: 0.9546  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2750/7110]  eta: 1:13:27  lr: 0.000010  loss: 0.2958  time: 1.0033  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2800/7110]  eta: 1:12:33  lr: 0.000010  loss: 0.2471  time: 0.9772  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2850/7110]  eta: 1:11:40  lr: 0.000010  loss: 0.3665  time: 0.9436  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2900/7110]  eta: 1:10:50  lr: 0.000010  loss: 0.0595  time: 0.9993  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [2950/7110]  eta: 1:09:58  lr: 0.000010  loss: 0.6062  time: 0.9662  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3000/7110]  eta: 1:09:08  lr: 0.000010  loss: 0.0529  time: 0.9878  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3050/7110]  eta: 1:08:17  lr: 0.000010  loss: 0.1769  time: 0.9683  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3100/7110]  eta: 1:07:23  lr: 0.000010  loss: 0.0994  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3150/7110]  eta: 1:06:32  lr: 0.000010  loss: 0.0463  time: 0.9712  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3200/7110]  eta: 1:05:41  lr: 0.000010  loss: 0.3729  time: 1.0190  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3250/7110]  eta: 1:04:54  lr: 0.000010  loss: 0.3194  time: 1.0942  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3300/7110]  eta: 1:04:00  lr: 0.000010  loss: 0.3644  time: 0.9665  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3350/7110]  eta: 1:03:10  lr: 0.000010  loss: 0.0280  time: 0.9670  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3400/7110]  eta: 1:02:19  lr: 0.000010  loss: 0.4294  time: 0.9751  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3450/7110]  eta: 1:01:28  lr: 0.000010  loss: 0.1265  time: 0.9880  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3500/7110]  eta: 1:00:37  lr: 0.000010  loss: 0.0497  time: 0.9777  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3550/7110]  eta: 0:59:46  lr: 0.000010  loss: 0.1856  time: 1.0169  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3600/7110]  eta: 0:58:57  lr: 0.000010  loss: 0.2354  time: 1.0819  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3650/7110]  eta: 0:58:06  lr: 0.000010  loss: 0.0144  time: 0.9707  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3700/7110]  eta: 0:57:15  lr: 0.000010  loss: 0.1546  time: 1.0043  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3750/7110]  eta: 0:56:24  lr: 0.000010  loss: 0.2241  time: 0.9562  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3800/7110]  eta: 0:55:35  lr: 0.000010  loss: 0.0813  time: 1.0534  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3850/7110]  eta: 0:54:45  lr: 0.000010  loss: 0.1776  time: 1.0366  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3900/7110]  eta: 0:53:53  lr: 0.000010  loss: 0.3586  time: 0.9910  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [3950/7110]  eta: 0:53:04  lr: 0.000010  loss: 0.0128  time: 1.0470  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4000/7110]  eta: 0:52:13  lr: 0.000010  loss: 0.3196  time: 0.9895  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4050/7110]  eta: 0:51:23  lr: 0.000010  loss: 1.5998  time: 1.0326  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4100/7110]  eta: 0:50:34  lr: 0.000010  loss: 1.6914  time: 1.0006  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4150/7110]  eta: 0:49:43  lr: 0.000010  loss: 0.0950  time: 0.9542  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4200/7110]  eta: 0:48:51  lr: 0.000010  loss: 0.0136  time: 0.9555  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4250/7110]  eta: 0:48:02  lr: 0.000010  loss: 0.6238  time: 0.9870  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4300/7110]  eta: 0:47:12  lr: 0.000010  loss: 0.0692  time: 0.9695  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4350/7110]  eta: 0:46:21  lr: 0.000010  loss: 0.2226  time: 1.0147  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4400/7110]  eta: 0:45:31  lr: 0.000010  loss: 0.3679  time: 1.0370  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4450/7110]  eta: 0:44:39  lr: 0.000010  loss: 0.2008  time: 0.9219  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4500/7110]  eta: 0:43:48  lr: 0.000010  loss: 0.1089  time: 0.9792  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4550/7110]  eta: 0:42:57  lr: 0.000010  loss: 0.5404  time: 1.0155  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4600/7110]  eta: 0:42:06  lr: 0.000010  loss: 0.2956  time: 0.9335  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4650/7110]  eta: 0:41:17  lr: 0.000010  loss: 0.0932  time: 1.0937  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4700/7110]  eta: 0:40:26  lr: 0.000010  loss: 0.3699  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4750/7110]  eta: 0:39:37  lr: 0.000010  loss: 0.1686  time: 1.0607  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4800/7110]  eta: 0:38:46  lr: 0.000010  loss: 0.3666  time: 1.0192  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4850/7110]  eta: 0:37:56  lr: 0.000010  loss: 0.1721  time: 0.9918  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4900/7110]  eta: 0:37:05  lr: 0.000010  loss: 0.0851  time: 0.9301  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [4950/7110]  eta: 0:36:13  lr: 0.000010  loss: 0.2535  time: 0.9939  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5000/7110]  eta: 0:35:23  lr: 0.000010  loss: 0.4761  time: 1.0643  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5050/7110]  eta: 0:34:32  lr: 0.000010  loss: 0.2228  time: 0.9412  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5100/7110]  eta: 0:33:43  lr: 0.000010  loss: 0.6202  time: 1.0339  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5150/7110]  eta: 0:32:53  lr: 0.000010  loss: 1.3601  time: 1.0518  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5200/7110]  eta: 0:32:02  lr: 0.000010  loss: 0.1838  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5250/7110]  eta: 0:31:11  lr: 0.000010  loss: 0.1318  time: 0.9894  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5300/7110]  eta: 0:30:20  lr: 0.000010  loss: 0.3214  time: 0.9270  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5350/7110]  eta: 0:29:30  lr: 0.000010  loss: 0.1652  time: 1.0497  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5400/7110]  eta: 0:28:40  lr: 0.000010  loss: 0.3099  time: 0.9936  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5450/7110]  eta: 0:27:50  lr: 0.000010  loss: 0.0354  time: 1.0595  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5500/7110]  eta: 0:27:00  lr: 0.000010  loss: 0.3282  time: 1.0475  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5550/7110]  eta: 0:26:10  lr: 0.000010  loss: 0.0398  time: 0.9896  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5600/7110]  eta: 0:25:19  lr: 0.000010  loss: 0.4192  time: 1.0457  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5650/7110]  eta: 0:24:28  lr: 0.000010  loss: 0.1628  time: 0.9489  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5700/7110]  eta: 0:23:38  lr: 0.000010  loss: 0.3948  time: 1.0074  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5750/7110]  eta: 0:22:47  lr: 0.000010  loss: 0.2796  time: 1.0113  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5800/7110]  eta: 0:21:56  lr: 0.000010  loss: 0.3170  time: 0.9891  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5850/7110]  eta: 0:21:06  lr: 0.000010  loss: 0.4109  time: 0.9428  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5900/7110]  eta: 0:20:16  lr: 0.000010  loss: 0.0238  time: 1.0375  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [5950/7110]  eta: 0:19:26  lr: 0.000010  loss: 0.1041  time: 1.0438  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6000/7110]  eta: 0:18:36  lr: 0.000010  loss: 0.7527  time: 1.0040  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6050/7110]  eta: 0:17:45  lr: 0.000010  loss: 0.2882  time: 0.9394  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6100/7110]  eta: 0:16:55  lr: 0.000010  loss: 0.1364  time: 1.0027  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6150/7110]  eta: 0:16:04  lr: 0.000010  loss: 0.4678  time: 0.9313  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6200/7110]  eta: 0:15:14  lr: 0.000010  loss: 0.2703  time: 0.9945  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6250/7110]  eta: 0:14:24  lr: 0.000010  loss: 0.0777  time: 1.0188  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6300/7110]  eta: 0:13:34  lr: 0.000010  loss: 0.1056  time: 0.9462  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6350/7110]  eta: 0:12:43  lr: 0.000010  loss: 0.3552  time: 0.9296  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6400/7110]  eta: 0:11:53  lr: 0.000010  loss: 0.0780  time: 0.9642  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6450/7110]  eta: 0:11:03  lr: 0.000010  loss: 0.4263  time: 1.0200  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6500/7110]  eta: 0:10:13  lr: 0.000010  loss: 0.0634  time: 1.0313  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.5760  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6600/7110]  eta: 0:08:32  lr: 0.000010  loss: 0.4713  time: 1.0345  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6650/7110]  eta: 0:07:42  lr: 0.000010  loss: 0.0477  time: 1.0310  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6700/7110]  eta: 0:06:52  lr: 0.000010  loss: 0.2386  time: 1.0795  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.1965  time: 1.0049  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.0308  time: 1.0236  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6850/7110]  eta: 0:04:21  lr: 0.000010  loss: 0.0946  time: 0.9564  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.2947  time: 1.0302  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1942  time: 0.9511  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.0578  time: 0.9882  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.5217  time: 1.0188  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.3241  time: 0.9784  data: 0.0000  max mem: 66110
Train: data epoch: [40]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.4920  time: 1.1017  data: 0.0000  max mem: 66110
Train: data epoch: [40] Total time: 1:59:03 (1.0047 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:20:57    time: 20.9126  data: 19.6675  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:34    time: 3.1896  data: 1.7889  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:13    time: 1.3747  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:30    time: 1.2651  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:51    time: 1.3010  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:15    time: 1.4184  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:19    time: 1.2977  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:34    time: 1.3044  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:50    time: 1.4210  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:23    time: 1.4405  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:10    time: 1.5491  data: 0.0009  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:48    time: 1.5581  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:09    time: 1.3954  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:50    time: 1.3989  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:16    time: 1.3938  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:49    time: 1.3067  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:26    time: 1.3538  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:55    time: 1.2938  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:36    time: 1.3138  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:26    time: 1.4958  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:02    time: 1.4421  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:58    time: 1.5115  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:34    time: 1.4910  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:10    time: 1.2622  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:52    time: 1.3094  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:36    time: 1.3985  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:28    time: 1.5552  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:05    time: 1.4342  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:46    time: 1.2556  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:37    time: 1.4901  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:14    time: 1.4151  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:54    time: 1.2030  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:39    time: 1.3288  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:20    time: 1.3410  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:07    time: 1.3871  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:52    time: 1.4617  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:33    time: 1.3186  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:17    time: 1.2893  data: 0.0011  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:05    time: 1.4428  data: 0.0012  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:50    time: 1.4818  data: 0.0012  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:37    time: 1.4548  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:21    time: 1.4287  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:07    time: 1.4176  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:51    time: 1.4055  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:33    time: 1.2419  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:17    time: 1.2440  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:03    time: 1.3990  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:47    time: 1.3735  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:33    time: 1.3810  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:20    time: 1.4738  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:09    time: 1.5891  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:57    time: 1.6497  data: 0.0009  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:42    time: 1.5377  data: 0.0009  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:28    time: 1.4539  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:14    time: 1.4612  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:59    time: 1.4304  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:42    time: 1.2506  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:27    time: 1.2587  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:13    time: 1.3926  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:56    time: 1.2820  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:43    time: 1.3639  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:29    time: 1.5079  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:14    time: 1.4233  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:00    time: 1.3859  data: 0.0009  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:45    time: 1.3565  data: 0.0009  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:32    time: 1.4384  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:17    time: 1.4516  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:03    time: 1.4385  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:49    time: 1.4966  data: 0.0012  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:36    time: 1.5070  data: 0.0011  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:21    time: 1.4405  data: 0.0011  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:06    time: 1.3117  data: 0.0012  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:50    time: 1.2611  data: 0.0011  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:35    time: 1.1597  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:20    time: 1.1938  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:06    time: 1.3688  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:51    time: 1.4110  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:37    time: 1.3813  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:23    time: 1.4471  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:08    time: 1.2943  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:54    time: 1.3572  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:40    time: 1.5361  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:26    time: 1.4534  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:13    time: 1.5197  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:58    time: 1.4655  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:44    time: 1.4236  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:30    time: 1.3950  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:16    time: 1.3504  data: 0.0011  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:02    time: 1.4898  data: 0.0012  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:47    time: 1.3681  data: 0.0011  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:33    time: 1.2857  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:19    time: 1.4251  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:05    time: 1.4348  data: 0.0011  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:50    time: 1.3719  data: 0.0011  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:36    time: 1.4071  data: 0.0011  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:22    time: 1.4224  data: 0.0012  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:08    time: 1.3906  data: 0.0011  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:54    time: 1.4311  data: 0.0011  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3474  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3893  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.5029  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.4796  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:43    time: 1.3202  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1532  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2238  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3287  data: 0.0009  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.4081  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4609  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.4026  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3505  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3220  data: 0.0420  max mem: 66110
Evaluation Total time: 0:25:41 (1.4101 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_40_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [41]  [   0/7110]  eta: 2 days, 5:30:58  lr: 0.000010  loss: 0.1252  time: 27.0969  data: 0.0001  max mem: 66110
Train: data epoch: [41]  [  50/7110]  eta: 2:59:57  lr: 0.000010  loss: 0.0594  time: 0.9985  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 100/7110]  eta: 2:29:55  lr: 0.000010  loss: 0.0409  time: 1.0395  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 150/7110]  eta: 2:20:16  lr: 0.000010  loss: 0.3019  time: 1.0487  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 200/7110]  eta: 2:13:00  lr: 0.000010  loss: 0.4776  time: 0.9838  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 250/7110]  eta: 2:08:41  lr: 0.000010  loss: 0.1241  time: 1.0095  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 300/7110]  eta: 2:05:39  lr: 0.000010  loss: 0.0627  time: 1.0023  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 350/7110]  eta: 2:03:06  lr: 0.000010  loss: 0.3450  time: 1.0023  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 400/7110]  eta: 2:00:37  lr: 0.000010  loss: 0.0767  time: 1.0058  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 450/7110]  eta: 1:58:44  lr: 0.000010  loss: 0.1710  time: 1.0580  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 500/7110]  eta: 1:56:56  lr: 0.000010  loss: 0.0649  time: 0.9432  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 550/7110]  eta: 1:55:26  lr: 0.000010  loss: 0.2060  time: 0.9625  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 600/7110]  eta: 1:53:43  lr: 0.000010  loss: 0.0905  time: 0.9826  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 650/7110]  eta: 1:52:31  lr: 0.000010  loss: 0.1970  time: 1.0041  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 700/7110]  eta: 1:51:20  lr: 0.000010  loss: 0.0964  time: 0.9674  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 750/7110]  eta: 1:50:04  lr: 0.000010  loss: 0.3200  time: 0.9807  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 800/7110]  eta: 1:49:05  lr: 0.000010  loss: 0.1222  time: 1.0157  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 850/7110]  eta: 1:48:13  lr: 0.000010  loss: 0.2648  time: 1.0901  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 900/7110]  eta: 1:47:25  lr: 0.000010  loss: 0.0929  time: 1.0782  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [ 950/7110]  eta: 1:46:14  lr: 0.000010  loss: 0.0619  time: 1.0162  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1000/7110]  eta: 1:45:18  lr: 0.000010  loss: 0.1135  time: 1.0074  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1050/7110]  eta: 1:44:15  lr: 0.000010  loss: 0.0539  time: 0.9998  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1100/7110]  eta: 1:43:14  lr: 0.000010  loss: 1.0936  time: 1.0263  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1150/7110]  eta: 1:42:18  lr: 0.000010  loss: 0.1731  time: 1.0658  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1200/7110]  eta: 1:41:25  lr: 0.000010  loss: 0.2593  time: 1.0035  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1250/7110]  eta: 1:40:30  lr: 0.000010  loss: 0.2197  time: 1.0595  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1300/7110]  eta: 1:39:35  lr: 0.000010  loss: 0.1160  time: 0.9970  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1350/7110]  eta: 1:38:31  lr: 0.000010  loss: 0.2755  time: 0.9367  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1400/7110]  eta: 1:37:32  lr: 0.000010  loss: 0.3952  time: 1.0340  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1450/7110]  eta: 1:36:35  lr: 0.000010  loss: 0.3544  time: 1.0273  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1500/7110]  eta: 1:35:36  lr: 0.000010  loss: 0.2582  time: 0.9886  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1550/7110]  eta: 1:34:36  lr: 0.000010  loss: 0.7798  time: 0.9427  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1600/7110]  eta: 1:33:38  lr: 0.000010  loss: 0.1588  time: 0.9949  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1650/7110]  eta: 1:32:37  lr: 0.000010  loss: 0.0526  time: 0.9734  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1700/7110]  eta: 1:31:51  lr: 0.000010  loss: 0.2018  time: 1.0171  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1750/7110]  eta: 1:30:59  lr: 0.000010  loss: 0.0191  time: 0.9597  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1800/7110]  eta: 1:30:01  lr: 0.000010  loss: 0.1824  time: 0.9582  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1850/7110]  eta: 1:29:07  lr: 0.000010  loss: 0.1091  time: 0.9994  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1900/7110]  eta: 1:28:13  lr: 0.000010  loss: 0.0372  time: 1.0143  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [1950/7110]  eta: 1:27:18  lr: 0.000010  loss: 0.2525  time: 0.9905  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2000/7110]  eta: 1:26:28  lr: 0.000010  loss: 0.4683  time: 1.0349  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2050/7110]  eta: 1:25:34  lr: 0.000010  loss: 0.2221  time: 0.9991  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2100/7110]  eta: 1:24:47  lr: 0.000010  loss: 0.5164  time: 1.0222  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2150/7110]  eta: 1:23:53  lr: 0.000010  loss: 0.2767  time: 0.9885  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2200/7110]  eta: 1:23:03  lr: 0.000010  loss: 0.3311  time: 1.0401  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2250/7110]  eta: 1:22:11  lr: 0.000010  loss: 0.2744  time: 1.0014  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2300/7110]  eta: 1:21:17  lr: 0.000010  loss: 0.1239  time: 0.9669  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2350/7110]  eta: 1:20:27  lr: 0.000010  loss: 0.1504  time: 1.0050  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2400/7110]  eta: 1:19:37  lr: 0.000010  loss: 0.1129  time: 1.0745  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2450/7110]  eta: 1:18:43  lr: 0.000010  loss: 0.7015  time: 0.9662  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2500/7110]  eta: 1:17:55  lr: 0.000010  loss: 0.5193  time: 1.0470  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2550/7110]  eta: 1:17:04  lr: 0.000010  loss: 0.3447  time: 0.9988  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2600/7110]  eta: 1:16:12  lr: 0.000010  loss: 0.2938  time: 0.9893  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2650/7110]  eta: 1:15:20  lr: 0.000010  loss: 0.0740  time: 0.9961  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2700/7110]  eta: 1:14:27  lr: 0.000010  loss: 0.1146  time: 0.9887  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2750/7110]  eta: 1:13:38  lr: 0.000010  loss: 0.0172  time: 1.0079  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2800/7110]  eta: 1:12:45  lr: 0.000010  loss: 0.0556  time: 1.0080  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2850/7110]  eta: 1:11:55  lr: 0.000010  loss: 0.3292  time: 0.9760  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2900/7110]  eta: 1:11:04  lr: 0.000010  loss: 0.3028  time: 0.9926  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [2950/7110]  eta: 1:10:11  lr: 0.000010  loss: 0.1774  time: 1.0081  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3000/7110]  eta: 1:09:18  lr: 0.000010  loss: 0.5293  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3050/7110]  eta: 1:08:28  lr: 0.000010  loss: 0.1449  time: 0.9601  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3100/7110]  eta: 1:07:34  lr: 0.000010  loss: 1.3628  time: 0.9456  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3150/7110]  eta: 1:06:43  lr: 0.000010  loss: 0.2871  time: 0.9816  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3200/7110]  eta: 1:05:50  lr: 0.000010  loss: 0.5263  time: 0.9448  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3250/7110]  eta: 1:04:56  lr: 0.000010  loss: 0.3446  time: 0.9550  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3300/7110]  eta: 1:04:06  lr: 0.000010  loss: 0.0993  time: 1.0511  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3350/7110]  eta: 1:03:17  lr: 0.000010  loss: 0.0476  time: 1.0591  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3400/7110]  eta: 1:02:24  lr: 0.000010  loss: 0.3148  time: 0.9267  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3450/7110]  eta: 1:01:34  lr: 0.000010  loss: 0.2471  time: 1.0190  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3500/7110]  eta: 1:00:42  lr: 0.000010  loss: 0.4740  time: 0.9523  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3550/7110]  eta: 0:59:50  lr: 0.000010  loss: 0.1841  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3600/7110]  eta: 0:58:59  lr: 0.000010  loss: 0.2700  time: 0.9772  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3650/7110]  eta: 0:58:09  lr: 0.000010  loss: 0.6444  time: 0.9792  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3700/7110]  eta: 0:57:19  lr: 0.000010  loss: 0.1590  time: 1.0084  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3750/7110]  eta: 0:56:31  lr: 0.000010  loss: 0.2690  time: 1.0821  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3800/7110]  eta: 0:55:39  lr: 0.000010  loss: 0.0205  time: 0.9278  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3850/7110]  eta: 0:54:48  lr: 0.000010  loss: 0.0413  time: 1.0030  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3900/7110]  eta: 0:53:59  lr: 0.000010  loss: 0.0104  time: 1.0079  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [3950/7110]  eta: 0:53:08  lr: 0.000010  loss: 0.3048  time: 1.0342  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4000/7110]  eta: 0:52:18  lr: 0.000010  loss: 0.1453  time: 1.0323  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4050/7110]  eta: 0:51:26  lr: 0.000010  loss: 0.0057  time: 0.9821  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4100/7110]  eta: 0:50:35  lr: 0.000010  loss: 0.0479  time: 1.0001  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4150/7110]  eta: 0:49:45  lr: 0.000010  loss: 0.1922  time: 1.0092  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4200/7110]  eta: 0:48:54  lr: 0.000010  loss: 1.1177  time: 0.9679  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4250/7110]  eta: 0:48:04  lr: 0.000010  loss: 0.0726  time: 1.0113  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4300/7110]  eta: 0:47:12  lr: 0.000010  loss: 0.1604  time: 0.9436  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4350/7110]  eta: 0:46:21  lr: 0.000010  loss: 0.0450  time: 0.9550  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4400/7110]  eta: 0:45:29  lr: 0.000010  loss: 0.0561  time: 0.9357  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4450/7110]  eta: 0:44:38  lr: 0.000010  loss: 0.0955  time: 1.0120  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4500/7110]  eta: 0:43:47  lr: 0.000010  loss: 0.1190  time: 0.9276  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4550/7110]  eta: 0:42:56  lr: 0.000010  loss: 0.0526  time: 0.9725  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4600/7110]  eta: 0:42:05  lr: 0.000010  loss: 0.0353  time: 1.0001  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4650/7110]  eta: 0:41:16  lr: 0.000010  loss: 0.1286  time: 1.0465  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4700/7110]  eta: 0:40:25  lr: 0.000010  loss: 0.1549  time: 0.9705  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4750/7110]  eta: 0:39:33  lr: 0.000010  loss: 0.0195  time: 0.9840  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4800/7110]  eta: 0:38:43  lr: 0.000010  loss: 1.2777  time: 0.9716  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4850/7110]  eta: 0:37:52  lr: 0.000010  loss: 0.1130  time: 0.9742  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4900/7110]  eta: 0:37:01  lr: 0.000010  loss: 0.3965  time: 0.9909  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [4950/7110]  eta: 0:36:10  lr: 0.000010  loss: 0.3679  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5000/7110]  eta: 0:35:19  lr: 0.000010  loss: 0.1969  time: 1.0183  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5050/7110]  eta: 0:34:28  lr: 0.000010  loss: 0.0383  time: 0.9362  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5100/7110]  eta: 0:33:39  lr: 0.000010  loss: 0.0093  time: 1.0383  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5150/7110]  eta: 0:32:48  lr: 0.000010  loss: 0.0653  time: 0.9632  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5200/7110]  eta: 0:31:58  lr: 0.000010  loss: 0.1646  time: 1.0163  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5250/7110]  eta: 0:31:08  lr: 0.000010  loss: 0.1047  time: 1.0413  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5300/7110]  eta: 0:30:17  lr: 0.000010  loss: 0.3639  time: 0.9734  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5350/7110]  eta: 0:29:27  lr: 0.000010  loss: 0.0470  time: 1.0099  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5400/7110]  eta: 0:28:36  lr: 0.000010  loss: 0.0540  time: 0.9762  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5450/7110]  eta: 0:27:47  lr: 0.000010  loss: 0.3463  time: 1.0099  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5500/7110]  eta: 0:26:56  lr: 0.000010  loss: 0.4580  time: 0.9567  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5550/7110]  eta: 0:26:06  lr: 0.000010  loss: 0.6705  time: 0.9743  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5600/7110]  eta: 0:25:15  lr: 0.000010  loss: 0.3519  time: 0.9835  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5650/7110]  eta: 0:24:25  lr: 0.000010  loss: 0.1281  time: 1.0144  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.1359  time: 0.9558  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.1792  time: 0.9647  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.0670  time: 0.9948  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 0.1035  time: 0.9566  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.1128  time: 0.9702  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.0813  time: 0.9919  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.5795  time: 1.0170  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.2830  time: 0.9936  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.1024  time: 0.9918  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.0388  time: 1.0243  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.2015  time: 1.0139  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.1777  time: 1.0126  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 1.5190  time: 1.0907  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.0958  time: 0.9911  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.1963  time: 1.0056  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.1335  time: 1.0060  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.2374  time: 0.9873  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.0681  time: 0.9746  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.0598  time: 1.0007  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1474  time: 1.0021  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.0542  time: 1.0803  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.8348  time: 1.0641  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.0712  time: 0.9797  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1062  time: 0.9539  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.2183  time: 1.0060  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.5372  time: 1.0732  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.3740  time: 0.9789  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.4193  time: 1.0165  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.0874  time: 1.0033  data: 0.0000  max mem: 66110
Train: data epoch: [41]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.1656  time: 1.0918  data: 0.0000  max mem: 66110
Train: data epoch: [41] Total time: 1:58:56 (1.0037 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:18:11    time: 20.7604  data: 19.5029  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:01    time: 3.1594  data: 1.7739  max mem: 66110
Evaluation  [  20/1093]  eta: 0:40:50    time: 1.3600  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:08    time: 1.2501  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:33    time: 1.2879  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:02    time: 1.4192  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:09    time: 1.3020  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:26    time: 1.3053  data: 0.0012  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:52    time: 1.4602  data: 0.0012  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:19    time: 1.4532  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:09    time: 1.5350  data: 0.0009  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:44    time: 1.5532  data: 0.0009  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:05    time: 1.3796  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:50    time: 1.4186  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:16    time: 1.4123  data: 0.0011  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:45    time: 1.2768  data: 0.0012  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:22    time: 1.3261  data: 0.0011  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:51    time: 1.2901  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:33    time: 1.3162  data: 0.0011  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:22    time: 1.4976  data: 0.0012  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:59    time: 1.4353  data: 0.0011  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:54    time: 1.5056  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:29    time: 1.4690  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:05    time: 1.2351  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:48    time: 1.3124  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:32    time: 1.4108  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:24    time: 1.5517  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:01    time: 1.4274  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:41    time: 1.2363  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:33    time: 1.4838  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:10    time: 1.4173  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:51    time: 1.1968  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:37    time: 1.3666  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:19    time: 1.3930  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:07    time: 1.4118  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:53    time: 1.4921  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:32    time: 1.3071  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:15    time: 1.2123  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:03    time: 1.4203  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:48    time: 1.4980  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:35    time: 1.4505  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:20    time: 1.4378  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:06    time: 1.4276  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:50    time: 1.4061  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:30    time: 1.2091  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:15    time: 1.2228  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:01    time: 1.4083  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:46    time: 1.3986  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:33    time: 1.4309  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:20    time: 1.4992  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:08    time: 1.5734  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:57    time: 1.6836  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:43    time: 1.6098  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:29    time: 1.4804  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:15    time: 1.4671  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:01    time: 1.4432  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:43    time: 1.2766  data: 0.0012  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:28    time: 1.2489  data: 0.0011  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:13    time: 1.3657  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:57    time: 1.2968  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:44    time: 1.3996  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:30    time: 1.5235  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:16    time: 1.4186  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:01    time: 1.3852  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:46    time: 1.3528  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:33    time: 1.4391  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:17    time: 1.4309  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:04    time: 1.3861  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:49    time: 1.4555  data: 0.0011  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:36    time: 1.4758  data: 0.0011  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:21    time: 1.4380  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:05    time: 1.2364  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:50    time: 1.1772  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:34    time: 1.1618  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:19    time: 1.1646  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:05    time: 1.3231  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:50    time: 1.3912  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:36    time: 1.4053  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:22    time: 1.4702  data: 0.0011  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:07    time: 1.3079  data: 0.0012  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:54    time: 1.3766  data: 0.0011  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:40    time: 1.5624  data: 0.0011  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:26    time: 1.4706  data: 0.0011  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:12    time: 1.5181  data: 0.0012  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:58    time: 1.4781  data: 0.0013  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:44    time: 1.4598  data: 0.0011  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:30    time: 1.4328  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:16    time: 1.3757  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:02    time: 1.5072  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:47    time: 1.3549  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:33    time: 1.2698  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:19    time: 1.4418  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:05    time: 1.4348  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:50    time: 1.3474  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:36    time: 1.4277  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:22    time: 1.4949  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:08    time: 1.4315  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:54    time: 1.4213  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3515  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3776  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4653  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.4640  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:43    time: 1.3102  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1401  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2371  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3438  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3881  data: 0.0011  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4316  data: 0.0011  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3598  data: 0.0011  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3321  data: 0.0011  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3029  data: 0.0422  max mem: 66110
Evaluation Total time: 0:25:40 (1.4095 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_41_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [42]  [   0/7110]  eta: 2 days, 6:16:22  lr: 0.000010  loss: 0.0399  time: 27.4799  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [  50/7110]  eta: 3:00:44  lr: 0.000010  loss: 0.1313  time: 0.9713  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 100/7110]  eta: 2:31:08  lr: 0.000010  loss: 0.2368  time: 1.0555  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 150/7110]  eta: 2:19:13  lr: 0.000010  loss: 0.0670  time: 1.0736  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 200/7110]  eta: 2:12:18  lr: 0.000010  loss: 0.2758  time: 0.9716  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 250/7110]  eta: 2:07:42  lr: 0.000010  loss: 0.1923  time: 0.9765  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 300/7110]  eta: 2:03:46  lr: 0.000010  loss: 0.0380  time: 0.9218  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 350/7110]  eta: 2:01:02  lr: 0.000010  loss: 0.0779  time: 0.9655  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 400/7110]  eta: 1:58:31  lr: 0.000010  loss: 0.2929  time: 0.9733  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 450/7110]  eta: 1:56:53  lr: 0.000010  loss: 0.2502  time: 1.0143  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 500/7110]  eta: 1:55:20  lr: 0.000010  loss: 0.3687  time: 1.0177  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 550/7110]  eta: 1:53:34  lr: 0.000010  loss: 0.0331  time: 0.9582  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 600/7110]  eta: 1:52:23  lr: 0.000010  loss: 0.1138  time: 0.9791  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 650/7110]  eta: 1:51:11  lr: 0.000010  loss: 0.0742  time: 0.9991  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 700/7110]  eta: 1:49:58  lr: 0.000010  loss: 0.1994  time: 0.9585  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 750/7110]  eta: 1:48:57  lr: 0.000010  loss: 0.1264  time: 1.0114  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 800/7110]  eta: 1:48:02  lr: 0.000010  loss: 0.4135  time: 1.0209  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 850/7110]  eta: 1:46:56  lr: 0.000010  loss: 1.7202  time: 0.9624  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 900/7110]  eta: 1:45:49  lr: 0.000010  loss: 0.1219  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [ 950/7110]  eta: 1:44:49  lr: 0.000010  loss: 0.1997  time: 0.9827  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1000/7110]  eta: 1:43:51  lr: 0.000010  loss: 0.4119  time: 0.9859  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1050/7110]  eta: 1:42:59  lr: 0.000010  loss: 0.2946  time: 1.0674  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1100/7110]  eta: 1:41:56  lr: 0.000010  loss: 0.0779  time: 0.9408  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1150/7110]  eta: 1:40:51  lr: 0.000010  loss: 0.4192  time: 0.9459  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1200/7110]  eta: 1:40:03  lr: 0.000010  loss: 0.2511  time: 1.0282  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1250/7110]  eta: 1:39:15  lr: 0.000010  loss: 0.1899  time: 0.9863  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1300/7110]  eta: 1:38:11  lr: 0.000010  loss: 0.1630  time: 0.9472  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1350/7110]  eta: 1:37:20  lr: 0.000010  loss: 0.0884  time: 1.0246  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1400/7110]  eta: 1:36:35  lr: 0.000010  loss: 0.1379  time: 1.0207  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1450/7110]  eta: 1:35:46  lr: 0.000010  loss: 0.1172  time: 1.0241  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1500/7110]  eta: 1:34:50  lr: 0.000010  loss: 0.7341  time: 0.9689  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1550/7110]  eta: 1:33:57  lr: 0.000010  loss: 0.2167  time: 0.9811  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1600/7110]  eta: 1:33:05  lr: 0.000010  loss: 0.4470  time: 0.9802  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1650/7110]  eta: 1:32:14  lr: 0.000010  loss: 0.0328  time: 1.0186  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1700/7110]  eta: 1:31:21  lr: 0.000010  loss: 0.1295  time: 0.9648  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1750/7110]  eta: 1:30:29  lr: 0.000010  loss: 0.0915  time: 0.9869  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1800/7110]  eta: 1:29:36  lr: 0.000010  loss: 0.0768  time: 0.9693  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1850/7110]  eta: 1:28:44  lr: 0.000010  loss: 0.0914  time: 0.9844  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1900/7110]  eta: 1:27:53  lr: 0.000010  loss: 0.2451  time: 0.9745  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [1950/7110]  eta: 1:27:07  lr: 0.000010  loss: 0.2700  time: 1.0404  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2000/7110]  eta: 1:26:21  lr: 0.000010  loss: 0.2198  time: 1.0489  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2050/7110]  eta: 1:25:31  lr: 0.000010  loss: 0.0715  time: 1.0173  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2100/7110]  eta: 1:24:39  lr: 0.000010  loss: 0.1055  time: 1.0041  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2150/7110]  eta: 1:23:45  lr: 0.000010  loss: 0.3046  time: 1.0678  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2200/7110]  eta: 1:22:53  lr: 0.000010  loss: 1.1384  time: 0.9787  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2250/7110]  eta: 1:22:01  lr: 0.000010  loss: 0.2505  time: 1.0306  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2300/7110]  eta: 1:21:09  lr: 0.000010  loss: 0.1918  time: 0.9796  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2350/7110]  eta: 1:20:17  lr: 0.000010  loss: 0.2363  time: 0.9820  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2400/7110]  eta: 1:19:23  lr: 0.000010  loss: 0.3206  time: 0.9674  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2450/7110]  eta: 1:18:30  lr: 0.000010  loss: 0.0967  time: 0.9592  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2500/7110]  eta: 1:17:34  lr: 0.000010  loss: 0.0445  time: 1.0149  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2550/7110]  eta: 1:16:45  lr: 0.000010  loss: 0.1657  time: 1.0717  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2600/7110]  eta: 1:15:53  lr: 0.000010  loss: 0.2483  time: 0.9646  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2650/7110]  eta: 1:14:59  lr: 0.000010  loss: 0.2974  time: 0.9843  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2700/7110]  eta: 1:14:08  lr: 0.000010  loss: 0.1069  time: 0.9887  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2750/7110]  eta: 1:13:18  lr: 0.000010  loss: 0.2608  time: 1.0062  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2800/7110]  eta: 1:12:28  lr: 0.000010  loss: 0.1856  time: 0.9639  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2850/7110]  eta: 1:11:37  lr: 0.000010  loss: 0.1453  time: 0.9887  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2900/7110]  eta: 1:10:47  lr: 0.000010  loss: 1.4782  time: 0.9806  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [2950/7110]  eta: 1:09:55  lr: 0.000010  loss: 0.2925  time: 1.0109  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3000/7110]  eta: 1:09:06  lr: 0.000010  loss: 0.0232  time: 1.0384  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3050/7110]  eta: 1:08:13  lr: 0.000010  loss: 0.1043  time: 0.9818  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3100/7110]  eta: 1:07:21  lr: 0.000010  loss: 0.0182  time: 0.9960  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3150/7110]  eta: 1:06:29  lr: 0.000010  loss: 0.1000  time: 1.0082  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3200/7110]  eta: 1:05:37  lr: 0.000010  loss: 0.3560  time: 1.0250  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3250/7110]  eta: 1:04:46  lr: 0.000010  loss: 0.1310  time: 0.9902  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3300/7110]  eta: 1:03:55  lr: 0.000010  loss: 0.3096  time: 0.9738  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3350/7110]  eta: 1:03:05  lr: 0.000010  loss: 0.1163  time: 0.9779  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3400/7110]  eta: 1:02:10  lr: 0.000010  loss: 0.3678  time: 0.9248  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3450/7110]  eta: 1:01:21  lr: 0.000010  loss: 0.1888  time: 1.0489  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3500/7110]  eta: 1:00:31  lr: 0.000010  loss: 0.1926  time: 1.0218  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3550/7110]  eta: 0:59:39  lr: 0.000010  loss: 0.0938  time: 0.9267  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3600/7110]  eta: 0:58:50  lr: 0.000010  loss: 0.5539  time: 1.0349  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3650/7110]  eta: 0:57:59  lr: 0.000010  loss: 0.2408  time: 0.9958  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3700/7110]  eta: 0:57:06  lr: 0.000010  loss: 0.2067  time: 0.9688  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3750/7110]  eta: 0:56:15  lr: 0.000010  loss: 0.4443  time: 0.9931  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3800/7110]  eta: 0:55:24  lr: 0.000010  loss: 0.3228  time: 0.9838  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3850/7110]  eta: 0:54:34  lr: 0.000010  loss: 0.2641  time: 0.9836  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3900/7110]  eta: 0:53:43  lr: 0.000010  loss: 0.0771  time: 1.0237  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [3950/7110]  eta: 0:52:51  lr: 0.000010  loss: 0.1107  time: 0.9690  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4000/7110]  eta: 0:52:02  lr: 0.000010  loss: 0.0980  time: 1.0713  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4050/7110]  eta: 0:51:11  lr: 0.000010  loss: 0.0773  time: 0.9631  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4100/7110]  eta: 0:50:21  lr: 0.000010  loss: 0.1355  time: 1.0197  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4150/7110]  eta: 0:49:30  lr: 0.000010  loss: 0.2100  time: 1.0433  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4200/7110]  eta: 0:48:40  lr: 0.000010  loss: 0.0875  time: 0.9931  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4250/7110]  eta: 0:47:51  lr: 0.000010  loss: 0.3388  time: 0.9962  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4300/7110]  eta: 0:47:00  lr: 0.000010  loss: 0.0782  time: 1.0088  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4350/7110]  eta: 0:46:10  lr: 0.000010  loss: 1.6409  time: 0.9909  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4400/7110]  eta: 0:45:20  lr: 0.000010  loss: 0.3550  time: 0.9646  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4450/7110]  eta: 0:44:30  lr: 0.000010  loss: 0.4424  time: 1.0102  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4500/7110]  eta: 0:43:41  lr: 0.000010  loss: 0.1079  time: 1.0714  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4550/7110]  eta: 0:42:50  lr: 0.000010  loss: 0.3866  time: 0.9501  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4600/7110]  eta: 0:42:00  lr: 0.000010  loss: 0.2771  time: 0.9609  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4650/7110]  eta: 0:41:10  lr: 0.000010  loss: 0.1528  time: 1.0313  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4700/7110]  eta: 0:40:20  lr: 0.000010  loss: 1.5513  time: 1.0207  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4750/7110]  eta: 0:39:30  lr: 0.000010  loss: 0.2008  time: 1.0064  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4800/7110]  eta: 0:38:39  lr: 0.000010  loss: 0.0593  time: 0.9970  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4850/7110]  eta: 0:37:48  lr: 0.000010  loss: 0.1811  time: 1.0031  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4900/7110]  eta: 0:36:58  lr: 0.000010  loss: 0.0490  time: 1.0254  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [4950/7110]  eta: 0:36:08  lr: 0.000010  loss: 0.0598  time: 1.0457  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5000/7110]  eta: 0:35:18  lr: 0.000010  loss: 0.1270  time: 0.9904  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5050/7110]  eta: 0:34:28  lr: 0.000010  loss: 0.0703  time: 1.0002  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5100/7110]  eta: 0:33:37  lr: 0.000010  loss: 0.3066  time: 1.0062  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5150/7110]  eta: 0:32:47  lr: 0.000010  loss: 0.3247  time: 1.0296  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5200/7110]  eta: 0:31:56  lr: 0.000010  loss: 0.4929  time: 1.0174  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5250/7110]  eta: 0:31:06  lr: 0.000010  loss: 0.0771  time: 0.9651  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5300/7110]  eta: 0:30:16  lr: 0.000010  loss: 0.0764  time: 0.9807  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5350/7110]  eta: 0:29:25  lr: 0.000010  loss: 0.0423  time: 0.9738  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5400/7110]  eta: 0:28:35  lr: 0.000010  loss: 0.4366  time: 1.0189  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5450/7110]  eta: 0:27:45  lr: 0.000010  loss: 0.5316  time: 1.0544  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5500/7110]  eta: 0:26:56  lr: 0.000010  loss: 0.0950  time: 1.0295  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5550/7110]  eta: 0:26:05  lr: 0.000010  loss: 0.2428  time: 0.9548  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5600/7110]  eta: 0:25:14  lr: 0.000010  loss: 0.1756  time: 1.0029  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5650/7110]  eta: 0:24:24  lr: 0.000010  loss: 0.2799  time: 0.9983  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 1.5803  time: 0.9803  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5750/7110]  eta: 0:22:43  lr: 0.000010  loss: 0.0401  time: 1.0213  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5800/7110]  eta: 0:21:53  lr: 0.000010  loss: 0.0860  time: 0.9954  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.0753  time: 0.9750  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5900/7110]  eta: 0:20:12  lr: 0.000010  loss: 0.5920  time: 0.9762  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [5950/7110]  eta: 0:19:22  lr: 0.000010  loss: 0.0910  time: 1.0073  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6000/7110]  eta: 0:18:32  lr: 0.000010  loss: 0.0147  time: 0.9931  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.3744  time: 0.9764  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.0697  time: 0.9791  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.3682  time: 0.9293  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6200/7110]  eta: 0:15:11  lr: 0.000010  loss: 0.5224  time: 0.9737  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6250/7110]  eta: 0:14:21  lr: 0.000010  loss: 0.2090  time: 0.9640  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6300/7110]  eta: 0:13:31  lr: 0.000010  loss: 0.2609  time: 0.9956  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.0775  time: 0.9551  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.0287  time: 0.9956  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.4184  time: 0.9822  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6500/7110]  eta: 0:10:10  lr: 0.000010  loss: 0.0468  time: 0.9645  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6550/7110]  eta: 0:09:20  lr: 0.000010  loss: 0.1607  time: 1.0192  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.2815  time: 0.9707  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.0542  time: 1.0125  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.4839  time: 1.0238  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.1768  time: 0.9899  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.2148  time: 0.9503  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.0230  time: 0.9684  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.3303  time: 1.0268  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.0515  time: 0.9630  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.0739  time: 0.9914  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.3090  time: 0.9914  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.4752  time: 0.9733  data: 0.0000  max mem: 66110
Train: data epoch: [42]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.5936  time: 1.0885  data: 0.0000  max mem: 66110
Train: data epoch: [42] Total time: 1:58:44 (1.0021 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:21:40    time: 20.9522  data: 19.7194  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:27    time: 3.1837  data: 1.7936  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:05    time: 1.3652  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:10    time: 1.2392  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:36    time: 1.2801  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:05    time: 1.4237  data: 0.0009  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:06    time: 1.2853  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:20    time: 1.2782  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:45    time: 1.4408  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:18    time: 1.4673  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:06    time: 1.5457  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:41    time: 1.5447  data: 0.0009  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:03    time: 1.3837  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:44    time: 1.3951  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:11    time: 1.3870  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:43    time: 1.3003  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:20    time: 1.3491  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:50    time: 1.2918  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:32    time: 1.3172  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:20    time: 1.4813  data: 0.0009  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:56    time: 1.4184  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:47    time: 1.4461  data: 0.0009  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:22    time: 1.4094  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:59    time: 1.2415  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:40    time: 1.2905  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:24    time: 1.3667  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:15    time: 1.5223  data: 0.0009  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:52    time: 1.4081  data: 0.0009  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:35    time: 1.2683  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:26    time: 1.5075  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:04    time: 1.4126  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:44    time: 1.1806  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:28    time: 1.2971  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:11    time: 1.3449  data: 0.0009  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:17:58    time: 1.4021  data: 0.0009  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:43    time: 1.4628  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:25    time: 1.3107  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:08    time: 1.2494  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:16:56    time: 1.4327  data: 0.0009  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:40    time: 1.4293  data: 0.0009  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:26    time: 1.3856  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:12    time: 1.4359  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:15:57    time: 1.4067  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:42    time: 1.3895  data: 0.0009  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:24    time: 1.2477  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:09    time: 1.2518  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:53    time: 1.3439  data: 0.0009  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:38    time: 1.3344  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:25    time: 1.3923  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:12    time: 1.4705  data: 0.0009  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:00    time: 1.5728  data: 0.0009  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:48    time: 1.5995  data: 0.0009  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:34    time: 1.5147  data: 0.0009  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:20    time: 1.4631  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:06    time: 1.4342  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:51    time: 1.3930  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:34    time: 1.2365  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:19    time: 1.2313  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:05    time: 1.3648  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:48    time: 1.2576  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:35    time: 1.3436  data: 0.0009  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:22    time: 1.5052  data: 0.0009  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:07    time: 1.4143  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:53    time: 1.3680  data: 0.0009  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:38    time: 1.3411  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:25    time: 1.4313  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:10    time: 1.4102  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:56    time: 1.3518  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:42    time: 1.4161  data: 0.0009  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:28    time: 1.4765  data: 0.0009  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:14    time: 1.4348  data: 0.0009  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:59    time: 1.2641  data: 0.0009  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:43    time: 1.2189  data: 0.0009  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:28    time: 1.1390  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:13    time: 1.1621  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:59    time: 1.3504  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:45    time: 1.4072  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:31    time: 1.3962  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:18    time: 1.4477  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:02    time: 1.2840  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:49    time: 1.3201  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:35    time: 1.5154  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:22    time: 1.4654  data: 0.0009  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:08    time: 1.5140  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:54    time: 1.4526  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:40    time: 1.4041  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:26    time: 1.3764  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:12    time: 1.3544  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:58    time: 1.5049  data: 0.0009  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:44    time: 1.3683  data: 0.0009  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:30    time: 1.2479  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:16    time: 1.3850  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:02    time: 1.4286  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.3689  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:34    time: 1.3653  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.3669  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:06    time: 1.3921  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:51    time: 1.4094  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.2692  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:23    time: 1.2972  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:09    time: 1.4529  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:55    time: 1.4538  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:41    time: 1.2992  data: 0.0009  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1566  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2313  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3286  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:45    time: 1.3686  data: 0.0009  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4298  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3629  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3212  data: 0.0009  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2921  data: 0.0413  max mem: 66110
Evaluation Total time: 0:25:20 (1.3911 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_42_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [43]  [   0/7110]  eta: 2 days, 9:20:31  lr: 0.000010  loss: 1.8660  time: 29.0340  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [  50/7110]  eta: 2:59:37  lr: 0.000010  loss: 0.1411  time: 0.9690  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 100/7110]  eta: 2:26:12  lr: 0.000010  loss: 0.2714  time: 0.9466  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 150/7110]  eta: 2:14:20  lr: 0.000010  loss: 0.2147  time: 0.9881  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 200/7110]  eta: 2:09:24  lr: 0.000010  loss: 0.1806  time: 1.0145  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 250/7110]  eta: 2:05:40  lr: 0.000010  loss: 0.3021  time: 0.9490  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 300/7110]  eta: 2:03:28  lr: 0.000010  loss: 0.1744  time: 1.0030  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 350/7110]  eta: 2:01:00  lr: 0.000010  loss: 0.2233  time: 1.0033  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 400/7110]  eta: 1:58:43  lr: 0.000010  loss: 0.0521  time: 0.9627  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 450/7110]  eta: 1:57:17  lr: 0.000010  loss: 0.1602  time: 1.0188  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 500/7110]  eta: 1:55:42  lr: 0.000010  loss: 0.0839  time: 1.0077  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 550/7110]  eta: 1:54:01  lr: 0.000010  loss: 0.0461  time: 0.9561  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 600/7110]  eta: 1:52:58  lr: 0.000010  loss: 0.0447  time: 1.0206  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 650/7110]  eta: 1:51:52  lr: 0.000010  loss: 0.4379  time: 1.0335  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 700/7110]  eta: 1:50:38  lr: 0.000010  loss: 0.4077  time: 0.9835  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 750/7110]  eta: 1:49:35  lr: 0.000010  loss: 0.2508  time: 0.9979  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 800/7110]  eta: 1:48:43  lr: 0.000010  loss: 0.2916  time: 1.0525  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 850/7110]  eta: 1:47:28  lr: 0.000010  loss: 0.2817  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 900/7110]  eta: 1:46:10  lr: 0.000010  loss: 0.2193  time: 0.9257  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [ 950/7110]  eta: 1:45:20  lr: 0.000010  loss: 0.3881  time: 1.1260  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1000/7110]  eta: 1:44:09  lr: 0.000010  loss: 0.0185  time: 0.9568  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1050/7110]  eta: 1:43:15  lr: 0.000010  loss: 0.2392  time: 1.0512  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1100/7110]  eta: 1:42:13  lr: 0.000010  loss: 0.1192  time: 0.9768  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1150/7110]  eta: 1:41:16  lr: 0.000010  loss: 0.0190  time: 0.9455  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1200/7110]  eta: 1:40:30  lr: 0.000010  loss: 0.0232  time: 1.0263  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1250/7110]  eta: 1:39:36  lr: 0.000010  loss: 0.4708  time: 1.0550  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1300/7110]  eta: 1:38:44  lr: 0.000010  loss: 0.0604  time: 1.1078  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1350/7110]  eta: 1:37:49  lr: 0.000010  loss: 0.1129  time: 1.0060  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1400/7110]  eta: 1:36:52  lr: 0.000010  loss: 0.4363  time: 0.9601  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1450/7110]  eta: 1:35:50  lr: 0.000010  loss: 0.8172  time: 0.9311  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1500/7110]  eta: 1:35:00  lr: 0.000010  loss: 0.0594  time: 1.0375  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1550/7110]  eta: 1:34:13  lr: 0.000010  loss: 0.6659  time: 1.0288  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1600/7110]  eta: 1:33:27  lr: 0.000010  loss: 0.0490  time: 1.0406  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1650/7110]  eta: 1:32:37  lr: 0.000010  loss: 0.8743  time: 0.9635  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1700/7110]  eta: 1:31:45  lr: 0.000010  loss: 0.5644  time: 0.9839  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1750/7110]  eta: 1:30:52  lr: 0.000010  loss: 0.0303  time: 0.9944  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1800/7110]  eta: 1:29:52  lr: 0.000010  loss: 0.5334  time: 0.9381  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1850/7110]  eta: 1:29:04  lr: 0.000010  loss: 0.3357  time: 1.0144  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1900/7110]  eta: 1:28:13  lr: 0.000010  loss: 0.0548  time: 1.0198  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [1950/7110]  eta: 1:27:20  lr: 0.000010  loss: 0.5500  time: 0.9781  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2000/7110]  eta: 1:26:26  lr: 0.000010  loss: 0.2454  time: 1.0247  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2050/7110]  eta: 1:25:33  lr: 0.000010  loss: 0.1542  time: 0.9828  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2100/7110]  eta: 1:24:39  lr: 0.000010  loss: 0.0891  time: 0.9879  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2150/7110]  eta: 1:23:45  lr: 0.000010  loss: 0.2782  time: 1.0333  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2200/7110]  eta: 1:22:48  lr: 0.000010  loss: 0.5615  time: 0.9906  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2250/7110]  eta: 1:22:00  lr: 0.000010  loss: 0.0531  time: 1.0590  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2300/7110]  eta: 1:21:10  lr: 0.000010  loss: 0.3505  time: 1.0300  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2350/7110]  eta: 1:20:18  lr: 0.000010  loss: 0.1652  time: 0.9710  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2400/7110]  eta: 1:19:26  lr: 0.000010  loss: 0.1861  time: 0.9867  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2450/7110]  eta: 1:18:30  lr: 0.000010  loss: 0.0155  time: 0.9687  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2500/7110]  eta: 1:17:39  lr: 0.000010  loss: 0.1705  time: 1.0579  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2550/7110]  eta: 1:16:51  lr: 0.000010  loss: 0.1457  time: 0.9873  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2600/7110]  eta: 1:16:01  lr: 0.000010  loss: 0.0409  time: 1.0318  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2650/7110]  eta: 1:15:09  lr: 0.000010  loss: 0.1621  time: 1.0147  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2700/7110]  eta: 1:14:20  lr: 0.000010  loss: 0.3627  time: 1.0170  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2750/7110]  eta: 1:13:28  lr: 0.000010  loss: 0.2802  time: 1.0207  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2800/7110]  eta: 1:12:37  lr: 0.000010  loss: 0.3805  time: 1.0293  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2850/7110]  eta: 1:11:45  lr: 0.000010  loss: 0.1966  time: 1.0186  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2900/7110]  eta: 1:10:52  lr: 0.000010  loss: 0.1467  time: 0.9351  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [2950/7110]  eta: 1:09:57  lr: 0.000010  loss: 0.1341  time: 0.9362  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3000/7110]  eta: 1:09:04  lr: 0.000010  loss: 0.0293  time: 0.9800  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3050/7110]  eta: 1:08:10  lr: 0.000010  loss: 0.2023  time: 0.9614  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3100/7110]  eta: 1:07:17  lr: 0.000010  loss: 0.2862  time: 0.9560  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3150/7110]  eta: 1:06:26  lr: 0.000010  loss: 0.0876  time: 0.9921  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3200/7110]  eta: 1:05:34  lr: 0.000010  loss: 0.0864  time: 0.9563  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3250/7110]  eta: 1:04:44  lr: 0.000010  loss: 0.2379  time: 1.0280  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3300/7110]  eta: 1:03:52  lr: 0.000010  loss: 0.2652  time: 1.0259  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3350/7110]  eta: 1:03:01  lr: 0.000010  loss: 0.0407  time: 0.9510  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3400/7110]  eta: 1:02:12  lr: 0.000010  loss: 0.0527  time: 0.9978  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3450/7110]  eta: 1:01:23  lr: 0.000010  loss: 0.1037  time: 1.0231  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3500/7110]  eta: 1:00:31  lr: 0.000010  loss: 0.2785  time: 0.9159  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3550/7110]  eta: 0:59:41  lr: 0.000010  loss: 0.1386  time: 1.0063  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3600/7110]  eta: 0:58:48  lr: 0.000010  loss: 0.0430  time: 0.9569  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3650/7110]  eta: 0:57:58  lr: 0.000010  loss: 0.3119  time: 1.0192  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3700/7110]  eta: 0:57:08  lr: 0.000010  loss: 0.3115  time: 1.0149  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3750/7110]  eta: 0:56:17  lr: 0.000010  loss: 0.0568  time: 0.9906  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3800/7110]  eta: 0:55:26  lr: 0.000010  loss: 0.6688  time: 1.0195  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3850/7110]  eta: 0:54:35  lr: 0.000010  loss: 0.0492  time: 1.0034  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3900/7110]  eta: 0:53:44  lr: 0.000010  loss: 0.1445  time: 1.0027  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [3950/7110]  eta: 0:52:53  lr: 0.000010  loss: 0.3185  time: 0.9763  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4000/7110]  eta: 0:52:04  lr: 0.000010  loss: 0.4239  time: 0.9793  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4050/7110]  eta: 0:51:13  lr: 0.000010  loss: 0.2816  time: 0.9702  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4100/7110]  eta: 0:50:22  lr: 0.000010  loss: 0.1292  time: 0.9742  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4150/7110]  eta: 0:49:31  lr: 0.000010  loss: 0.2339  time: 0.9801  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4200/7110]  eta: 0:48:40  lr: 0.000010  loss: 0.1037  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4250/7110]  eta: 0:47:50  lr: 0.000010  loss: 0.2685  time: 1.0000  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4300/7110]  eta: 0:46:58  lr: 0.000010  loss: 0.1735  time: 0.9347  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4350/7110]  eta: 0:46:10  lr: 0.000010  loss: 0.6778  time: 1.0674  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4400/7110]  eta: 0:45:19  lr: 0.000010  loss: 0.1723  time: 1.0252  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4450/7110]  eta: 0:44:28  lr: 0.000010  loss: 0.3067  time: 0.9587  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4500/7110]  eta: 0:43:38  lr: 0.000010  loss: 0.1895  time: 1.0155  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4550/7110]  eta: 0:42:47  lr: 0.000010  loss: 0.2775  time: 1.0283  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4600/7110]  eta: 0:41:57  lr: 0.000010  loss: 0.1402  time: 1.0206  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4650/7110]  eta: 0:41:07  lr: 0.000010  loss: 0.0086  time: 0.9946  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4700/7110]  eta: 0:40:17  lr: 0.000010  loss: 0.0733  time: 1.0060  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4750/7110]  eta: 0:39:28  lr: 0.000010  loss: 0.0753  time: 1.0575  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4800/7110]  eta: 0:38:36  lr: 0.000010  loss: 0.0458  time: 1.0037  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4850/7110]  eta: 0:37:47  lr: 0.000010  loss: 0.3074  time: 1.0097  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4900/7110]  eta: 0:36:56  lr: 0.000010  loss: 0.0467  time: 0.9271  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [4950/7110]  eta: 0:36:05  lr: 0.000010  loss: 0.1918  time: 0.9490  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5000/7110]  eta: 0:35:15  lr: 0.000010  loss: 0.1048  time: 0.9908  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5050/7110]  eta: 0:34:25  lr: 0.000010  loss: 0.1592  time: 0.9897  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5100/7110]  eta: 0:33:35  lr: 0.000010  loss: 0.4075  time: 1.0341  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5150/7110]  eta: 0:32:45  lr: 0.000010  loss: 0.0268  time: 0.9796  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5200/7110]  eta: 0:31:54  lr: 0.000010  loss: 0.0593  time: 0.9746  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5250/7110]  eta: 0:31:04  lr: 0.000010  loss: 0.4065  time: 0.9546  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5300/7110]  eta: 0:30:14  lr: 0.000010  loss: 0.0855  time: 1.0298  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5350/7110]  eta: 0:29:24  lr: 0.000010  loss: 0.0404  time: 1.0203  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5400/7110]  eta: 0:28:34  lr: 0.000010  loss: 0.0480  time: 0.9874  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5450/7110]  eta: 0:27:44  lr: 0.000010  loss: 0.0355  time: 0.9300  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5500/7110]  eta: 0:26:54  lr: 0.000010  loss: 0.5074  time: 1.0119  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5550/7110]  eta: 0:26:03  lr: 0.000010  loss: 0.2026  time: 1.0227  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5600/7110]  eta: 0:25:13  lr: 0.000010  loss: 0.1221  time: 0.9849  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5650/7110]  eta: 0:24:23  lr: 0.000010  loss: 1.4565  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5700/7110]  eta: 0:23:33  lr: 0.000010  loss: 0.2809  time: 0.9601  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5750/7110]  eta: 0:22:43  lr: 0.000010  loss: 0.7138  time: 0.9272  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5800/7110]  eta: 0:21:53  lr: 0.000010  loss: 0.2423  time: 1.0369  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5850/7110]  eta: 0:21:03  lr: 0.000010  loss: 0.1619  time: 0.9725  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5900/7110]  eta: 0:20:13  lr: 0.000010  loss: 0.3610  time: 1.0057  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [5950/7110]  eta: 0:19:22  lr: 0.000010  loss: 1.4349  time: 1.0291  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6000/7110]  eta: 0:18:32  lr: 0.000010  loss: 0.0789  time: 0.9712  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.0310  time: 0.9827  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6100/7110]  eta: 0:16:51  lr: 0.000010  loss: 0.5408  time: 0.9652  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6150/7110]  eta: 0:16:01  lr: 0.000010  loss: 0.0187  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6200/7110]  eta: 0:15:11  lr: 0.000010  loss: 0.2907  time: 0.9371  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6250/7110]  eta: 0:14:21  lr: 0.000010  loss: 0.1018  time: 1.0443  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6300/7110]  eta: 0:13:31  lr: 0.000010  loss: 0.0562  time: 0.9413  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 1.3099  time: 1.0501  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.8212  time: 0.9691  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.6779  time: 0.9854  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.5598  time: 1.0551  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.1712  time: 1.0046  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.1478  time: 0.9586  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1927  time: 1.1045  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.0777  time: 1.0069  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.1129  time: 1.0004  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.0266  time: 1.0028  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.3051  time: 0.9393  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.2860  time: 1.0089  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.0245  time: 0.9592  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.2825  time: 0.9668  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1484  time: 0.9718  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.2976  time: 0.9945  data: 0.0000  max mem: 66110
Train: data epoch: [43]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0913  time: 1.0927  data: 0.0000  max mem: 66110
Train: data epoch: [43] Total time: 1:58:51 (1.0030 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 7:02:08    time: 23.1735  data: 21.9303  max mem: 66110
Evaluation  [  10/1093]  eta: 1:01:13    time: 3.3921  data: 1.9957  max mem: 66110
Evaluation  [  20/1093]  eta: 0:43:05    time: 1.3715  data: 0.0016  max mem: 66110
Evaluation  [  30/1093]  eta: 0:35:47    time: 1.2664  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:56    time: 1.3172  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:31:03    time: 1.4235  data: 0.0009  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:40    time: 1.2315  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:28:07    time: 1.3012  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:27:22    time: 1.4860  data: 0.0009  max mem: 66110
Evaluation  [  90/1093]  eta: 0:27:01    time: 1.4986  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:43    time: 1.5902  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:26:12    time: 1.5214  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:31    time: 1.3651  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:25:12    time: 1.4135  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:32    time: 1.3722  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:24:00    time: 1.2417  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:35    time: 1.3239  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:23:04    time: 1.2909  data: 0.0009  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:44    time: 1.3133  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:33    time: 1.4864  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:08    time: 1.4190  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:59    time: 1.4532  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:38    time: 1.4843  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:14    time: 1.3002  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:52    time: 1.2605  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:36    time: 1.3515  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:27    time: 1.5360  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:04    time: 1.4189  data: 0.0009  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:46    time: 1.2789  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:38    time: 1.5146  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:15    time: 1.4204  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:54    time: 1.1805  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:38    time: 1.3014  data: 0.0009  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:21    time: 1.3590  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:08    time: 1.4190  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:53    time: 1.4770  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:33    time: 1.2987  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:17    time: 1.2426  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:05    time: 1.4585  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:49    time: 1.4606  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:35    time: 1.3991  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:19    time: 1.3990  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:04    time: 1.3733  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:49    time: 1.3938  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:29    time: 1.2199  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:14    time: 1.2194  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:00    time: 1.3782  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:44    time: 1.3447  data: 0.0009  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:30    time: 1.3688  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:17    time: 1.4787  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:05    time: 1.5652  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:53    time: 1.6109  data: 0.0009  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:39    time: 1.5284  data: 0.0009  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:25    time: 1.4500  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:11    time: 1.4572  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:56    time: 1.4281  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:39    time: 1.2627  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:24    time: 1.2347  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:09    time: 1.3670  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:53    time: 1.2649  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:40    time: 1.3455  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:26    time: 1.5109  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:11    time: 1.4183  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:57    time: 1.3920  data: 0.0009  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:42    time: 1.3454  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:29    time: 1.4227  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:13    time: 1.4218  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:59    time: 1.3497  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:45    time: 1.4131  data: 0.0009  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:31    time: 1.4642  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:17    time: 1.4327  data: 0.0009  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:01    time: 1.2496  data: 0.0009  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:46    time: 1.1938  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:30    time: 1.1523  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:16    time: 1.1885  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:02    time: 1.3576  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:47    time: 1.3836  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:33    time: 1.3833  data: 0.0009  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:20    time: 1.4515  data: 0.0009  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:04    time: 1.2749  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:51    time: 1.3475  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:37    time: 1.5384  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:23    time: 1.4152  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:10    time: 1.4813  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:55    time: 1.4696  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:41    time: 1.3931  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:27    time: 1.3457  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:13    time: 1.3465  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:59    time: 1.5017  data: 0.0011  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:45    time: 1.3526  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:30    time: 1.2339  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:16    time: 1.3887  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:02    time: 1.4159  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:48    time: 1.3628  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:34    time: 1.4315  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:20    time: 1.4150  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:06    time: 1.3452  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:52    time: 1.4234  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:38    time: 1.3639  data: 0.0009  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.4154  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:10    time: 1.5245  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4508  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.2963  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1509  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2356  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3426  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3854  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4346  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3716  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3468  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3200  data: 0.0439  max mem: 66110
Evaluation Total time: 0:25:28 (1.3987 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_43_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [44]  [   0/7110]  eta: 2 days, 6:53:39  lr: 0.000010  loss: 0.3845  time: 27.7946  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [  50/7110]  eta: 2:58:09  lr: 0.000010  loss: 0.0406  time: 0.9883  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 100/7110]  eta: 2:25:39  lr: 0.000010  loss: 0.4157  time: 0.9890  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 150/7110]  eta: 2:14:24  lr: 0.000010  loss: 0.2065  time: 1.0267  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 200/7110]  eta: 2:08:51  lr: 0.000010  loss: 0.2549  time: 0.9637  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 250/7110]  eta: 2:05:56  lr: 0.000010  loss: 0.1847  time: 1.0148  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 300/7110]  eta: 2:02:34  lr: 0.000010  loss: 0.1229  time: 0.9883  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 350/7110]  eta: 1:59:40  lr: 0.000010  loss: 0.1463  time: 0.9635  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 400/7110]  eta: 1:58:15  lr: 0.000010  loss: 0.6644  time: 1.0574  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 450/7110]  eta: 1:56:23  lr: 0.000010  loss: 0.0800  time: 0.9865  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 500/7110]  eta: 1:54:42  lr: 0.000010  loss: 0.1515  time: 0.9758  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 550/7110]  eta: 1:53:40  lr: 0.000010  loss: 0.1435  time: 1.0027  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 600/7110]  eta: 1:52:25  lr: 0.000010  loss: 0.3326  time: 1.0390  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 650/7110]  eta: 1:51:18  lr: 0.000010  loss: 0.0667  time: 1.0515  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 700/7110]  eta: 1:50:11  lr: 0.000010  loss: 0.1415  time: 1.0348  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 750/7110]  eta: 1:49:16  lr: 0.000010  loss: 1.7926  time: 1.0240  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 800/7110]  eta: 1:48:24  lr: 0.000010  loss: 0.3571  time: 1.0568  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 850/7110]  eta: 1:47:22  lr: 0.000010  loss: 0.2540  time: 1.0782  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 900/7110]  eta: 1:46:15  lr: 0.000010  loss: 0.0883  time: 1.0159  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [ 950/7110]  eta: 1:45:22  lr: 0.000010  loss: 0.1187  time: 1.0255  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1000/7110]  eta: 1:44:27  lr: 0.000010  loss: 0.3345  time: 0.9961  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1050/7110]  eta: 1:43:29  lr: 0.000010  loss: 0.1419  time: 1.0211  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1100/7110]  eta: 1:42:36  lr: 0.000010  loss: 0.0488  time: 1.0234  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1150/7110]  eta: 1:41:41  lr: 0.000010  loss: 0.1596  time: 1.0246  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1200/7110]  eta: 1:40:47  lr: 0.000010  loss: 0.0223  time: 1.0054  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1250/7110]  eta: 1:39:45  lr: 0.000010  loss: 0.2346  time: 1.0030  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1300/7110]  eta: 1:38:44  lr: 0.000010  loss: 0.7732  time: 0.9717  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1350/7110]  eta: 1:37:40  lr: 0.000010  loss: 0.4557  time: 0.9415  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1400/7110]  eta: 1:36:47  lr: 0.000010  loss: 1.1345  time: 1.0044  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1450/7110]  eta: 1:35:50  lr: 0.000010  loss: 0.4395  time: 1.0163  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1500/7110]  eta: 1:34:49  lr: 0.000010  loss: 0.1293  time: 0.9534  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1550/7110]  eta: 1:33:57  lr: 0.000010  loss: 1.7819  time: 1.0325  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1600/7110]  eta: 1:33:03  lr: 0.000010  loss: 0.2652  time: 0.9835  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1650/7110]  eta: 1:32:07  lr: 0.000010  loss: 0.3054  time: 0.9700  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1700/7110]  eta: 1:31:09  lr: 0.000010  loss: 0.1585  time: 0.9800  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1750/7110]  eta: 1:30:17  lr: 0.000010  loss: 0.1056  time: 1.0276  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1800/7110]  eta: 1:29:23  lr: 0.000010  loss: 0.0662  time: 0.9923  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1850/7110]  eta: 1:28:30  lr: 0.000010  loss: 0.0394  time: 0.9811  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1900/7110]  eta: 1:27:36  lr: 0.000010  loss: 0.0804  time: 0.9745  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [1950/7110]  eta: 1:26:46  lr: 0.000010  loss: 0.3781  time: 1.0039  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2000/7110]  eta: 1:25:53  lr: 0.000010  loss: 0.2552  time: 0.9607  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2050/7110]  eta: 1:25:05  lr: 0.000010  loss: 0.1081  time: 0.9958  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2100/7110]  eta: 1:24:11  lr: 0.000010  loss: 0.2467  time: 0.9889  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2150/7110]  eta: 1:23:21  lr: 0.000010  loss: 0.1991  time: 1.0089  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2200/7110]  eta: 1:22:28  lr: 0.000010  loss: 0.1035  time: 0.9648  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2250/7110]  eta: 1:21:35  lr: 0.000010  loss: 0.1860  time: 1.0017  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2300/7110]  eta: 1:20:43  lr: 0.000010  loss: 0.1200  time: 0.9932  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2350/7110]  eta: 1:19:52  lr: 0.000010  loss: 0.6685  time: 0.9486  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2400/7110]  eta: 1:19:06  lr: 0.000010  loss: 0.0317  time: 1.0274  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2450/7110]  eta: 1:18:20  lr: 0.000010  loss: 0.3415  time: 1.0900  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2500/7110]  eta: 1:17:27  lr: 0.000010  loss: 0.2708  time: 0.9638  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2550/7110]  eta: 1:16:35  lr: 0.000010  loss: 0.0175  time: 1.0294  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2600/7110]  eta: 1:15:44  lr: 0.000010  loss: 0.1616  time: 0.9655  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2650/7110]  eta: 1:14:54  lr: 0.000010  loss: 0.0591  time: 0.9854  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2700/7110]  eta: 1:14:03  lr: 0.000010  loss: 0.1805  time: 1.0048  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2750/7110]  eta: 1:13:12  lr: 0.000010  loss: 0.0578  time: 1.0240  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2800/7110]  eta: 1:12:17  lr: 0.000010  loss: 0.1246  time: 0.9749  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2850/7110]  eta: 1:11:25  lr: 0.000010  loss: 0.0421  time: 0.9699  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2900/7110]  eta: 1:10:33  lr: 0.000010  loss: 0.2147  time: 1.0012  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [2950/7110]  eta: 1:09:42  lr: 0.000010  loss: 1.3737  time: 1.0296  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3000/7110]  eta: 1:08:52  lr: 0.000010  loss: 0.1238  time: 0.9786  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3050/7110]  eta: 1:07:58  lr: 0.000010  loss: 0.0782  time: 0.9801  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3100/7110]  eta: 1:07:06  lr: 0.000010  loss: 0.3979  time: 0.9955  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3150/7110]  eta: 1:06:17  lr: 0.000010  loss: 0.1723  time: 1.0287  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3200/7110]  eta: 1:05:28  lr: 0.000010  loss: 0.0667  time: 0.9976  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3250/7110]  eta: 1:04:39  lr: 0.000010  loss: 0.1103  time: 0.9981  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3300/7110]  eta: 1:03:48  lr: 0.000010  loss: 0.4595  time: 0.9997  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3350/7110]  eta: 1:02:58  lr: 0.000010  loss: 0.0371  time: 1.0469  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3400/7110]  eta: 1:02:07  lr: 0.000010  loss: 0.0448  time: 0.9920  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3450/7110]  eta: 1:01:16  lr: 0.000010  loss: 0.1569  time: 0.9627  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3500/7110]  eta: 1:00:26  lr: 0.000010  loss: 0.1810  time: 0.9518  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3550/7110]  eta: 0:59:37  lr: 0.000010  loss: 0.1003  time: 0.9868  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3600/7110]  eta: 0:58:46  lr: 0.000010  loss: 0.2507  time: 0.9500  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3650/7110]  eta: 0:57:57  lr: 0.000010  loss: 0.5305  time: 1.0068  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3700/7110]  eta: 0:57:04  lr: 0.000010  loss: 0.1306  time: 0.9595  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3750/7110]  eta: 0:56:15  lr: 0.000010  loss: 0.0879  time: 1.0338  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3800/7110]  eta: 0:55:25  lr: 0.000010  loss: 0.0853  time: 1.0342  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3850/7110]  eta: 0:54:35  lr: 0.000010  loss: 0.0337  time: 1.0334  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3900/7110]  eta: 0:53:43  lr: 0.000010  loss: 0.0119  time: 0.9448  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [3950/7110]  eta: 0:52:54  lr: 0.000010  loss: 0.1256  time: 1.0179  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4000/7110]  eta: 0:52:04  lr: 0.000010  loss: 0.2401  time: 1.0345  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4050/7110]  eta: 0:51:14  lr: 0.000010  loss: 0.1144  time: 1.0201  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4100/7110]  eta: 0:50:24  lr: 0.000010  loss: 0.2487  time: 1.0208  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4150/7110]  eta: 0:49:33  lr: 0.000010  loss: 0.3121  time: 1.0931  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4200/7110]  eta: 0:48:43  lr: 0.000010  loss: 0.2076  time: 0.9733  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4250/7110]  eta: 0:47:51  lr: 0.000010  loss: 0.7159  time: 0.9710  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4300/7110]  eta: 0:47:01  lr: 0.000010  loss: 0.6066  time: 0.9904  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4350/7110]  eta: 0:46:09  lr: 0.000010  loss: 0.0652  time: 0.9439  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4400/7110]  eta: 0:45:20  lr: 0.000010  loss: 0.0521  time: 1.0227  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4450/7110]  eta: 0:44:29  lr: 0.000010  loss: 0.5687  time: 0.9625  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4500/7110]  eta: 0:43:40  lr: 0.000010  loss: 0.0780  time: 1.0476  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4550/7110]  eta: 0:42:48  lr: 0.000010  loss: 0.2423  time: 0.9414  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4600/7110]  eta: 0:41:58  lr: 0.000010  loss: 0.2220  time: 1.0279  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4650/7110]  eta: 0:41:08  lr: 0.000010  loss: 0.1401  time: 1.0139  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4700/7110]  eta: 0:40:18  lr: 0.000010  loss: 0.0783  time: 0.9983  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4750/7110]  eta: 0:39:28  lr: 0.000010  loss: 0.0581  time: 1.0155  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4800/7110]  eta: 0:38:38  lr: 0.000010  loss: 0.1289  time: 0.9712  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4850/7110]  eta: 0:37:47  lr: 0.000010  loss: 0.1053  time: 0.9654  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4900/7110]  eta: 0:36:57  lr: 0.000010  loss: 0.5210  time: 0.9847  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [4950/7110]  eta: 0:36:07  lr: 0.000010  loss: 0.6172  time: 1.0446  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5000/7110]  eta: 0:35:17  lr: 0.000010  loss: 0.0194  time: 1.0623  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5050/7110]  eta: 0:34:27  lr: 0.000010  loss: 0.6452  time: 0.9996  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5100/7110]  eta: 0:33:37  lr: 0.000010  loss: 0.2998  time: 1.1012  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5150/7110]  eta: 0:32:46  lr: 0.000010  loss: 0.0988  time: 0.9361  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5200/7110]  eta: 0:31:56  lr: 0.000010  loss: 0.0431  time: 1.0404  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5250/7110]  eta: 0:31:06  lr: 0.000010  loss: 0.1605  time: 1.0531  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5300/7110]  eta: 0:30:15  lr: 0.000010  loss: 0.0351  time: 0.9946  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5350/7110]  eta: 0:29:25  lr: 0.000010  loss: 0.0469  time: 0.9965  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5400/7110]  eta: 0:28:35  lr: 0.000010  loss: 0.2646  time: 1.0594  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5450/7110]  eta: 0:27:45  lr: 0.000010  loss: 0.1335  time: 0.9875  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5500/7110]  eta: 0:26:55  lr: 0.000010  loss: 0.3306  time: 0.9714  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5550/7110]  eta: 0:26:05  lr: 0.000010  loss: 0.5270  time: 1.0128  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5600/7110]  eta: 0:25:15  lr: 0.000010  loss: 0.3655  time: 0.9833  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5650/7110]  eta: 0:24:25  lr: 0.000010  loss: 0.3328  time: 1.0383  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5700/7110]  eta: 0:23:35  lr: 0.000010  loss: 0.0884  time: 1.0507  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5750/7110]  eta: 0:22:45  lr: 0.000010  loss: 0.3938  time: 1.0229  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5800/7110]  eta: 0:21:55  lr: 0.000010  loss: 0.2581  time: 0.9897  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 0.0359  time: 0.9954  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.0722  time: 0.9694  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.1925  time: 1.0202  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.0366  time: 0.9598  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.0824  time: 0.9780  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.0164  time: 1.0141  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.0648  time: 1.0028  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.3487  time: 1.0137  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.0683  time: 0.9521  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6300/7110]  eta: 0:13:31  lr: 0.000010  loss: 0.2148  time: 0.9218  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.3916  time: 1.0720  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.3083  time: 0.9874  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.1545  time: 0.9874  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.0768  time: 1.0256  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.1132  time: 0.9525  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.2520  time: 0.9787  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.1869  time: 1.0080  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.3075  time: 1.0379  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.2276  time: 0.9798  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.2200  time: 1.0021  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1536  time: 1.0089  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.5225  time: 0.9920  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.0112  time: 1.0802  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.4724  time: 1.0440  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.5663  time: 0.9860  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.0647  time: 0.9614  data: 0.0000  max mem: 66110
Train: data epoch: [44]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.2042  time: 1.0603  data: 0.0000  max mem: 66110
Train: data epoch: [44] Total time: 1:58:49 (1.0027 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:10:59    time: 20.3652  data: 19.2901  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:05    time: 3.1630  data: 1.7544  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:39    time: 1.4278  data: 0.0009  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:38    time: 1.2913  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:48    time: 1.2695  data: 0.0009  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:12    time: 1.4016  data: 0.0009  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:00    time: 1.2457  data: 0.0009  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:15    time: 1.2431  data: 0.0009  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:43    time: 1.4487  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:25    time: 1.5160  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:12    time: 1.5860  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:48    time: 1.5497  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:09    time: 1.3880  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:51    time: 1.4030  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:17    time: 1.3965  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:50    time: 1.3051  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:26    time: 1.3542  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:55    time: 1.2910  data: 0.0009  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:35    time: 1.3035  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:25    time: 1.4896  data: 0.0009  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:02    time: 1.4445  data: 0.0009  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:52    time: 1.4479  data: 0.0009  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:31    time: 1.4603  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:08    time: 1.2961  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:50    time: 1.3154  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:34    time: 1.4034  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:25    time: 1.5342  data: 0.0009  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:02    time: 1.4275  data: 0.0009  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:43    time: 1.2596  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:34    time: 1.4789  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:12    time: 1.4120  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:53    time: 1.2326  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:38    time: 1.3539  data: 0.0011  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:20    time: 1.3579  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:08    time: 1.4286  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:53    time: 1.4803  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:33    time: 1.2904  data: 0.0009  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:16    time: 1.2357  data: 0.0009  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:04    time: 1.4402  data: 0.0009  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:50    time: 1.5028  data: 0.0009  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:36    time: 1.4563  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:21    time: 1.4385  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:07    time: 1.4303  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:51    time: 1.4085  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:32    time: 1.2336  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:17    time: 1.2252  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:01    time: 1.3292  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:45    time: 1.3032  data: 0.0009  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:31    time: 1.3707  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:18    time: 1.4777  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:06    time: 1.5707  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:52    time: 1.5519  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:38    time: 1.4362  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:24    time: 1.4463  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:10    time: 1.4629  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:55    time: 1.4062  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:37    time: 1.1851  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:22    time: 1.1725  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:07    time: 1.3559  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:51    time: 1.2669  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:38    time: 1.3606  data: 0.0009  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:24    time: 1.5048  data: 0.0009  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:09    time: 1.4119  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:55    time: 1.3810  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:40    time: 1.3414  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:26    time: 1.3667  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:11    time: 1.3626  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:57    time: 1.3518  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:43    time: 1.4253  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:29    time: 1.4699  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:15    time: 1.4112  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:08:59    time: 1.2169  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:44    time: 1.1551  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:28    time: 1.1314  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:13    time: 1.1109  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:07:59    time: 1.3058  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:45    time: 1.4218  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:31    time: 1.3928  data: 0.0009  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:18    time: 1.4865  data: 0.0009  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:03    time: 1.3086  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:49    time: 1.3265  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:36    time: 1.5250  data: 0.0009  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:22    time: 1.4569  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:08    time: 1.5217  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:54    time: 1.4618  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:40    time: 1.4059  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:26    time: 1.3859  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:12    time: 1.3665  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:58    time: 1.5004  data: 0.0009  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:44    time: 1.3443  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:30    time: 1.2409  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:16    time: 1.3999  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:02    time: 1.3793  data: 0.0009  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:47    time: 1.3088  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:33    time: 1.3577  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:19    time: 1.3711  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:05    time: 1.3867  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:52    time: 1.4402  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:37    time: 1.3642  data: 0.0009  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.3918  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:10    time: 1.5136  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4788  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3004  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:27    time: 1.1592  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:13    time: 1.2384  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:00:59    time: 1.3249  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3809  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4446  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3631  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3354  data: 0.0009  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3047  data: 0.0402  max mem: 66110
Evaluation Total time: 0:25:23 (1.3940 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_44_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [45]  [   0/7110]  eta: 2 days, 6:39:03  lr: 0.000010  loss: 1.7676  time: 27.6714  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [  50/7110]  eta: 3:02:17  lr: 0.000010  loss: 0.0174  time: 1.0262  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 100/7110]  eta: 2:31:52  lr: 0.000010  loss: 0.1552  time: 1.0806  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 150/7110]  eta: 2:21:22  lr: 0.000010  loss: 0.1651  time: 1.0129  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 200/7110]  eta: 2:13:43  lr: 0.000010  loss: 0.1780  time: 0.9663  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 250/7110]  eta: 2:09:28  lr: 0.000010  loss: 0.0357  time: 1.0765  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 300/7110]  eta: 2:05:26  lr: 0.000010  loss: 0.2878  time: 0.9956  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 350/7110]  eta: 2:02:54  lr: 0.000010  loss: 0.0541  time: 1.0170  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 400/7110]  eta: 2:01:03  lr: 0.000010  loss: 0.4411  time: 1.0279  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 450/7110]  eta: 1:58:46  lr: 0.000010  loss: 0.1805  time: 0.9987  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 500/7110]  eta: 1:56:41  lr: 0.000010  loss: 0.1574  time: 0.9663  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 550/7110]  eta: 1:55:13  lr: 0.000010  loss: 0.1564  time: 1.0404  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 600/7110]  eta: 1:53:34  lr: 0.000010  loss: 0.0382  time: 1.0236  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 650/7110]  eta: 1:52:17  lr: 0.000010  loss: 0.1781  time: 1.0225  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 700/7110]  eta: 1:51:00  lr: 0.000010  loss: 0.5042  time: 0.9823  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 750/7110]  eta: 1:49:44  lr: 0.000010  loss: 0.0982  time: 0.9690  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 800/7110]  eta: 1:48:39  lr: 0.000010  loss: 0.0214  time: 0.9904  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 850/7110]  eta: 1:47:29  lr: 0.000010  loss: 0.2244  time: 0.9652  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 900/7110]  eta: 1:46:28  lr: 0.000010  loss: 0.3775  time: 0.9883  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [ 950/7110]  eta: 1:45:31  lr: 0.000010  loss: 0.7337  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1000/7110]  eta: 1:44:35  lr: 0.000010  loss: 0.1864  time: 1.0374  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1050/7110]  eta: 1:43:29  lr: 0.000010  loss: 0.1590  time: 0.9915  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1100/7110]  eta: 1:42:31  lr: 0.000010  loss: 0.1881  time: 0.9662  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1150/7110]  eta: 1:41:34  lr: 0.000010  loss: 0.0157  time: 1.0397  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1200/7110]  eta: 1:40:29  lr: 0.000010  loss: 0.3400  time: 0.9646  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1250/7110]  eta: 1:39:21  lr: 0.000010  loss: 0.0359  time: 0.9218  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1300/7110]  eta: 1:38:29  lr: 0.000010  loss: 0.1215  time: 0.9940  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1350/7110]  eta: 1:37:37  lr: 0.000010  loss: 0.0567  time: 0.9860  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1400/7110]  eta: 1:36:37  lr: 0.000010  loss: 0.6017  time: 0.9410  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1450/7110]  eta: 1:35:44  lr: 0.000010  loss: 0.0348  time: 1.0290  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1500/7110]  eta: 1:34:51  lr: 0.000010  loss: 0.2534  time: 1.0390  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1550/7110]  eta: 1:34:02  lr: 0.000010  loss: 0.2091  time: 1.0201  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1600/7110]  eta: 1:33:06  lr: 0.000010  loss: 1.5607  time: 1.0024  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1650/7110]  eta: 1:32:15  lr: 0.000010  loss: 0.0366  time: 1.0593  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1700/7110]  eta: 1:31:27  lr: 0.000010  loss: 0.3507  time: 1.0303  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1750/7110]  eta: 1:30:33  lr: 0.000010  loss: 0.4456  time: 1.0365  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1800/7110]  eta: 1:29:39  lr: 0.000010  loss: 0.2129  time: 0.9523  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1850/7110]  eta: 1:28:44  lr: 0.000010  loss: 0.2357  time: 0.9759  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1900/7110]  eta: 1:27:49  lr: 0.000010  loss: 0.1838  time: 1.0125  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [1950/7110]  eta: 1:27:01  lr: 0.000010  loss: 0.6154  time: 1.0771  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2000/7110]  eta: 1:26:10  lr: 0.000010  loss: 0.1522  time: 1.0092  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2050/7110]  eta: 1:25:17  lr: 0.000010  loss: 0.1691  time: 0.9857  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2100/7110]  eta: 1:24:27  lr: 0.000010  loss: 0.0793  time: 0.9975  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2150/7110]  eta: 1:23:40  lr: 0.000010  loss: 0.1817  time: 1.0252  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2200/7110]  eta: 1:22:45  lr: 0.000010  loss: 0.3093  time: 0.9925  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2250/7110]  eta: 1:21:55  lr: 0.000010  loss: 0.1641  time: 1.0426  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2300/7110]  eta: 1:21:05  lr: 0.000010  loss: 0.5327  time: 1.0227  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2350/7110]  eta: 1:20:14  lr: 0.000010  loss: 0.1057  time: 1.0236  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2400/7110]  eta: 1:19:19  lr: 0.000010  loss: 0.2343  time: 0.9333  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2450/7110]  eta: 1:18:26  lr: 0.000010  loss: 0.3978  time: 0.9899  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2500/7110]  eta: 1:17:29  lr: 0.000010  loss: 0.1254  time: 0.9708  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2550/7110]  eta: 1:16:37  lr: 0.000010  loss: 0.1646  time: 0.9466  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2600/7110]  eta: 1:15:40  lr: 0.000010  loss: 0.1466  time: 0.8989  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2650/7110]  eta: 1:14:49  lr: 0.000010  loss: 1.4508  time: 1.0029  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2700/7110]  eta: 1:13:57  lr: 0.000010  loss: 0.0950  time: 1.0287  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2750/7110]  eta: 1:13:06  lr: 0.000010  loss: 0.1511  time: 1.0209  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2800/7110]  eta: 1:12:12  lr: 0.000010  loss: 0.6142  time: 1.0074  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2850/7110]  eta: 1:11:19  lr: 0.000010  loss: 0.2746  time: 0.9827  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2900/7110]  eta: 1:10:28  lr: 0.000010  loss: 0.2183  time: 1.0146  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [2950/7110]  eta: 1:09:36  lr: 0.000010  loss: 0.0764  time: 0.9600  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3000/7110]  eta: 1:08:46  lr: 0.000010  loss: 0.2547  time: 1.0079  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3050/7110]  eta: 1:07:57  lr: 0.000010  loss: 0.1309  time: 1.0563  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3100/7110]  eta: 1:07:09  lr: 0.000010  loss: 0.0996  time: 1.0365  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3150/7110]  eta: 1:06:19  lr: 0.000010  loss: 0.1206  time: 1.0006  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3200/7110]  eta: 1:05:27  lr: 0.000010  loss: 0.5545  time: 0.9733  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3250/7110]  eta: 1:04:37  lr: 0.000010  loss: 0.2106  time: 0.9729  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3300/7110]  eta: 1:03:49  lr: 0.000010  loss: 0.2230  time: 1.0299  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3350/7110]  eta: 1:02:58  lr: 0.000010  loss: 1.2363  time: 0.9525  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3400/7110]  eta: 1:02:10  lr: 0.000010  loss: 0.6036  time: 1.0428  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3450/7110]  eta: 1:01:17  lr: 0.000010  loss: 0.2239  time: 0.9336  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3500/7110]  eta: 1:00:25  lr: 0.000010  loss: 0.1358  time: 1.0202  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3550/7110]  eta: 0:59:37  lr: 0.000010  loss: 0.0941  time: 1.0458  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3600/7110]  eta: 0:58:46  lr: 0.000010  loss: 0.3119  time: 1.0374  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3650/7110]  eta: 0:57:55  lr: 0.000010  loss: 1.3553  time: 0.9558  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3700/7110]  eta: 0:57:03  lr: 0.000010  loss: 0.1990  time: 0.9894  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3750/7110]  eta: 0:56:11  lr: 0.000010  loss: 0.1140  time: 0.9261  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3800/7110]  eta: 0:55:19  lr: 0.000010  loss: 0.3694  time: 0.9695  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3850/7110]  eta: 0:54:29  lr: 0.000010  loss: 0.1062  time: 0.9792  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3900/7110]  eta: 0:53:39  lr: 0.000010  loss: 0.0690  time: 1.0285  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [3950/7110]  eta: 0:52:50  lr: 0.000010  loss: 0.4333  time: 1.0016  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4000/7110]  eta: 0:52:01  lr: 0.000010  loss: 0.1116  time: 1.0160  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4050/7110]  eta: 0:51:10  lr: 0.000010  loss: 0.0928  time: 0.9895  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4100/7110]  eta: 0:50:20  lr: 0.000010  loss: 0.0410  time: 1.0552  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4150/7110]  eta: 0:49:30  lr: 0.000010  loss: 0.3595  time: 1.0690  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4200/7110]  eta: 0:48:40  lr: 0.000010  loss: 0.1726  time: 0.9722  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4250/7110]  eta: 0:47:49  lr: 0.000010  loss: 0.1443  time: 1.0074  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4300/7110]  eta: 0:46:58  lr: 0.000010  loss: 0.1500  time: 1.0053  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4350/7110]  eta: 0:46:09  lr: 0.000010  loss: 0.1115  time: 1.0287  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4400/7110]  eta: 0:45:17  lr: 0.000010  loss: 0.1709  time: 0.9202  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4450/7110]  eta: 0:44:27  lr: 0.000010  loss: 0.1920  time: 1.0279  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4500/7110]  eta: 0:43:37  lr: 0.000010  loss: 1.0337  time: 1.0025  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4550/7110]  eta: 0:42:46  lr: 0.000010  loss: 0.2408  time: 0.9385  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4600/7110]  eta: 0:41:55  lr: 0.000010  loss: 0.3677  time: 0.9664  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4650/7110]  eta: 0:41:05  lr: 0.000010  loss: 0.2740  time: 1.0259  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4700/7110]  eta: 0:40:15  lr: 0.000010  loss: 0.0152  time: 1.0044  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4750/7110]  eta: 0:39:25  lr: 0.000010  loss: 0.1653  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4800/7110]  eta: 0:38:34  lr: 0.000010  loss: 0.0429  time: 1.0171  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4850/7110]  eta: 0:37:44  lr: 0.000010  loss: 0.1996  time: 0.9500  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4900/7110]  eta: 0:36:53  lr: 0.000010  loss: 0.4151  time: 0.9612  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [4950/7110]  eta: 0:36:04  lr: 0.000010  loss: 0.4648  time: 1.0655  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5000/7110]  eta: 0:35:14  lr: 0.000010  loss: 0.1549  time: 0.9955  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5050/7110]  eta: 0:34:24  lr: 0.000010  loss: 0.1194  time: 1.0200  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5100/7110]  eta: 0:33:34  lr: 0.000010  loss: 0.2005  time: 1.0247  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5150/7110]  eta: 0:32:45  lr: 0.000010  loss: 0.0117  time: 1.0591  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5200/7110]  eta: 0:31:54  lr: 0.000010  loss: 0.1923  time: 1.0319  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5250/7110]  eta: 0:31:04  lr: 0.000010  loss: 0.1337  time: 0.9747  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5300/7110]  eta: 0:30:14  lr: 0.000010  loss: 0.0452  time: 0.9676  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5350/7110]  eta: 0:29:24  lr: 0.000010  loss: 0.0037  time: 0.9999  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5400/7110]  eta: 0:28:33  lr: 0.000010  loss: 0.2444  time: 1.0084  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5450/7110]  eta: 0:27:43  lr: 0.000010  loss: 1.4782  time: 1.0364  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5500/7110]  eta: 0:26:52  lr: 0.000010  loss: 0.1840  time: 1.0090  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5550/7110]  eta: 0:26:02  lr: 0.000010  loss: 0.3128  time: 1.0123  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5600/7110]  eta: 0:25:12  lr: 0.000010  loss: 0.0307  time: 1.0267  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5650/7110]  eta: 0:24:22  lr: 0.000010  loss: 0.1663  time: 0.9840  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5700/7110]  eta: 0:23:32  lr: 0.000010  loss: 0.1051  time: 1.0199  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5750/7110]  eta: 0:22:43  lr: 0.000010  loss: 0.1409  time: 1.0478  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5800/7110]  eta: 0:21:52  lr: 0.000010  loss: 0.1286  time: 1.0123  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5850/7110]  eta: 0:21:02  lr: 0.000010  loss: 0.2810  time: 1.0576  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5900/7110]  eta: 0:20:12  lr: 0.000010  loss: 0.2590  time: 0.9615  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [5950/7110]  eta: 0:19:22  lr: 0.000010  loss: 0.6409  time: 1.0522  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6000/7110]  eta: 0:18:32  lr: 0.000010  loss: 0.4841  time: 1.0081  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.0560  time: 1.0472  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6100/7110]  eta: 0:16:52  lr: 0.000010  loss: 0.1735  time: 0.9618  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.3333  time: 0.9687  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6200/7110]  eta: 0:15:11  lr: 0.000010  loss: 0.3987  time: 0.9571  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6250/7110]  eta: 0:14:21  lr: 0.000010  loss: 0.2444  time: 0.9576  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6300/7110]  eta: 0:13:31  lr: 0.000010  loss: 0.0887  time: 0.9584  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.2312  time: 0.9446  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.1683  time: 0.9610  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.0823  time: 1.0337  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.1369  time: 0.9477  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6550/7110]  eta: 0:09:20  lr: 0.000010  loss: 0.3602  time: 0.9824  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.2307  time: 1.0525  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.3186  time: 1.0227  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.2741  time: 1.0814  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.3377  time: 1.0020  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.0740  time: 1.0401  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.2131  time: 0.9556  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.2018  time: 0.9408  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 1.7661  time: 1.0052  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.4055  time: 0.9976  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1062  time: 0.9698  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1421  time: 0.9721  data: 0.0000  max mem: 66110
Train: data epoch: [45]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.1754  time: 1.0491  data: 0.0000  max mem: 66110
Train: data epoch: [45] Total time: 1:58:41 (1.0016 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:11:21    time: 20.3856  data: 19.1200  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:31    time: 3.1320  data: 1.7390  max mem: 66110
Evaluation  [  20/1093]  eta: 0:40:36    time: 1.3650  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:33:55    time: 1.2459  data: 0.0011  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:26    time: 1.2879  data: 0.0011  max mem: 66110
Evaluation  [  50/1093]  eta: 0:29:49    time: 1.4070  data: 0.0011  max mem: 66110
Evaluation  [  60/1093]  eta: 0:27:58    time: 1.2848  data: 0.0012  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:17    time: 1.3069  data: 0.0012  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:45    time: 1.4633  data: 0.0012  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:30    time: 1.5309  data: 0.0013  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:18    time: 1.6089  data: 0.0013  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:54    time: 1.5637  data: 0.0011  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:15    time: 1.3924  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:55    time: 1.3940  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:20    time: 1.3824  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:49    time: 1.2746  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:26    time: 1.3285  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:55    time: 1.2953  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:35    time: 1.3039  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:25    time: 1.4857  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:03    time: 1.4550  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:58    time: 1.5232  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:38    time: 1.5420  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:15    time: 1.3163  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:52    time: 1.2612  data: 0.0011  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:38    time: 1.3648  data: 0.0011  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:28    time: 1.5530  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:05    time: 1.4183  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:48    time: 1.2898  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:39    time: 1.5218  data: 0.0011  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:15    time: 1.3962  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:54    time: 1.1641  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:38    time: 1.2992  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:21    time: 1.3526  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:08    time: 1.4063  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:53    time: 1.4743  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:33    time: 1.3074  data: 0.0011  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:16    time: 1.2380  data: 0.0011  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:05    time: 1.4412  data: 0.0011  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:50    time: 1.4948  data: 0.0011  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:37    time: 1.4659  data: 0.0012  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:22    time: 1.4595  data: 0.0013  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:08    time: 1.4271  data: 0.0012  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:52    time: 1.4018  data: 0.0011  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:33    time: 1.2543  data: 0.0013  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:19    time: 1.2833  data: 0.0012  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:03    time: 1.3542  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:46    time: 1.2893  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:33    time: 1.3680  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:19    time: 1.4786  data: 0.0012  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:08    time: 1.5842  data: 0.0012  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:55    time: 1.6135  data: 0.0012  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:41    time: 1.5086  data: 0.0012  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:27    time: 1.4568  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:13    time: 1.4826  data: 0.0011  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:58    time: 1.4425  data: 0.0012  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:41    time: 1.2519  data: 0.0011  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:26    time: 1.2352  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:12    time: 1.3683  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:55    time: 1.2614  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:42    time: 1.3638  data: 0.0011  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:28    time: 1.5371  data: 0.0011  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:14    time: 1.4336  data: 0.0011  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:59    time: 1.4013  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:44    time: 1.3523  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:31    time: 1.4116  data: 0.0011  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:15    time: 1.4028  data: 0.0011  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:01    time: 1.3540  data: 0.0011  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:47    time: 1.4400  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:34    time: 1.5080  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:19    time: 1.4488  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:03    time: 1.2323  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:48    time: 1.1827  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:32    time: 1.1521  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:17    time: 1.1335  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:03    time: 1.3424  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:49    time: 1.4246  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:35    time: 1.3618  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:21    time: 1.4345  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:05    time: 1.2738  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:52    time: 1.3395  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:38    time: 1.5587  data: 0.0011  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:24    time: 1.4784  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:11    time: 1.5245  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:57    time: 1.4598  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:43    time: 1.3977  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:28    time: 1.3894  data: 0.0011  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:14    time: 1.3825  data: 0.0011  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:00    time: 1.5083  data: 0.0011  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:46    time: 1.3586  data: 0.0011  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:32    time: 1.2575  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:18    time: 1.4052  data: 0.0011  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:03    time: 1.4112  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:49    time: 1.3582  data: 0.0011  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:35    time: 1.4156  data: 0.0012  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:21    time: 1.4073  data: 0.0012  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.3916  data: 0.0011  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4062  data: 0.0011  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3246  data: 0.0011  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3841  data: 0.0011  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4917  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4650  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3158  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1989  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2962  data: 0.0011  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3785  data: 0.0011  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3944  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4402  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3618  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3431  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3265  data: 0.0498  max mem: 66110
Evaluation Total time: 0:25:35 (1.4046 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_45_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [46]  [   0/7110]  eta: 2 days, 5:47:07  lr: 0.000010  loss: 0.1340  time: 27.2331  data: 0.0001  max mem: 66110
Train: data epoch: [46]  [  50/7110]  eta: 3:01:19  lr: 0.000010  loss: 0.0226  time: 1.0519  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 100/7110]  eta: 2:28:12  lr: 0.000010  loss: 0.1245  time: 1.0035  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 150/7110]  eta: 2:15:53  lr: 0.000010  loss: 0.0995  time: 0.9413  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 200/7110]  eta: 2:09:28  lr: 0.000010  loss: 0.1573  time: 0.9605  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 250/7110]  eta: 2:05:39  lr: 0.000010  loss: 0.8266  time: 1.0707  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 300/7110]  eta: 2:02:38  lr: 0.000010  loss: 1.6648  time: 1.0166  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 350/7110]  eta: 2:00:19  lr: 0.000010  loss: 0.1172  time: 0.9901  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 400/7110]  eta: 1:58:54  lr: 0.000010  loss: 0.3202  time: 1.0575  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 450/7110]  eta: 1:57:34  lr: 0.000010  loss: 0.0800  time: 1.0201  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 500/7110]  eta: 1:56:10  lr: 0.000010  loss: 0.1010  time: 1.0071  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 550/7110]  eta: 1:55:01  lr: 0.000010  loss: 0.1593  time: 1.0011  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 600/7110]  eta: 1:53:38  lr: 0.000010  loss: 0.1310  time: 1.0099  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 650/7110]  eta: 1:52:22  lr: 0.000010  loss: 0.3203  time: 1.0307  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 700/7110]  eta: 1:51:01  lr: 0.000010  loss: 0.2358  time: 0.9880  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 750/7110]  eta: 1:49:40  lr: 0.000010  loss: 0.1208  time: 0.9781  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 800/7110]  eta: 1:48:34  lr: 0.000010  loss: 0.6072  time: 0.9635  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 850/7110]  eta: 1:47:36  lr: 0.000010  loss: 0.2939  time: 1.0254  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 900/7110]  eta: 1:46:47  lr: 0.000010  loss: 0.1083  time: 1.0313  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [ 950/7110]  eta: 1:45:40  lr: 0.000010  loss: 0.5813  time: 0.9592  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1000/7110]  eta: 1:44:35  lr: 0.000010  loss: 0.3789  time: 0.9964  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1050/7110]  eta: 1:43:33  lr: 0.000010  loss: 0.2547  time: 1.0382  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1100/7110]  eta: 1:42:24  lr: 0.000010  loss: 0.2056  time: 0.9340  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1150/7110]  eta: 1:41:20  lr: 0.000010  loss: 0.7532  time: 0.9610  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1200/7110]  eta: 1:40:33  lr: 0.000010  loss: 0.0789  time: 1.0381  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1250/7110]  eta: 1:39:39  lr: 0.000010  loss: 0.2560  time: 0.9926  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1300/7110]  eta: 1:38:48  lr: 0.000010  loss: 0.0784  time: 1.0352  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1350/7110]  eta: 1:38:03  lr: 0.000010  loss: 0.2326  time: 0.9906  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1400/7110]  eta: 1:37:11  lr: 0.000010  loss: 0.1846  time: 0.9897  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1450/7110]  eta: 1:36:22  lr: 0.000010  loss: 0.2612  time: 1.0570  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1500/7110]  eta: 1:35:25  lr: 0.000010  loss: 0.0179  time: 0.9954  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1550/7110]  eta: 1:34:25  lr: 0.000010  loss: 0.2553  time: 0.9727  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1600/7110]  eta: 1:33:33  lr: 0.000010  loss: 1.3565  time: 1.0460  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1650/7110]  eta: 1:32:39  lr: 0.000010  loss: 0.4445  time: 1.0254  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1700/7110]  eta: 1:31:44  lr: 0.000010  loss: 0.1030  time: 0.9763  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1750/7110]  eta: 1:30:51  lr: 0.000010  loss: 0.2532  time: 0.9853  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1800/7110]  eta: 1:29:57  lr: 0.000010  loss: 0.0485  time: 1.0066  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1850/7110]  eta: 1:29:05  lr: 0.000010  loss: 0.4542  time: 0.9486  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1900/7110]  eta: 1:28:10  lr: 0.000010  loss: 1.4900  time: 1.0129  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [1950/7110]  eta: 1:27:20  lr: 0.000010  loss: 0.4203  time: 1.0241  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2000/7110]  eta: 1:26:27  lr: 0.000010  loss: 0.0399  time: 0.9965  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2050/7110]  eta: 1:25:37  lr: 0.000010  loss: 0.2529  time: 1.0398  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2100/7110]  eta: 1:24:44  lr: 0.000010  loss: 0.1038  time: 0.9958  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2150/7110]  eta: 1:23:48  lr: 0.000010  loss: 0.2768  time: 0.9558  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2200/7110]  eta: 1:22:52  lr: 0.000010  loss: 0.1674  time: 0.9524  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2250/7110]  eta: 1:21:58  lr: 0.000010  loss: 0.0267  time: 0.9905  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2300/7110]  eta: 1:21:05  lr: 0.000010  loss: 0.1123  time: 0.9705  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2350/7110]  eta: 1:20:10  lr: 0.000010  loss: 0.3717  time: 0.9400  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2400/7110]  eta: 1:19:15  lr: 0.000010  loss: 0.0969  time: 0.9666  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2450/7110]  eta: 1:18:27  lr: 0.000010  loss: 0.1220  time: 1.0423  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2500/7110]  eta: 1:17:37  lr: 0.000010  loss: 0.0508  time: 1.0151  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2550/7110]  eta: 1:16:47  lr: 0.000010  loss: 0.1234  time: 1.0619  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2600/7110]  eta: 1:16:00  lr: 0.000010  loss: 0.0954  time: 1.0601  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2650/7110]  eta: 1:15:09  lr: 0.000010  loss: 0.3000  time: 0.9887  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2700/7110]  eta: 1:14:18  lr: 0.000010  loss: 0.4221  time: 1.0709  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2750/7110]  eta: 1:13:25  lr: 0.000010  loss: 0.2661  time: 0.9580  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2800/7110]  eta: 1:12:32  lr: 0.000010  loss: 0.6146  time: 0.9052  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2850/7110]  eta: 1:11:45  lr: 0.000010  loss: 0.3124  time: 1.0495  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2900/7110]  eta: 1:10:53  lr: 0.000010  loss: 0.5060  time: 0.9655  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [2950/7110]  eta: 1:10:02  lr: 0.000010  loss: 0.1744  time: 1.0020  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3000/7110]  eta: 1:09:09  lr: 0.000010  loss: 0.1156  time: 0.9696  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3050/7110]  eta: 1:08:19  lr: 0.000010  loss: 0.4855  time: 1.0943  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3100/7110]  eta: 1:07:28  lr: 0.000010  loss: 0.4615  time: 1.0020  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3150/7110]  eta: 1:06:39  lr: 0.000010  loss: 0.0604  time: 0.9703  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3200/7110]  eta: 1:05:48  lr: 0.000010  loss: 0.1540  time: 1.0247  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3250/7110]  eta: 1:04:58  lr: 0.000010  loss: 0.3116  time: 0.9730  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3300/7110]  eta: 1:04:09  lr: 0.000010  loss: 0.0527  time: 1.0621  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3350/7110]  eta: 1:03:18  lr: 0.000010  loss: 0.2218  time: 1.0035  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3400/7110]  eta: 1:02:29  lr: 0.000010  loss: 0.0928  time: 1.0362  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3450/7110]  eta: 1:01:38  lr: 0.000010  loss: 0.3336  time: 1.0271  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3500/7110]  eta: 1:00:49  lr: 0.000010  loss: 0.2994  time: 1.0050  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3550/7110]  eta: 0:59:56  lr: 0.000010  loss: 0.2842  time: 0.9295  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3600/7110]  eta: 0:59:03  lr: 0.000010  loss: 0.0872  time: 1.0057  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3650/7110]  eta: 0:58:12  lr: 0.000010  loss: 0.0580  time: 1.0157  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3700/7110]  eta: 0:57:22  lr: 0.000010  loss: 0.2612  time: 1.0229  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3750/7110]  eta: 0:56:32  lr: 0.000010  loss: 0.3234  time: 1.0321  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3800/7110]  eta: 0:55:41  lr: 0.000010  loss: 0.2615  time: 0.9910  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3850/7110]  eta: 0:54:52  lr: 0.000010  loss: 0.4672  time: 1.0168  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3900/7110]  eta: 0:54:02  lr: 0.000010  loss: 0.0382  time: 1.0216  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [3950/7110]  eta: 0:53:12  lr: 0.000010  loss: 0.1232  time: 0.9927  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4000/7110]  eta: 0:52:21  lr: 0.000010  loss: 0.0329  time: 0.9795  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4050/7110]  eta: 0:51:30  lr: 0.000010  loss: 0.3432  time: 0.9590  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4100/7110]  eta: 0:50:40  lr: 0.000010  loss: 0.1372  time: 1.0532  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4150/7110]  eta: 0:49:48  lr: 0.000010  loss: 0.2875  time: 0.9851  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4200/7110]  eta: 0:48:58  lr: 0.000010  loss: 0.2577  time: 1.0439  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4250/7110]  eta: 0:48:07  lr: 0.000010  loss: 0.0715  time: 0.9954  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4300/7110]  eta: 0:47:16  lr: 0.000010  loss: 0.2791  time: 1.0107  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4350/7110]  eta: 0:46:26  lr: 0.000010  loss: 0.0803  time: 0.9588  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4400/7110]  eta: 0:45:35  lr: 0.000010  loss: 0.4057  time: 1.0000  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4450/7110]  eta: 0:44:44  lr: 0.000010  loss: 1.2682  time: 1.0300  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4500/7110]  eta: 0:43:54  lr: 0.000010  loss: 0.0182  time: 1.0835  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4550/7110]  eta: 0:43:04  lr: 0.000010  loss: 0.1175  time: 0.9904  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4600/7110]  eta: 0:42:12  lr: 0.000010  loss: 0.2843  time: 0.9643  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4650/7110]  eta: 0:41:21  lr: 0.000010  loss: 0.0587  time: 1.0792  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4700/7110]  eta: 0:40:31  lr: 0.000010  loss: 0.6750  time: 1.0269  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4750/7110]  eta: 0:39:39  lr: 0.000010  loss: 0.3910  time: 0.9591  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4800/7110]  eta: 0:38:48  lr: 0.000010  loss: 0.2184  time: 0.9977  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4850/7110]  eta: 0:37:58  lr: 0.000010  loss: 0.3077  time: 1.0242  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4900/7110]  eta: 0:37:07  lr: 0.000010  loss: 0.9817  time: 1.0613  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [4950/7110]  eta: 0:36:16  lr: 0.000010  loss: 0.1542  time: 0.9781  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5000/7110]  eta: 0:35:25  lr: 0.000010  loss: 0.2935  time: 0.9909  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5050/7110]  eta: 0:34:33  lr: 0.000010  loss: 0.2419  time: 0.9547  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5100/7110]  eta: 0:33:43  lr: 0.000010  loss: 0.2870  time: 1.0001  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5150/7110]  eta: 0:32:53  lr: 0.000010  loss: 0.0677  time: 1.0448  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5200/7110]  eta: 0:32:03  lr: 0.000010  loss: 0.1022  time: 0.9892  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5250/7110]  eta: 0:31:12  lr: 0.000010  loss: 0.4302  time: 1.0102  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5300/7110]  eta: 0:30:21  lr: 0.000010  loss: 0.0238  time: 0.9204  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5350/7110]  eta: 0:29:30  lr: 0.000010  loss: 0.1162  time: 0.9155  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5400/7110]  eta: 0:28:40  lr: 0.000010  loss: 0.2921  time: 0.9693  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5450/7110]  eta: 0:27:50  lr: 0.000010  loss: 0.1774  time: 1.0186  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5500/7110]  eta: 0:27:00  lr: 0.000010  loss: 0.0461  time: 0.9751  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5550/7110]  eta: 0:26:09  lr: 0.000010  loss: 0.1649  time: 0.9487  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5600/7110]  eta: 0:25:19  lr: 0.000010  loss: 1.3578  time: 1.0110  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5650/7110]  eta: 0:24:28  lr: 0.000010  loss: 0.3028  time: 0.9836  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5700/7110]  eta: 0:23:37  lr: 0.000010  loss: 0.2385  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5750/7110]  eta: 0:22:47  lr: 0.000010  loss: 0.0999  time: 1.0063  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5800/7110]  eta: 0:21:56  lr: 0.000010  loss: 0.3022  time: 1.0506  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5850/7110]  eta: 0:21:06  lr: 0.000010  loss: 0.5953  time: 1.0269  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5900/7110]  eta: 0:20:16  lr: 0.000010  loss: 0.3070  time: 0.9420  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [5950/7110]  eta: 0:19:25  lr: 0.000010  loss: 0.0831  time: 1.0179  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6000/7110]  eta: 0:18:35  lr: 0.000010  loss: 0.3656  time: 0.9909  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6050/7110]  eta: 0:17:45  lr: 0.000010  loss: 0.1680  time: 0.9427  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6100/7110]  eta: 0:16:54  lr: 0.000010  loss: 0.1717  time: 1.0459  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6150/7110]  eta: 0:16:04  lr: 0.000010  loss: 0.6019  time: 0.9609  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6200/7110]  eta: 0:15:14  lr: 0.000010  loss: 0.2426  time: 1.0029  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6250/7110]  eta: 0:14:24  lr: 0.000010  loss: 0.3964  time: 1.0203  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6300/7110]  eta: 0:13:34  lr: 0.000010  loss: 0.3399  time: 1.0631  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6350/7110]  eta: 0:12:43  lr: 0.000010  loss: 0.3181  time: 0.9783  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6400/7110]  eta: 0:11:53  lr: 0.000010  loss: 0.0994  time: 0.9317  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6450/7110]  eta: 0:11:03  lr: 0.000010  loss: 0.3061  time: 1.0314  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.3378  time: 1.0110  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.2636  time: 1.0068  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6600/7110]  eta: 0:08:32  lr: 0.000010  loss: 0.1815  time: 0.9378  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6650/7110]  eta: 0:07:42  lr: 0.000010  loss: 0.2409  time: 1.0364  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.3757  time: 1.0487  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.3713  time: 0.9353  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.0638  time: 0.9891  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6850/7110]  eta: 0:04:21  lr: 0.000010  loss: 0.6258  time: 1.0126  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1190  time: 0.9979  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 1.6381  time: 0.9941  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.2155  time: 0.9591  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1658  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1170  time: 0.9821  data: 0.0000  max mem: 66110
Train: data epoch: [46]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.3070  time: 1.0930  data: 0.0000  max mem: 66110
Train: data epoch: [46] Total time: 1:58:59 (1.0042 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:27:23    time: 21.2660  data: 20.0260  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:56    time: 3.2105  data: 1.8215  max mem: 66110
Evaluation  [  20/1093]  eta: 0:42:37    time: 1.4390  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:35:18    time: 1.3233  data: 0.0011  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:27    time: 1.2892  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:42    time: 1.4162  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:42    time: 1.2940  data: 0.0009  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:51    time: 1.2955  data: 0.0009  max mem: 66110
Evaluation  [  80/1093]  eta: 0:27:09    time: 1.4279  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:49    time: 1.4996  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:35    time: 1.6025  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:26:09    time: 1.5603  data: 0.0009  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:28    time: 1.3873  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:25:08    time: 1.4009  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:33    time: 1.3989  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:24:06    time: 1.3215  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:44    time: 1.3858  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:23:12    time: 1.3163  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:52    time: 1.3151  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:39    time: 1.4837  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:19    time: 1.4717  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:22:13    time: 1.5487  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:51    time: 1.5308  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:27    time: 1.3020  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:21:07    time: 1.2964  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:50    time: 1.3828  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:42    time: 1.5646  data: 0.0011  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:21    time: 1.4921  data: 0.0011  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:20:02    time: 1.3127  data: 0.0011  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:52    time: 1.5040  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:29    time: 1.4161  data: 0.0011  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:19:07    time: 1.1847  data: 0.0011  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:52    time: 1.3133  data: 0.0012  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:34    time: 1.3743  data: 0.0012  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:20    time: 1.4162  data: 0.0013  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:18:05    time: 1.4647  data: 0.0012  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:46    time: 1.3341  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:28    time: 1.2747  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:17    time: 1.4505  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:17:02    time: 1.5148  data: 0.0011  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:48    time: 1.4815  data: 0.0011  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:35    time: 1.5305  data: 0.0012  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:21    time: 1.5105  data: 0.0011  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:16:05    time: 1.4159  data: 0.0011  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:45    time: 1.2396  data: 0.0011  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:30    time: 1.2772  data: 0.0012  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:15    time: 1.3909  data: 0.0011  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:59    time: 1.3378  data: 0.0011  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:44    time: 1.3780  data: 0.0012  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:30    time: 1.4715  data: 0.0011  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:18    time: 1.5566  data: 0.0011  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:14:05    time: 1.6040  data: 0.0011  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:51    time: 1.5415  data: 0.0011  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:37    time: 1.4726  data: 0.0012  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:22    time: 1.4652  data: 0.0012  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:07    time: 1.4249  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:50    time: 1.2560  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:34    time: 1.2465  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:19    time: 1.3598  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:12:02    time: 1.2512  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:49    time: 1.3740  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:35    time: 1.5374  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:20    time: 1.4205  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:05    time: 1.3819  data: 0.0011  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:50    time: 1.3440  data: 0.0011  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:36    time: 1.3936  data: 0.0011  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:21    time: 1.3859  data: 0.0011  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:07    time: 1.3968  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:52    time: 1.4537  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:39    time: 1.4814  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:24    time: 1.4577  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:08    time: 1.2587  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:53    time: 1.2586  data: 0.0009  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:37    time: 1.2236  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:22    time: 1.1365  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:08    time: 1.3185  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:53    time: 1.4185  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:39    time: 1.3675  data: 0.0012  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:25    time: 1.4290  data: 0.0012  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:09    time: 1.2943  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:56    time: 1.3512  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:42    time: 1.5261  data: 0.0011  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:28    time: 1.4536  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:14    time: 1.5210  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:59    time: 1.4553  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:46    time: 1.4312  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:31    time: 1.4082  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:17    time: 1.3583  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:03    time: 1.5007  data: 0.0009  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:48    time: 1.3587  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:34    time: 1.2732  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:20    time: 1.4207  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:05    time: 1.4345  data: 0.0009  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:51    time: 1.3662  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:37    time: 1.3974  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:23    time: 1.4101  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:08    time: 1.3760  data: 0.0011  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:54    time: 1.4188  data: 0.0011  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:40    time: 1.3588  data: 0.0011  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:26    time: 1.3846  data: 0.0011  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:12    time: 1.4778  data: 0.0011  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.4555  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:43    time: 1.2930  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:29    time: 1.1346  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2273  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3392  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3910  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4431  data: 0.0011  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3535  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3089  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2870  data: 0.0458  max mem: 66110
Evaluation Total time: 0:25:42 (1.4116 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_46_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [47]  [   0/7110]  eta: 2 days, 5:14:48  lr: 0.000010  loss: 0.0733  time: 26.9604  data: 0.0001  max mem: 66110
Train: data epoch: [47]  [  50/7110]  eta: 2:54:43  lr: 0.000010  loss: 0.1582  time: 0.9528  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 100/7110]  eta: 2:23:16  lr: 0.000010  loss: 0.3069  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 150/7110]  eta: 2:15:15  lr: 0.000010  loss: 0.0551  time: 1.0662  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 200/7110]  eta: 2:07:57  lr: 0.000010  loss: 0.3160  time: 0.9091  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 250/7110]  eta: 2:03:57  lr: 0.000010  loss: 0.2711  time: 0.9592  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 300/7110]  eta: 2:02:08  lr: 0.000010  loss: 0.0997  time: 0.9906  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 350/7110]  eta: 1:59:56  lr: 0.000010  loss: 0.6048  time: 0.9960  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 400/7110]  eta: 1:58:06  lr: 0.000010  loss: 0.0493  time: 0.9938  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 450/7110]  eta: 1:57:15  lr: 0.000010  loss: 0.4329  time: 1.0143  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 500/7110]  eta: 1:55:42  lr: 0.000010  loss: 0.3287  time: 0.9781  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 550/7110]  eta: 1:54:03  lr: 0.000010  loss: 0.2536  time: 0.9707  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 600/7110]  eta: 1:52:52  lr: 0.000010  loss: 0.3780  time: 1.0260  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 650/7110]  eta: 1:52:01  lr: 0.000010  loss: 0.0870  time: 0.9432  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 700/7110]  eta: 1:50:58  lr: 0.000010  loss: 0.2653  time: 0.9882  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 750/7110]  eta: 1:49:39  lr: 0.000010  loss: 0.5546  time: 0.9624  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 800/7110]  eta: 1:48:32  lr: 0.000010  loss: 0.1620  time: 0.9670  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 850/7110]  eta: 1:47:31  lr: 0.000010  loss: 0.0143  time: 1.0202  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 900/7110]  eta: 1:46:25  lr: 0.000010  loss: 0.1499  time: 1.0235  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [ 950/7110]  eta: 1:45:26  lr: 0.000010  loss: 0.2190  time: 0.9646  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1000/7110]  eta: 1:44:29  lr: 0.000010  loss: 0.1756  time: 1.0307  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1050/7110]  eta: 1:43:30  lr: 0.000010  loss: 0.5135  time: 0.9588  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1100/7110]  eta: 1:42:37  lr: 0.000010  loss: 0.4086  time: 1.0365  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1150/7110]  eta: 1:41:29  lr: 0.000010  loss: 0.1488  time: 0.9313  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1200/7110]  eta: 1:40:36  lr: 0.000010  loss: 0.0419  time: 1.0832  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1250/7110]  eta: 1:39:41  lr: 0.000010  loss: 0.3317  time: 1.0135  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1300/7110]  eta: 1:38:36  lr: 0.000010  loss: 0.0900  time: 0.9775  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1350/7110]  eta: 1:37:44  lr: 0.000010  loss: 0.1470  time: 1.0701  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1400/7110]  eta: 1:36:52  lr: 0.000010  loss: 0.1462  time: 1.0687  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1450/7110]  eta: 1:35:59  lr: 0.000010  loss: 0.4190  time: 0.9852  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1500/7110]  eta: 1:35:09  lr: 0.000010  loss: 0.2812  time: 0.9852  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1550/7110]  eta: 1:34:05  lr: 0.000010  loss: 0.3079  time: 0.9476  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1600/7110]  eta: 1:33:11  lr: 0.000010  loss: 0.0699  time: 1.0113  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1650/7110]  eta: 1:32:16  lr: 0.000010  loss: 0.6510  time: 0.9952  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1700/7110]  eta: 1:31:32  lr: 0.000010  loss: 0.0128  time: 1.0572  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1750/7110]  eta: 1:30:39  lr: 0.000010  loss: 0.6555  time: 0.9875  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1800/7110]  eta: 1:29:43  lr: 0.000010  loss: 0.0936  time: 0.9597  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1850/7110]  eta: 1:28:52  lr: 0.000010  loss: 0.2463  time: 1.0238  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1900/7110]  eta: 1:28:06  lr: 0.000010  loss: 0.2976  time: 1.0195  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [1950/7110]  eta: 1:27:10  lr: 0.000010  loss: 0.0519  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2000/7110]  eta: 1:26:15  lr: 0.000010  loss: 0.5027  time: 1.0123  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2050/7110]  eta: 1:25:26  lr: 0.000010  loss: 0.3241  time: 1.0229  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2100/7110]  eta: 1:24:35  lr: 0.000010  loss: 0.3258  time: 1.0245  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2150/7110]  eta: 1:23:42  lr: 0.000010  loss: 0.1662  time: 1.0180  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2200/7110]  eta: 1:22:47  lr: 0.000010  loss: 0.1182  time: 0.9412  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2250/7110]  eta: 1:21:58  lr: 0.000010  loss: 0.1755  time: 1.0390  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2300/7110]  eta: 1:21:06  lr: 0.000010  loss: 0.2081  time: 1.0353  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2350/7110]  eta: 1:20:14  lr: 0.000010  loss: 0.1272  time: 1.0482  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2400/7110]  eta: 1:19:26  lr: 0.000010  loss: 0.2510  time: 1.0356  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2450/7110]  eta: 1:18:36  lr: 0.000010  loss: 0.0795  time: 1.0016  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2500/7110]  eta: 1:17:46  lr: 0.000010  loss: 0.0710  time: 1.0486  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2550/7110]  eta: 1:16:54  lr: 0.000010  loss: 1.7661  time: 1.0266  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2600/7110]  eta: 1:16:04  lr: 0.000010  loss: 0.3463  time: 1.0459  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2650/7110]  eta: 1:15:11  lr: 0.000010  loss: 0.0559  time: 0.9591  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2700/7110]  eta: 1:14:17  lr: 0.000010  loss: 0.2158  time: 0.8975  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2750/7110]  eta: 1:13:29  lr: 0.000010  loss: 0.7164  time: 0.9995  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2800/7110]  eta: 1:12:37  lr: 0.000010  loss: 0.0319  time: 0.9828  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2850/7110]  eta: 1:11:48  lr: 0.000010  loss: 0.2400  time: 1.0157  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2900/7110]  eta: 1:10:58  lr: 0.000010  loss: 0.1896  time: 1.0323  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [2950/7110]  eta: 1:10:07  lr: 0.000010  loss: 0.0392  time: 1.0246  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3000/7110]  eta: 1:09:15  lr: 0.000010  loss: 0.0754  time: 0.9988  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3050/7110]  eta: 1:08:24  lr: 0.000010  loss: 0.5236  time: 1.0272  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3100/7110]  eta: 1:07:33  lr: 0.000010  loss: 0.3187  time: 1.0025  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3150/7110]  eta: 1:06:42  lr: 0.000010  loss: 0.0980  time: 1.0215  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3200/7110]  eta: 1:05:50  lr: 0.000010  loss: 0.1600  time: 1.0172  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3250/7110]  eta: 1:05:00  lr: 0.000010  loss: 0.2257  time: 1.0474  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3300/7110]  eta: 1:04:09  lr: 0.000010  loss: 0.0358  time: 1.0060  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3350/7110]  eta: 1:03:18  lr: 0.000010  loss: 0.1713  time: 1.0464  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3400/7110]  eta: 1:02:27  lr: 0.000010  loss: 0.0387  time: 1.0326  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3450/7110]  eta: 1:01:36  lr: 0.000010  loss: 0.0766  time: 1.0017  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3500/7110]  eta: 1:00:42  lr: 0.000010  loss: 0.1552  time: 0.9173  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3550/7110]  eta: 0:59:52  lr: 0.000010  loss: 0.1451  time: 1.0452  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3600/7110]  eta: 0:59:01  lr: 0.000010  loss: 0.3715  time: 0.9996  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3650/7110]  eta: 0:58:10  lr: 0.000010  loss: 0.1924  time: 0.9612  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3700/7110]  eta: 0:57:18  lr: 0.000010  loss: 0.6420  time: 0.9780  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3750/7110]  eta: 0:56:26  lr: 0.000010  loss: 0.2371  time: 0.9261  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3800/7110]  eta: 0:55:35  lr: 0.000010  loss: 0.1131  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3850/7110]  eta: 0:54:44  lr: 0.000010  loss: 0.1262  time: 0.9704  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3900/7110]  eta: 0:53:53  lr: 0.000010  loss: 0.3174  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [3950/7110]  eta: 0:53:03  lr: 0.000010  loss: 0.3122  time: 0.9797  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4000/7110]  eta: 0:52:12  lr: 0.000010  loss: 0.1573  time: 1.0088  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4050/7110]  eta: 0:51:21  lr: 0.000010  loss: 0.0084  time: 1.0363  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4100/7110]  eta: 0:50:30  lr: 0.000010  loss: 0.0453  time: 0.9664  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4150/7110]  eta: 0:49:41  lr: 0.000010  loss: 0.1404  time: 1.0808  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4200/7110]  eta: 0:48:50  lr: 0.000010  loss: 0.6711  time: 0.9640  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4250/7110]  eta: 0:47:59  lr: 0.000010  loss: 0.2491  time: 1.0569  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4300/7110]  eta: 0:47:08  lr: 0.000010  loss: 0.1449  time: 0.9960  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4350/7110]  eta: 0:46:17  lr: 0.000010  loss: 0.4992  time: 0.9382  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4400/7110]  eta: 0:45:27  lr: 0.000010  loss: 0.1468  time: 1.0562  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4450/7110]  eta: 0:44:36  lr: 0.000010  loss: 0.0734  time: 0.9681  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4500/7110]  eta: 0:43:46  lr: 0.000010  loss: 0.3782  time: 1.0267  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4550/7110]  eta: 0:42:56  lr: 0.000010  loss: 0.1632  time: 1.0556  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4600/7110]  eta: 0:42:05  lr: 0.000010  loss: 0.8149  time: 0.9952  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4650/7110]  eta: 0:41:13  lr: 0.000010  loss: 0.2633  time: 0.9449  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4700/7110]  eta: 0:40:23  lr: 0.000010  loss: 0.1852  time: 0.9319  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4750/7110]  eta: 0:39:33  lr: 0.000010  loss: 0.0997  time: 0.9776  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4800/7110]  eta: 0:38:42  lr: 0.000010  loss: 0.0524  time: 0.9860  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4850/7110]  eta: 0:37:52  lr: 0.000010  loss: 0.1273  time: 1.0032  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4900/7110]  eta: 0:37:01  lr: 0.000010  loss: 0.0934  time: 0.9796  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [4950/7110]  eta: 0:36:10  lr: 0.000010  loss: 0.2173  time: 0.9843  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5000/7110]  eta: 0:35:20  lr: 0.000010  loss: 0.0735  time: 1.0159  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5050/7110]  eta: 0:34:30  lr: 0.000010  loss: 0.1126  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5100/7110]  eta: 0:33:40  lr: 0.000010  loss: 0.1881  time: 1.0456  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5150/7110]  eta: 0:32:48  lr: 0.000010  loss: 0.1824  time: 0.9544  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5200/7110]  eta: 0:31:59  lr: 0.000010  loss: 0.2226  time: 1.0969  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5250/7110]  eta: 0:31:09  lr: 0.000010  loss: 0.0291  time: 1.0572  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5300/7110]  eta: 0:30:19  lr: 0.000010  loss: 0.3545  time: 1.0642  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5350/7110]  eta: 0:29:28  lr: 0.000010  loss: 0.0762  time: 1.0012  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5400/7110]  eta: 0:28:38  lr: 0.000010  loss: 0.0952  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5450/7110]  eta: 0:27:48  lr: 0.000010  loss: 0.1100  time: 0.9408  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5500/7110]  eta: 0:26:57  lr: 0.000010  loss: 0.1005  time: 0.9541  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5550/7110]  eta: 0:26:07  lr: 0.000010  loss: 0.2698  time: 0.9676  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5600/7110]  eta: 0:25:17  lr: 0.000010  loss: 0.0727  time: 1.0190  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5650/7110]  eta: 0:24:26  lr: 0.000010  loss: 0.0395  time: 0.9547  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5700/7110]  eta: 0:23:36  lr: 0.000010  loss: 0.1705  time: 1.0138  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5750/7110]  eta: 0:22:45  lr: 0.000010  loss: 0.2129  time: 1.0059  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5800/7110]  eta: 0:21:55  lr: 0.000010  loss: 0.2408  time: 0.9574  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5850/7110]  eta: 0:21:05  lr: 0.000010  loss: 0.0740  time: 0.9758  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5900/7110]  eta: 0:20:15  lr: 0.000010  loss: 1.6308  time: 1.0543  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [5950/7110]  eta: 0:19:24  lr: 0.000010  loss: 0.2041  time: 0.9805  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6000/7110]  eta: 0:18:34  lr: 0.000010  loss: 0.4371  time: 1.0207  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.0294  time: 1.0062  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.1418  time: 0.9188  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.0655  time: 0.9979  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.0185  time: 1.0140  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.4073  time: 0.9595  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.0584  time: 1.0172  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.1889  time: 0.9828  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.8782  time: 1.0282  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.2182  time: 1.0642  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.2468  time: 0.9707  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.1472  time: 1.0259  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.4826  time: 1.0497  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1523  time: 0.9430  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.3136  time: 1.0226  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.2742  time: 1.0267  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.0343  time: 0.9604  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 1.1679  time: 0.9925  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.0521  time: 1.0406  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.4423  time: 1.0046  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.2945  time: 0.9627  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.8671  time: 1.0470  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.2276  time: 0.9530  data: 0.0000  max mem: 66110
Train: data epoch: [47]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.2487  time: 1.0819  data: 0.0000  max mem: 66110
Train: data epoch: [47] Total time: 1:58:53 (1.0033 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:13:39    time: 20.5121  data: 19.2507  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:47    time: 3.1467  data: 1.7510  max mem: 66110
Evaluation  [  20/1093]  eta: 0:42:08    time: 1.4491  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:35:05    time: 1.3394  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:21    time: 1.3046  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:33    time: 1.4135  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:34    time: 1.2843  data: 0.0009  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:51    time: 1.3160  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:27:10    time: 1.4538  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:47    time: 1.4911  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:34    time: 1.5907  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:26:06    time: 1.5539  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:22    time: 1.3611  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:25:04    time: 1.3903  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:29    time: 1.4037  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:24:00    time: 1.2959  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:40    time: 1.3854  data: 0.0011  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:23:08    time: 1.3323  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:48    time: 1.3093  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:38    time: 1.5025  data: 0.0009  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:14    time: 1.4523  data: 0.0009  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:22:03    time: 1.4504  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:38    time: 1.4162  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:15    time: 1.2557  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:56    time: 1.3189  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:41    time: 1.4175  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:32    time: 1.5613  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:09    time: 1.4215  data: 0.0009  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:50    time: 1.2592  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:41    time: 1.4978  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:18    time: 1.4194  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:57    time: 1.1878  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:42    time: 1.3014  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:23    time: 1.3430  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:10    time: 1.3944  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:55    time: 1.4698  data: 0.0011  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:37    time: 1.3409  data: 0.0012  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:19    time: 1.2648  data: 0.0011  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:08    time: 1.4428  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:51    time: 1.4627  data: 0.0009  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:37    time: 1.3987  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:22    time: 1.4366  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:07    time: 1.3993  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:52    time: 1.3779  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:32    time: 1.2056  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:17    time: 1.2087  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:02    time: 1.3792  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:46    time: 1.3648  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:32    time: 1.3884  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:19    time: 1.4745  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:07    time: 1.5586  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:54    time: 1.5866  data: 0.0009  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:40    time: 1.5000  data: 0.0009  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:26    time: 1.4549  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:12    time: 1.4606  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:57    time: 1.4088  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:40    time: 1.2395  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:25    time: 1.2410  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:10    time: 1.3738  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:53    time: 1.2439  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:40    time: 1.3366  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:26    time: 1.5040  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:12    time: 1.4190  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:57    time: 1.3930  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:42    time: 1.3450  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:28    time: 1.3683  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:13    time: 1.3639  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:00    time: 1.4290  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:46    time: 1.4989  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:33    time: 1.5114  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:18    time: 1.4723  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:02    time: 1.2500  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:48    time: 1.2531  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:32    time: 1.2142  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:17    time: 1.1617  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:03    time: 1.3336  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:49    time: 1.3965  data: 0.0009  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:35    time: 1.3763  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:21    time: 1.4419  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:06    time: 1.3125  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:52    time: 1.3851  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:39    time: 1.5449  data: 0.0011  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:25    time: 1.4453  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:11    time: 1.5120  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:57    time: 1.4579  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:43    time: 1.3937  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:28    time: 1.3909  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:14    time: 1.3860  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:01    time: 1.5072  data: 0.0011  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:46    time: 1.3673  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:32    time: 1.2872  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:18    time: 1.4309  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:04    time: 1.4457  data: 0.0011  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:49    time: 1.3845  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:35    time: 1.4092  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:21    time: 1.3864  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.3585  data: 0.0011  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4268  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3589  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.4038  data: 0.0012  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4965  data: 0.0012  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.4427  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.2774  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1388  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2150  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3080  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3717  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4364  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3663  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3212  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2936  data: 0.0435  max mem: 66110
Evaluation Total time: 0:25:33 (1.4027 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_47_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [48]  [   0/7110]  eta: 2 days, 6:10:27  lr: 0.000010  loss: 0.3608  time: 27.4301  data: 0.0001  max mem: 66110
Train: data epoch: [48]  [  50/7110]  eta: 2:55:50  lr: 0.000010  loss: 0.0654  time: 0.9841  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 100/7110]  eta: 2:26:28  lr: 0.000010  loss: 0.2033  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 150/7110]  eta: 2:16:02  lr: 0.000010  loss: 0.1393  time: 0.9876  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 200/7110]  eta: 2:10:49  lr: 0.000010  loss: 0.2124  time: 1.0487  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 250/7110]  eta: 2:06:25  lr: 0.000010  loss: 0.1591  time: 0.9809  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 300/7110]  eta: 2:03:51  lr: 0.000010  loss: 0.2765  time: 1.0083  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 350/7110]  eta: 2:00:46  lr: 0.000010  loss: 0.0282  time: 0.9408  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 400/7110]  eta: 1:59:04  lr: 0.000010  loss: 0.0325  time: 1.0415  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 450/7110]  eta: 1:57:19  lr: 0.000010  loss: 0.1080  time: 0.9788  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 500/7110]  eta: 1:55:31  lr: 0.000010  loss: 0.0728  time: 0.9629  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 550/7110]  eta: 1:54:17  lr: 0.000010  loss: 0.2816  time: 0.9985  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 600/7110]  eta: 1:53:00  lr: 0.000010  loss: 0.1862  time: 1.0522  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 650/7110]  eta: 1:51:33  lr: 0.000010  loss: 0.0062  time: 0.9475  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 700/7110]  eta: 1:50:24  lr: 0.000010  loss: 0.0204  time: 1.0502  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 750/7110]  eta: 1:49:22  lr: 0.000010  loss: 0.1538  time: 1.0460  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 800/7110]  eta: 1:48:14  lr: 0.000010  loss: 0.3294  time: 0.9883  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 850/7110]  eta: 1:47:13  lr: 0.000010  loss: 0.1439  time: 1.0008  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 900/7110]  eta: 1:46:27  lr: 0.000010  loss: 0.0921  time: 1.0172  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [ 950/7110]  eta: 1:45:43  lr: 0.000010  loss: 0.0323  time: 0.9888  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1000/7110]  eta: 1:44:42  lr: 0.000010  loss: 0.4073  time: 0.9700  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1050/7110]  eta: 1:43:38  lr: 0.000010  loss: 0.0802  time: 0.9448  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1100/7110]  eta: 1:42:33  lr: 0.000010  loss: 0.1950  time: 0.9487  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1150/7110]  eta: 1:41:32  lr: 0.000010  loss: 0.1497  time: 0.9855  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1200/7110]  eta: 1:40:35  lr: 0.000010  loss: 0.0609  time: 1.0130  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1250/7110]  eta: 1:39:39  lr: 0.000010  loss: 0.0773  time: 0.9880  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1300/7110]  eta: 1:38:43  lr: 0.000010  loss: 0.3037  time: 0.9702  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1350/7110]  eta: 1:37:47  lr: 0.000010  loss: 0.0611  time: 1.0206  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1400/7110]  eta: 1:36:53  lr: 0.000010  loss: 0.0056  time: 0.9755  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1450/7110]  eta: 1:35:57  lr: 0.000010  loss: 0.0591  time: 1.0340  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1500/7110]  eta: 1:35:12  lr: 0.000010  loss: 0.2675  time: 0.9723  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1550/7110]  eta: 1:34:19  lr: 0.000010  loss: 0.0666  time: 0.9838  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1600/7110]  eta: 1:33:23  lr: 0.000010  loss: 0.2681  time: 0.9600  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1650/7110]  eta: 1:32:29  lr: 0.000010  loss: 0.3158  time: 1.0026  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1700/7110]  eta: 1:31:36  lr: 0.000010  loss: 0.3558  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1750/7110]  eta: 1:30:43  lr: 0.000010  loss: 0.1061  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1800/7110]  eta: 1:29:47  lr: 0.000010  loss: 0.1049  time: 0.9830  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1850/7110]  eta: 1:28:55  lr: 0.000010  loss: 0.1451  time: 0.9740  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1900/7110]  eta: 1:28:00  lr: 0.000010  loss: 0.0486  time: 0.9628  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [1950/7110]  eta: 1:27:09  lr: 0.000010  loss: 0.4321  time: 0.9853  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2000/7110]  eta: 1:26:13  lr: 0.000010  loss: 0.3346  time: 0.9553  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2050/7110]  eta: 1:25:20  lr: 0.000010  loss: 0.0184  time: 1.0315  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2100/7110]  eta: 1:24:26  lr: 0.000010  loss: 0.3425  time: 0.9644  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2150/7110]  eta: 1:23:36  lr: 0.000010  loss: 0.1099  time: 1.0105  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2200/7110]  eta: 1:22:43  lr: 0.000010  loss: 0.3078  time: 0.9690  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2250/7110]  eta: 1:21:49  lr: 0.000010  loss: 0.6763  time: 0.9743  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2300/7110]  eta: 1:21:00  lr: 0.000010  loss: 0.1697  time: 1.0137  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2350/7110]  eta: 1:20:07  lr: 0.000010  loss: 0.2323  time: 1.0036  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2400/7110]  eta: 1:19:15  lr: 0.000010  loss: 0.6529  time: 1.0601  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2450/7110]  eta: 1:18:23  lr: 0.000010  loss: 0.2341  time: 0.9981  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2500/7110]  eta: 1:17:32  lr: 0.000010  loss: 0.4737  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2550/7110]  eta: 1:16:44  lr: 0.000010  loss: 0.1896  time: 1.0748  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2600/7110]  eta: 1:15:52  lr: 0.000010  loss: 0.2475  time: 0.9653  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2650/7110]  eta: 1:15:00  lr: 0.000010  loss: 0.0891  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2700/7110]  eta: 1:14:06  lr: 0.000010  loss: 0.1559  time: 0.9573  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2750/7110]  eta: 1:13:13  lr: 0.000010  loss: 0.0533  time: 0.9656  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2800/7110]  eta: 1:12:22  lr: 0.000010  loss: 0.1028  time: 0.9938  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2850/7110]  eta: 1:11:30  lr: 0.000010  loss: 0.3048  time: 1.0125  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2900/7110]  eta: 1:10:38  lr: 0.000010  loss: 0.6413  time: 1.0014  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [2950/7110]  eta: 1:09:46  lr: 0.000010  loss: 0.6329  time: 0.9897  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3000/7110]  eta: 1:08:56  lr: 0.000010  loss: 0.1502  time: 1.0187  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3050/7110]  eta: 1:08:05  lr: 0.000010  loss: 0.3516  time: 1.0106  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3100/7110]  eta: 1:07:16  lr: 0.000010  loss: 0.0995  time: 0.9819  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3150/7110]  eta: 1:06:23  lr: 0.000010  loss: 0.9472  time: 0.9636  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3200/7110]  eta: 1:05:31  lr: 0.000010  loss: 0.4091  time: 0.9850  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3250/7110]  eta: 1:04:41  lr: 0.000010  loss: 0.1468  time: 0.9928  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3300/7110]  eta: 1:03:52  lr: 0.000010  loss: 0.3884  time: 0.9942  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3350/7110]  eta: 1:03:02  lr: 0.000010  loss: 0.6478  time: 1.0494  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3400/7110]  eta: 1:02:10  lr: 0.000010  loss: 0.3750  time: 0.9561  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3450/7110]  eta: 1:01:19  lr: 0.000010  loss: 0.3598  time: 0.9690  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3500/7110]  eta: 1:00:29  lr: 0.000010  loss: 0.1376  time: 1.0361  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3550/7110]  eta: 0:59:40  lr: 0.000010  loss: 0.0487  time: 1.0387  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3600/7110]  eta: 0:58:49  lr: 0.000010  loss: 0.1610  time: 0.9349  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3650/7110]  eta: 0:57:57  lr: 0.000010  loss: 0.0852  time: 0.9687  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3700/7110]  eta: 0:57:08  lr: 0.000010  loss: 0.0666  time: 1.0571  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3750/7110]  eta: 0:56:18  lr: 0.000010  loss: 0.1803  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3800/7110]  eta: 0:55:28  lr: 0.000010  loss: 0.0149  time: 0.9857  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3850/7110]  eta: 0:54:38  lr: 0.000010  loss: 0.1205  time: 0.9924  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3900/7110]  eta: 0:53:47  lr: 0.000010  loss: 0.0674  time: 0.9505  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [3950/7110]  eta: 0:52:55  lr: 0.000010  loss: 1.5863  time: 0.9551  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4000/7110]  eta: 0:52:05  lr: 0.000010  loss: 0.1219  time: 0.9507  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4050/7110]  eta: 0:51:14  lr: 0.000010  loss: 1.0124  time: 1.0219  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4100/7110]  eta: 0:50:22  lr: 0.000010  loss: 0.2504  time: 0.9376  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4150/7110]  eta: 0:49:32  lr: 0.000010  loss: 0.0354  time: 0.9714  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4200/7110]  eta: 0:48:41  lr: 0.000010  loss: 0.2161  time: 0.9820  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4250/7110]  eta: 0:47:51  lr: 0.000010  loss: 0.1377  time: 1.0147  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4300/7110]  eta: 0:47:00  lr: 0.000010  loss: 0.2439  time: 1.0436  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4350/7110]  eta: 0:46:09  lr: 0.000010  loss: 0.0291  time: 1.0248  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4400/7110]  eta: 0:45:18  lr: 0.000010  loss: 0.3192  time: 1.0012  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4450/7110]  eta: 0:44:27  lr: 0.000010  loss: 0.2279  time: 0.9786  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4500/7110]  eta: 0:43:37  lr: 0.000010  loss: 0.0794  time: 0.9885  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4550/7110]  eta: 0:42:47  lr: 0.000010  loss: 0.5066  time: 1.0314  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4600/7110]  eta: 0:41:57  lr: 0.000010  loss: 0.3586  time: 1.0452  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4650/7110]  eta: 0:41:07  lr: 0.000010  loss: 0.1880  time: 1.0467  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4700/7110]  eta: 0:40:17  lr: 0.000010  loss: 0.2712  time: 1.0284  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4750/7110]  eta: 0:39:26  lr: 0.000010  loss: 0.1795  time: 0.9668  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4800/7110]  eta: 0:38:35  lr: 0.000010  loss: 0.2613  time: 0.9732  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4850/7110]  eta: 0:37:44  lr: 0.000010  loss: 0.6554  time: 1.0152  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4900/7110]  eta: 0:36:56  lr: 0.000010  loss: 0.1768  time: 1.0682  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [4950/7110]  eta: 0:36:06  lr: 0.000010  loss: 0.1483  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5000/7110]  eta: 0:35:16  lr: 0.000010  loss: 1.0809  time: 0.9798  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5050/7110]  eta: 0:34:25  lr: 0.000010  loss: 0.2290  time: 1.0202  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5100/7110]  eta: 0:33:36  lr: 0.000010  loss: 0.0243  time: 1.0221  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5150/7110]  eta: 0:32:45  lr: 0.000010  loss: 0.4325  time: 0.9820  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5200/7110]  eta: 0:31:55  lr: 0.000010  loss: 0.7270  time: 1.0111  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5250/7110]  eta: 0:31:05  lr: 0.000010  loss: 0.1012  time: 1.0832  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5300/7110]  eta: 0:30:15  lr: 0.000010  loss: 1.6548  time: 1.0857  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5350/7110]  eta: 0:29:24  lr: 0.000010  loss: 0.1956  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5400/7110]  eta: 0:28:34  lr: 0.000010  loss: 0.3109  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5450/7110]  eta: 0:27:44  lr: 0.000010  loss: 0.2090  time: 0.9376  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5500/7110]  eta: 0:26:54  lr: 0.000010  loss: 0.1452  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5550/7110]  eta: 0:26:03  lr: 0.000010  loss: 0.0810  time: 0.9822  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5600/7110]  eta: 0:25:13  lr: 0.000010  loss: 0.0648  time: 0.9930  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5650/7110]  eta: 0:24:24  lr: 0.000010  loss: 0.0120  time: 1.0980  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.2508  time: 1.0162  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.1056  time: 1.0059  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.0530  time: 1.0436  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 0.5224  time: 0.9745  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.5426  time: 1.0180  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [5950/7110]  eta: 0:19:23  lr: 0.000010  loss: 0.0544  time: 0.9673  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.0759  time: 1.0013  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.2447  time: 0.9734  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.1653  time: 0.9652  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.1975  time: 0.9576  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.3359  time: 1.0085  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.0278  time: 0.9831  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.2318  time: 1.0253  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.0505  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.1167  time: 0.9816  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.3470  time: 1.0200  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.2314  time: 0.9929  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.0472  time: 1.0022  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.6402  time: 1.0212  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.3297  time: 0.9690  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.4098  time: 0.9882  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.3249  time: 0.9580  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.6281  time: 0.9222  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1883  time: 1.0130  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.0798  time: 0.9665  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1456  time: 0.9254  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.0721  time: 1.0103  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.0704  time: 1.0168  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.4246  time: 0.9833  data: 0.0000  max mem: 66110
Train: data epoch: [48]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0619  time: 1.1035  data: 0.0000  max mem: 66110
Train: data epoch: [48] Total time: 1:58:51 (1.0030 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:07:18    time: 20.1635  data: 18.9021  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:07    time: 3.1095  data: 1.7193  max mem: 66110
Evaluation  [  20/1093]  eta: 0:40:36    time: 1.3761  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:33:55    time: 1.2572  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:21    time: 1.2778  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:29:46    time: 1.3993  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:27:57    time: 1.2897  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:14    time: 1.3039  data: 0.0009  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:35    time: 1.4266  data: 0.0009  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:20    time: 1.5010  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:04    time: 1.5763  data: 0.0009  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:38    time: 1.5171  data: 0.0009  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:56    time: 1.3519  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:42    time: 1.3961  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:06    time: 1.3940  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:37    time: 1.2629  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:16    time: 1.3525  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:47    time: 1.3212  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:28    time: 1.3142  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:19    time: 1.4971  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:56    time: 1.4437  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:46    time: 1.4420  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:22    time: 1.4167  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:01    time: 1.2760  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:39    time: 1.2841  data: 0.0009  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:25    time: 1.3536  data: 0.0009  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:17    time: 1.5603  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:55    time: 1.4485  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:37    time: 1.2750  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:27    time: 1.4743  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:05    time: 1.3945  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:45    time: 1.1820  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:29    time: 1.3032  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:12    time: 1.3561  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:00    time: 1.4244  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:47    time: 1.5109  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:28    time: 1.3561  data: 0.0009  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:11    time: 1.2634  data: 0.0009  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:00    time: 1.4331  data: 0.0009  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:46    time: 1.5062  data: 0.0009  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:33    time: 1.4777  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:20    time: 1.5219  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:05    time: 1.4761  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:50    time: 1.3899  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:31    time: 1.2562  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:16    time: 1.2595  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:02    time: 1.3796  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:46    time: 1.3780  data: 0.0009  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:33    time: 1.4055  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:19    time: 1.4738  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:07    time: 1.5573  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:56    time: 1.6594  data: 0.0012  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:42    time: 1.5766  data: 0.0012  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:28    time: 1.4532  data: 0.0011  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:14    time: 1.4649  data: 0.0012  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:59    time: 1.4217  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:40    time: 1.1952  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:25    time: 1.1865  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:11    time: 1.3593  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:54    time: 1.2388  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:41    time: 1.3362  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:27    time: 1.5166  data: 0.0011  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:12    time: 1.4285  data: 0.0011  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:58    time: 1.3931  data: 0.0011  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:43    time: 1.3375  data: 0.0011  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:30    time: 1.4249  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:15    time: 1.4513  data: 0.0011  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:01    time: 1.4097  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:47    time: 1.4564  data: 0.0011  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:34    time: 1.5094  data: 0.0011  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:19    time: 1.4760  data: 0.0011  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:04    time: 1.2875  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:49    time: 1.2901  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:33    time: 1.2168  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:18    time: 1.1698  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:04    time: 1.3742  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:50    time: 1.4316  data: 0.0009  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:36    time: 1.3853  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:22    time: 1.4445  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:07    time: 1.2844  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:53    time: 1.3513  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:39    time: 1.5091  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:25    time: 1.3886  data: 0.0011  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:11    time: 1.4829  data: 0.0011  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:57    time: 1.4590  data: 0.0011  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:43    time: 1.4195  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:29    time: 1.3978  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:15    time: 1.3825  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:01    time: 1.5241  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:46    time: 1.3573  data: 0.0009  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:32    time: 1.2683  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:18    time: 1.4179  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:04    time: 1.4503  data: 0.0009  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:50    time: 1.3921  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:36    time: 1.4063  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:21    time: 1.4011  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.3698  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4189  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3543  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.4131  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.5167  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.4536  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.2744  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1491  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2625  data: 0.0009  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3498  data: 0.0009  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3741  data: 0.0009  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4315  data: 0.0009  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3881  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3592  data: 0.0011  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3338  data: 0.0424  max mem: 66110
Evaluation Total time: 0:25:37 (1.4063 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_48_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [49]  [   0/7110]  eta: 2 days, 6:12:01  lr: 0.000010  loss: 0.0776  time: 27.4433  data: 0.0001  max mem: 66110
Train: data epoch: [49]  [  50/7110]  eta: 2:57:58  lr: 0.000010  loss: 0.0359  time: 0.9743  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 100/7110]  eta: 2:25:05  lr: 0.000010  loss: 0.0515  time: 1.0194  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 150/7110]  eta: 2:13:42  lr: 0.000010  loss: 0.4986  time: 0.9997  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 200/7110]  eta: 2:08:56  lr: 0.000010  loss: 0.1856  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 250/7110]  eta: 2:04:45  lr: 0.000010  loss: 0.0541  time: 0.9847  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 300/7110]  eta: 2:01:18  lr: 0.000010  loss: 0.2977  time: 0.9709  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 350/7110]  eta: 1:59:21  lr: 0.000010  loss: 0.5577  time: 1.0088  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 400/7110]  eta: 1:57:36  lr: 0.000010  loss: 0.0317  time: 1.0174  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 450/7110]  eta: 1:55:56  lr: 0.000010  loss: 0.0130  time: 0.9603  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 500/7110]  eta: 1:53:59  lr: 0.000010  loss: 0.2713  time: 0.9427  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 550/7110]  eta: 1:52:36  lr: 0.000010  loss: 0.2451  time: 0.9588  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 600/7110]  eta: 1:51:23  lr: 0.000010  loss: 1.0422  time: 1.0214  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 650/7110]  eta: 1:50:17  lr: 0.000010  loss: 0.2497  time: 1.0215  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 700/7110]  eta: 1:48:54  lr: 0.000010  loss: 0.1088  time: 0.9397  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 750/7110]  eta: 1:47:54  lr: 0.000010  loss: 0.0518  time: 1.0111  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 800/7110]  eta: 1:47:07  lr: 0.000010  loss: 0.1855  time: 0.9763  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 850/7110]  eta: 1:46:09  lr: 0.000010  loss: 0.2616  time: 1.0301  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 900/7110]  eta: 1:45:12  lr: 0.000010  loss: 0.1208  time: 0.9597  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [ 950/7110]  eta: 1:44:04  lr: 0.000010  loss: 0.4367  time: 0.9393  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1000/7110]  eta: 1:43:11  lr: 0.000010  loss: 0.0989  time: 1.0195  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1050/7110]  eta: 1:42:17  lr: 0.000010  loss: 0.0986  time: 1.0282  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1100/7110]  eta: 1:41:35  lr: 0.000010  loss: 0.0380  time: 1.0998  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1150/7110]  eta: 1:40:40  lr: 0.000010  loss: 1.0124  time: 0.9635  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1200/7110]  eta: 1:39:44  lr: 0.000010  loss: 0.2210  time: 1.0258  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1250/7110]  eta: 1:38:54  lr: 0.000010  loss: 0.3085  time: 1.0717  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1300/7110]  eta: 1:38:02  lr: 0.000010  loss: 0.0969  time: 0.9907  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1350/7110]  eta: 1:37:05  lr: 0.000010  loss: 0.0527  time: 1.0024  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1400/7110]  eta: 1:36:15  lr: 0.000010  loss: 0.0066  time: 1.0299  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1450/7110]  eta: 1:35:23  lr: 0.000010  loss: 0.0252  time: 0.9706  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1500/7110]  eta: 1:34:28  lr: 0.000010  loss: 0.2467  time: 0.9691  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1550/7110]  eta: 1:33:34  lr: 0.000010  loss: 0.2407  time: 1.0381  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1600/7110]  eta: 1:32:37  lr: 0.000010  loss: 0.2765  time: 0.9511  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1650/7110]  eta: 1:31:55  lr: 0.000010  loss: 0.3462  time: 1.0359  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1700/7110]  eta: 1:31:00  lr: 0.000010  loss: 0.1715  time: 0.9763  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1750/7110]  eta: 1:30:09  lr: 0.000010  loss: 0.0476  time: 1.0066  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1800/7110]  eta: 1:29:18  lr: 0.000010  loss: 0.1486  time: 0.9527  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1850/7110]  eta: 1:28:29  lr: 0.000010  loss: 0.4694  time: 1.0474  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1900/7110]  eta: 1:27:37  lr: 0.000010  loss: 0.1891  time: 0.9882  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [1950/7110]  eta: 1:26:47  lr: 0.000010  loss: 0.4484  time: 1.0080  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2000/7110]  eta: 1:25:55  lr: 0.000010  loss: 0.0929  time: 0.9845  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2050/7110]  eta: 1:25:01  lr: 0.000010  loss: 0.1109  time: 0.9852  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2100/7110]  eta: 1:24:10  lr: 0.000010  loss: 0.1266  time: 0.9971  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2150/7110]  eta: 1:23:16  lr: 0.000010  loss: 0.0431  time: 0.9904  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2200/7110]  eta: 1:22:26  lr: 0.000010  loss: 0.0602  time: 0.9832  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2250/7110]  eta: 1:21:33  lr: 0.000010  loss: 0.2913  time: 0.9615  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2300/7110]  eta: 1:20:39  lr: 0.000010  loss: 0.4071  time: 0.9512  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2350/7110]  eta: 1:19:48  lr: 0.000010  loss: 0.2170  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2400/7110]  eta: 1:18:57  lr: 0.000010  loss: 0.1181  time: 1.0199  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2450/7110]  eta: 1:18:07  lr: 0.000010  loss: 0.4628  time: 1.0066  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2500/7110]  eta: 1:17:15  lr: 0.000010  loss: 0.2060  time: 0.9520  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2550/7110]  eta: 1:16:23  lr: 0.000010  loss: 0.3471  time: 0.9731  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2600/7110]  eta: 1:15:32  lr: 0.000010  loss: 0.1993  time: 0.9883  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2650/7110]  eta: 1:14:40  lr: 0.000010  loss: 0.2177  time: 0.9338  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2700/7110]  eta: 1:13:49  lr: 0.000010  loss: 0.0875  time: 0.9588  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2750/7110]  eta: 1:12:59  lr: 0.000010  loss: 0.0470  time: 1.0230  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2800/7110]  eta: 1:12:10  lr: 0.000010  loss: 0.0734  time: 1.0597  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2850/7110]  eta: 1:11:19  lr: 0.000010  loss: 0.1000  time: 0.9612  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2900/7110]  eta: 1:10:29  lr: 0.000010  loss: 1.6520  time: 1.0110  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [2950/7110]  eta: 1:09:37  lr: 0.000010  loss: 0.3048  time: 0.9662  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3000/7110]  eta: 1:08:45  lr: 0.000010  loss: 0.2578  time: 0.9492  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3050/7110]  eta: 1:07:53  lr: 0.000010  loss: 0.7760  time: 0.9768  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3100/7110]  eta: 1:07:03  lr: 0.000010  loss: 0.1210  time: 0.9654  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3150/7110]  eta: 1:06:12  lr: 0.000010  loss: 0.0250  time: 0.9871  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3200/7110]  eta: 1:05:19  lr: 0.000010  loss: 0.3365  time: 0.9709  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3250/7110]  eta: 1:04:27  lr: 0.000010  loss: 0.1572  time: 0.9612  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3300/7110]  eta: 1:03:35  lr: 0.000010  loss: 0.0658  time: 0.9810  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3350/7110]  eta: 1:02:45  lr: 0.000010  loss: 0.0216  time: 0.9810  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3400/7110]  eta: 1:01:56  lr: 0.000010  loss: 0.4719  time: 1.0101  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3450/7110]  eta: 1:01:05  lr: 0.000010  loss: 0.2266  time: 0.9722  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3500/7110]  eta: 1:00:15  lr: 0.000010  loss: 0.0351  time: 1.0582  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3550/7110]  eta: 0:59:26  lr: 0.000010  loss: 0.2883  time: 1.0177  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3600/7110]  eta: 0:58:36  lr: 0.000010  loss: 0.2096  time: 0.9858  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3650/7110]  eta: 0:57:48  lr: 0.000010  loss: 0.3551  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3700/7110]  eta: 0:56:57  lr: 0.000010  loss: 0.5952  time: 0.9649  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3750/7110]  eta: 0:56:07  lr: 0.000010  loss: 0.0325  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3800/7110]  eta: 0:55:19  lr: 0.000010  loss: 0.2251  time: 1.0673  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3850/7110]  eta: 0:54:27  lr: 0.000010  loss: 0.5587  time: 0.9783  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3900/7110]  eta: 0:53:38  lr: 0.000010  loss: 0.4890  time: 0.9181  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [3950/7110]  eta: 0:52:47  lr: 0.000010  loss: 0.1478  time: 0.9410  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4000/7110]  eta: 0:51:56  lr: 0.000010  loss: 0.1174  time: 0.9752  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4050/7110]  eta: 0:51:05  lr: 0.000010  loss: 0.1351  time: 0.9629  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4100/7110]  eta: 0:50:15  lr: 0.000010  loss: 0.1102  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4150/7110]  eta: 0:49:24  lr: 0.000010  loss: 0.6886  time: 1.0037  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4200/7110]  eta: 0:48:34  lr: 0.000010  loss: 0.1970  time: 0.9766  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4250/7110]  eta: 0:47:43  lr: 0.000010  loss: 0.1218  time: 0.9791  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4300/7110]  eta: 0:46:52  lr: 0.000010  loss: 0.1459  time: 0.9520  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4350/7110]  eta: 0:46:03  lr: 0.000010  loss: 0.3151  time: 1.0187  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4400/7110]  eta: 0:45:13  lr: 0.000010  loss: 0.4707  time: 1.0284  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4450/7110]  eta: 0:44:24  lr: 0.000010  loss: 0.2101  time: 1.0139  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4500/7110]  eta: 0:43:34  lr: 0.000010  loss: 0.4926  time: 0.9541  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4550/7110]  eta: 0:42:45  lr: 0.000010  loss: 0.2242  time: 1.0129  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4600/7110]  eta: 0:41:55  lr: 0.000010  loss: 0.0681  time: 1.0021  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4650/7110]  eta: 0:41:05  lr: 0.000010  loss: 0.0573  time: 0.9840  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4700/7110]  eta: 0:40:14  lr: 0.000010  loss: 0.0507  time: 0.9388  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4750/7110]  eta: 0:39:23  lr: 0.000010  loss: 0.2721  time: 1.0003  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4800/7110]  eta: 0:38:33  lr: 0.000010  loss: 0.1583  time: 1.0069  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4850/7110]  eta: 0:37:43  lr: 0.000010  loss: 0.3813  time: 1.0142  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4900/7110]  eta: 0:36:52  lr: 0.000010  loss: 0.0745  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [4950/7110]  eta: 0:36:02  lr: 0.000010  loss: 0.1009  time: 1.0087  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5000/7110]  eta: 0:35:13  lr: 0.000010  loss: 0.2844  time: 1.0603  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5050/7110]  eta: 0:34:23  lr: 0.000010  loss: 0.1919  time: 0.9640  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5100/7110]  eta: 0:33:33  lr: 0.000010  loss: 0.2845  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5150/7110]  eta: 0:32:43  lr: 0.000010  loss: 0.1210  time: 1.0335  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5200/7110]  eta: 0:31:53  lr: 0.000010  loss: 0.0879  time: 1.0055  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5250/7110]  eta: 0:31:03  lr: 0.000010  loss: 0.3126  time: 0.9712  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5300/7110]  eta: 0:30:13  lr: 0.000010  loss: 0.0731  time: 0.9992  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5350/7110]  eta: 0:29:23  lr: 0.000010  loss: 0.7441  time: 1.0594  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5400/7110]  eta: 0:28:33  lr: 0.000010  loss: 0.5456  time: 1.0292  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5450/7110]  eta: 0:27:43  lr: 0.000010  loss: 0.3040  time: 0.9540  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5500/7110]  eta: 0:26:53  lr: 0.000010  loss: 0.2449  time: 0.9932  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5550/7110]  eta: 0:26:03  lr: 0.000010  loss: 0.2286  time: 1.0283  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5600/7110]  eta: 0:25:13  lr: 0.000010  loss: 0.4482  time: 1.0367  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5650/7110]  eta: 0:24:23  lr: 0.000010  loss: 0.0752  time: 0.9817  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5700/7110]  eta: 0:23:32  lr: 0.000010  loss: 0.1128  time: 1.0283  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5750/7110]  eta: 0:22:42  lr: 0.000010  loss: 0.4669  time: 1.0080  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5800/7110]  eta: 0:21:52  lr: 0.000010  loss: 0.6399  time: 1.0039  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5850/7110]  eta: 0:21:02  lr: 0.000010  loss: 0.0695  time: 0.9982  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5900/7110]  eta: 0:20:12  lr: 0.000010  loss: 0.2537  time: 1.0405  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [5950/7110]  eta: 0:19:22  lr: 0.000010  loss: 0.2532  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6000/7110]  eta: 0:18:32  lr: 0.000010  loss: 0.4486  time: 1.0128  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.5149  time: 1.0831  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.0297  time: 0.9949  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6150/7110]  eta: 0:16:02  lr: 0.000010  loss: 0.0435  time: 1.0132  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6200/7110]  eta: 0:15:12  lr: 0.000010  loss: 0.0827  time: 0.9845  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6250/7110]  eta: 0:14:22  lr: 0.000010  loss: 0.3649  time: 0.9848  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.1432  time: 0.9408  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.2914  time: 0.9731  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.0261  time: 1.0207  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.0491  time: 1.0039  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.0421  time: 0.9802  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.1417  time: 0.9555  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.3461  time: 0.9908  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.2740  time: 1.0105  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.1990  time: 0.9744  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.3330  time: 0.9481  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.2542  time: 1.0373  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.3063  time: 1.0244  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1974  time: 1.1146  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1282  time: 0.9669  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.1562  time: 1.0026  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.2412  time: 1.0191  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1731  time: 1.0118  data: 0.0000  max mem: 66110
Train: data epoch: [49]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 1.5224  time: 1.1271  data: 0.0000  max mem: 66110
Train: data epoch: [49] Total time: 1:58:50 (1.0029 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:01:16    time: 19.8325  data: 18.5856  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:25    time: 3.1257  data: 1.6905  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:43    time: 1.4585  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:46    time: 1.3231  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:07    time: 1.3014  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:30    time: 1.4331  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:32    time: 1.3044  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:46    time: 1.3063  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:27:09    time: 1.4597  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:48    time: 1.5159  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:34    time: 1.5913  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:26:05    time: 1.5425  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:25    time: 1.3767  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:25:07    time: 1.4110  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:31    time: 1.4050  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:24:01    time: 1.2896  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:41    time: 1.3786  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:23:09    time: 1.3322  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:49    time: 1.3108  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:41    time: 1.5246  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:18    time: 1.4844  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:22:08    time: 1.4703  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:42    time: 1.4225  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:19    time: 1.2601  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:21:00    time: 1.3259  data: 0.0009  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:45    time: 1.4122  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:35    time: 1.5458  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:12    time: 1.4152  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:51    time: 1.2378  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:42    time: 1.4770  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:20    time: 1.4213  data: 0.0011  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:58    time: 1.1850  data: 0.0011  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:41    time: 1.2749  data: 0.0011  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:24    time: 1.3394  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:11    time: 1.4209  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:56    time: 1.4694  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:36    time: 1.3100  data: 0.0009  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:19    time: 1.2496  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:08    time: 1.4567  data: 0.0011  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:52    time: 1.4946  data: 0.0012  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:39    time: 1.4469  data: 0.0011  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:24    time: 1.4604  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:10    time: 1.4315  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:54    time: 1.4062  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:36    time: 1.2587  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:20    time: 1.2604  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:06    time: 1.3986  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:50    time: 1.3676  data: 0.0009  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:36    time: 1.3747  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:23    time: 1.4724  data: 0.0009  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:11    time: 1.5606  data: 0.0009  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:58    time: 1.6089  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:44    time: 1.5236  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:30    time: 1.4556  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:16    time: 1.4652  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:01    time: 1.4264  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:43    time: 1.2583  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:28    time: 1.2432  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:14    time: 1.3760  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:57    time: 1.2572  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:43    time: 1.3295  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:29    time: 1.5061  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:14    time: 1.4072  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:00    time: 1.3824  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:45    time: 1.3633  data: 0.0009  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:31    time: 1.3845  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:16    time: 1.3698  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:02    time: 1.3768  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:48    time: 1.4599  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:34    time: 1.5004  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:20    time: 1.4656  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:04    time: 1.2642  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:49    time: 1.2490  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:33    time: 1.2019  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:19    time: 1.1667  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:05    time: 1.3548  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:50    time: 1.4194  data: 0.0009  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:36    time: 1.3829  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:23    time: 1.4757  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:07    time: 1.3362  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:54    time: 1.3471  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:40    time: 1.5276  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:25    time: 1.4063  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:12    time: 1.4719  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:57    time: 1.4591  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:44    time: 1.4203  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:29    time: 1.4116  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:15    time: 1.3726  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:01    time: 1.4986  data: 0.0009  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:46    time: 1.3477  data: 0.0009  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:32    time: 1.2387  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:18    time: 1.3954  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:04    time: 1.4109  data: 0.0009  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:50    time: 1.3583  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:36    time: 1.4140  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:21    time: 1.4237  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.3924  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4191  data: 0.0011  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3476  data: 0.0011  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3942  data: 0.0011  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.5055  data: 0.0011  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.4980  data: 0.0012  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3159  data: 0.0011  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1352  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2339  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3376  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.4199  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4733  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3598  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3310  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3005  data: 0.0405  max mem: 66110
Evaluation Total time: 0:25:37 (1.4066 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_49_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [50]  [   0/7110]  eta: 2 days, 6:45:32  lr: 0.000010  loss: 0.2716  time: 27.7260  data: 0.0001  max mem: 66110
Train: data epoch: [50]  [  50/7110]  eta: 3:03:28  lr: 0.000010  loss: 0.3281  time: 1.0933  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 100/7110]  eta: 2:27:41  lr: 0.000010  loss: 0.4332  time: 0.9727  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 150/7110]  eta: 2:17:00  lr: 0.000010  loss: 0.5562  time: 0.9808  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 200/7110]  eta: 2:11:26  lr: 0.000010  loss: 0.0165  time: 0.9727  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 250/7110]  eta: 2:07:38  lr: 0.000010  loss: 0.1331  time: 0.9946  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 300/7110]  eta: 2:04:27  lr: 0.000010  loss: 0.0056  time: 1.0077  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 350/7110]  eta: 2:01:39  lr: 0.000010  loss: 0.0606  time: 0.9878  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 400/7110]  eta: 1:59:35  lr: 0.000010  loss: 0.4522  time: 0.9841  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 450/7110]  eta: 1:58:01  lr: 0.000010  loss: 0.3789  time: 0.9547  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 500/7110]  eta: 1:56:52  lr: 0.000010  loss: 0.3935  time: 1.0496  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 550/7110]  eta: 1:55:40  lr: 0.000010  loss: 0.0620  time: 1.0626  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 600/7110]  eta: 1:53:50  lr: 0.000010  loss: 0.3847  time: 0.9159  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 650/7110]  eta: 1:52:14  lr: 0.000010  loss: 0.0507  time: 0.9593  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 700/7110]  eta: 1:51:06  lr: 0.000010  loss: 0.4070  time: 0.9990  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 750/7110]  eta: 1:49:48  lr: 0.000010  loss: 0.0179  time: 0.9749  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 800/7110]  eta: 1:48:37  lr: 0.000010  loss: 0.0763  time: 0.9973  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 850/7110]  eta: 1:47:37  lr: 0.000010  loss: 0.0910  time: 0.9766  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 900/7110]  eta: 1:46:23  lr: 0.000010  loss: 0.0309  time: 1.0375  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [ 950/7110]  eta: 1:45:21  lr: 0.000010  loss: 0.4793  time: 1.0433  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1000/7110]  eta: 1:44:17  lr: 0.000010  loss: 0.3058  time: 1.0162  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1050/7110]  eta: 1:43:25  lr: 0.000010  loss: 0.2103  time: 1.0226  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1100/7110]  eta: 1:42:27  lr: 0.000010  loss: 0.0614  time: 1.0099  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1150/7110]  eta: 1:41:27  lr: 0.000010  loss: 0.0477  time: 0.9631  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1200/7110]  eta: 1:40:30  lr: 0.000010  loss: 0.3641  time: 0.9882  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1250/7110]  eta: 1:39:37  lr: 0.000010  loss: 0.1110  time: 1.0278  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1300/7110]  eta: 1:38:39  lr: 0.000010  loss: 0.3601  time: 1.0457  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1350/7110]  eta: 1:37:40  lr: 0.000010  loss: 0.1185  time: 0.9432  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1400/7110]  eta: 1:36:56  lr: 0.000010  loss: 0.2160  time: 1.0477  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1450/7110]  eta: 1:36:09  lr: 0.000010  loss: 0.6562  time: 1.0421  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1500/7110]  eta: 1:35:13  lr: 0.000010  loss: 0.4562  time: 0.9750  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1550/7110]  eta: 1:34:20  lr: 0.000010  loss: 0.0444  time: 0.9719  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1600/7110]  eta: 1:33:27  lr: 0.000010  loss: 0.0496  time: 1.0404  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1650/7110]  eta: 1:32:37  lr: 0.000010  loss: 0.2118  time: 0.9814  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1700/7110]  eta: 1:31:41  lr: 0.000010  loss: 0.1615  time: 0.9431  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1750/7110]  eta: 1:30:49  lr: 0.000010  loss: 0.1534  time: 1.0271  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1800/7110]  eta: 1:29:55  lr: 0.000010  loss: 0.2232  time: 1.0031  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1850/7110]  eta: 1:28:59  lr: 0.000010  loss: 1.2051  time: 0.9442  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1900/7110]  eta: 1:28:01  lr: 0.000010  loss: 0.1834  time: 0.9595  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [1950/7110]  eta: 1:27:08  lr: 0.000010  loss: 0.3260  time: 1.0265  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2000/7110]  eta: 1:26:14  lr: 0.000010  loss: 0.1138  time: 0.9645  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2050/7110]  eta: 1:25:24  lr: 0.000010  loss: 0.0600  time: 1.0094  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2100/7110]  eta: 1:24:30  lr: 0.000010  loss: 0.6463  time: 1.0724  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2150/7110]  eta: 1:23:37  lr: 0.000010  loss: 0.1720  time: 1.0058  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2200/7110]  eta: 1:22:46  lr: 0.000010  loss: 0.4146  time: 1.0178  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2250/7110]  eta: 1:21:56  lr: 0.000010  loss: 0.1652  time: 1.0469  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2300/7110]  eta: 1:21:03  lr: 0.000010  loss: 0.1370  time: 0.9499  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2350/7110]  eta: 1:20:10  lr: 0.000010  loss: 0.4546  time: 1.0134  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2400/7110]  eta: 1:19:22  lr: 0.000010  loss: 0.4731  time: 1.0398  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2450/7110]  eta: 1:18:28  lr: 0.000010  loss: 0.1253  time: 0.9682  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2500/7110]  eta: 1:17:33  lr: 0.000010  loss: 0.2103  time: 0.9902  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2550/7110]  eta: 1:16:43  lr: 0.000010  loss: 0.3230  time: 1.0251  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2600/7110]  eta: 1:15:54  lr: 0.000010  loss: 0.0481  time: 1.0182  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2650/7110]  eta: 1:15:05  lr: 0.000010  loss: 0.4407  time: 0.9795  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2700/7110]  eta: 1:14:15  lr: 0.000010  loss: 0.2860  time: 0.9897  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2750/7110]  eta: 1:13:24  lr: 0.000010  loss: 0.2595  time: 0.9936  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2800/7110]  eta: 1:12:30  lr: 0.000010  loss: 0.4205  time: 0.9717  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2850/7110]  eta: 1:11:38  lr: 0.000010  loss: 0.4166  time: 0.9598  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2900/7110]  eta: 1:10:46  lr: 0.000010  loss: 0.1765  time: 0.9768  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [2950/7110]  eta: 1:09:56  lr: 0.000010  loss: 0.2366  time: 1.0403  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3000/7110]  eta: 1:09:03  lr: 0.000010  loss: 0.4033  time: 0.9838  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3050/7110]  eta: 1:08:13  lr: 0.000010  loss: 0.6110  time: 1.0405  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3100/7110]  eta: 1:07:24  lr: 0.000010  loss: 0.0423  time: 1.0138  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3150/7110]  eta: 1:06:32  lr: 0.000010  loss: 0.0891  time: 0.9620  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3200/7110]  eta: 1:05:41  lr: 0.000010  loss: 0.2108  time: 0.9960  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3250/7110]  eta: 1:04:51  lr: 0.000010  loss: 0.0232  time: 1.0547  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3300/7110]  eta: 1:04:00  lr: 0.000010  loss: 0.1087  time: 0.9953  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3350/7110]  eta: 1:03:06  lr: 0.000010  loss: 0.2119  time: 0.9485  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3400/7110]  eta: 1:02:15  lr: 0.000010  loss: 1.4565  time: 1.0081  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3450/7110]  eta: 1:01:22  lr: 0.000010  loss: 0.7247  time: 1.0173  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3500/7110]  eta: 1:00:33  lr: 0.000010  loss: 0.6401  time: 0.9644  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3550/7110]  eta: 0:59:41  lr: 0.000010  loss: 0.0651  time: 0.9423  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3600/7110]  eta: 0:58:51  lr: 0.000010  loss: 0.0747  time: 1.0011  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3650/7110]  eta: 0:58:03  lr: 0.000010  loss: 0.3295  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3700/7110]  eta: 0:57:13  lr: 0.000010  loss: 0.2769  time: 1.0149  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3750/7110]  eta: 0:56:21  lr: 0.000010  loss: 0.0380  time: 0.9933  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3800/7110]  eta: 0:55:32  lr: 0.000010  loss: 0.3124  time: 1.0414  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3850/7110]  eta: 0:54:43  lr: 0.000010  loss: 0.1571  time: 1.0538  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3900/7110]  eta: 0:53:52  lr: 0.000010  loss: 0.3922  time: 0.9951  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [3950/7110]  eta: 0:53:01  lr: 0.000010  loss: 0.2753  time: 0.9545  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4000/7110]  eta: 0:52:10  lr: 0.000010  loss: 0.2847  time: 0.9910  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4050/7110]  eta: 0:51:20  lr: 0.000010  loss: 0.1803  time: 1.0436  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4100/7110]  eta: 0:50:29  lr: 0.000010  loss: 0.1991  time: 1.0017  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4150/7110]  eta: 0:49:38  lr: 0.000010  loss: 0.1319  time: 1.0266  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4200/7110]  eta: 0:48:47  lr: 0.000010  loss: 0.3761  time: 0.9513  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4250/7110]  eta: 0:47:56  lr: 0.000010  loss: 0.4224  time: 0.9388  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4300/7110]  eta: 0:47:06  lr: 0.000010  loss: 0.0465  time: 1.0416  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4350/7110]  eta: 0:46:15  lr: 0.000010  loss: 0.1776  time: 0.9430  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4400/7110]  eta: 0:45:26  lr: 0.000010  loss: 0.0335  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4450/7110]  eta: 0:44:36  lr: 0.000010  loss: 0.1689  time: 1.0421  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4500/7110]  eta: 0:43:46  lr: 0.000010  loss: 0.1225  time: 1.0257  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4550/7110]  eta: 0:42:56  lr: 0.000010  loss: 0.0715  time: 0.9472  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4600/7110]  eta: 0:42:06  lr: 0.000010  loss: 0.3154  time: 1.0173  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4650/7110]  eta: 0:41:15  lr: 0.000010  loss: 0.3548  time: 0.9545  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4700/7110]  eta: 0:40:25  lr: 0.000010  loss: 0.0497  time: 1.0048  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4750/7110]  eta: 0:39:34  lr: 0.000010  loss: 0.2313  time: 0.9589  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4800/7110]  eta: 0:38:43  lr: 0.000010  loss: 0.4504  time: 0.9482  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4850/7110]  eta: 0:37:53  lr: 0.000010  loss: 0.3447  time: 1.0297  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4900/7110]  eta: 0:37:03  lr: 0.000010  loss: 0.0289  time: 0.9673  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [4950/7110]  eta: 0:36:12  lr: 0.000010  loss: 0.4281  time: 0.9973  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5000/7110]  eta: 0:35:22  lr: 0.000010  loss: 0.0087  time: 0.9943  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5050/7110]  eta: 0:34:31  lr: 0.000010  loss: 0.2456  time: 1.0158  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5100/7110]  eta: 0:33:40  lr: 0.000010  loss: 0.2807  time: 0.9876  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5150/7110]  eta: 0:32:50  lr: 0.000010  loss: 0.1674  time: 0.9765  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5200/7110]  eta: 0:32:00  lr: 0.000010  loss: 0.4185  time: 1.0069  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5250/7110]  eta: 0:31:09  lr: 0.000010  loss: 0.0685  time: 1.0542  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5300/7110]  eta: 0:30:19  lr: 0.000010  loss: 1.1929  time: 1.0398  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5350/7110]  eta: 0:29:29  lr: 0.000010  loss: 0.2248  time: 0.9752  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5400/7110]  eta: 0:28:38  lr: 0.000010  loss: 1.2353  time: 0.9715  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5450/7110]  eta: 0:27:47  lr: 0.000010  loss: 0.1600  time: 1.0289  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5500/7110]  eta: 0:26:57  lr: 0.000010  loss: 0.0421  time: 1.0425  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5550/7110]  eta: 0:26:06  lr: 0.000010  loss: 0.2221  time: 1.0097  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5600/7110]  eta: 0:25:16  lr: 0.000010  loss: 0.1981  time: 0.9648  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5650/7110]  eta: 0:24:25  lr: 0.000010  loss: 0.1162  time: 0.9412  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.2868  time: 0.9718  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5750/7110]  eta: 0:22:44  lr: 0.000010  loss: 0.3616  time: 0.9972  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5800/7110]  eta: 0:21:54  lr: 0.000010  loss: 0.2104  time: 0.9695  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5850/7110]  eta: 0:21:04  lr: 0.000010  loss: 1.1497  time: 1.0518  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5900/7110]  eta: 0:20:14  lr: 0.000010  loss: 0.3276  time: 1.0390  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [5950/7110]  eta: 0:19:24  lr: 0.000010  loss: 0.0673  time: 0.9929  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6000/7110]  eta: 0:18:33  lr: 0.000010  loss: 0.0063  time: 0.9786  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6050/7110]  eta: 0:17:43  lr: 0.000010  loss: 0.0883  time: 1.0054  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6100/7110]  eta: 0:16:53  lr: 0.000010  loss: 0.1510  time: 0.9602  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.0272  time: 0.9725  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.0247  time: 1.0274  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.1566  time: 1.1084  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.2481  time: 1.0293  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.0839  time: 1.0349  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.1245  time: 1.0117  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.2744  time: 0.9884  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.1959  time: 0.9824  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.2326  time: 0.9328  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.2120  time: 0.9589  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.3113  time: 0.9680  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.1452  time: 0.8664  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.1435  time: 1.0229  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.0699  time: 0.9662  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1879  time: 0.9839  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.2221  time: 0.9890  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.1498  time: 1.0757  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.1990  time: 1.0117  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.0800  time: 0.9577  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.2743  time: 1.0125  data: 0.0000  max mem: 66110
Train: data epoch: [50]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.1283  time: 1.1192  data: 0.0000  max mem: 66110
Train: data epoch: [50] Total time: 1:58:48 (1.0026 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:36:45    time: 21.7798  data: 20.5236  max mem: 66110
Evaluation  [  10/1093]  eta: 1:00:19    time: 3.3421  data: 1.8670  max mem: 66110
Evaluation  [  20/1093]  eta: 0:43:44    time: 1.4793  data: 0.0012  max mem: 66110
Evaluation  [  30/1093]  eta: 0:36:09    time: 1.3255  data: 0.0011  max mem: 66110
Evaluation  [  40/1093]  eta: 0:33:03    time: 1.2934  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:31:14    time: 1.4196  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:29:08    time: 1.3018  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:28:19    time: 1.3158  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:27:34    time: 1.4506  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:27:11    time: 1.5020  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:56    time: 1.6090  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:26:28    time: 1.5684  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:46    time: 1.3902  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:25:25    time: 1.4079  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:44    time: 1.3677  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:24:10    time: 1.2354  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:48    time: 1.3411  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:23:18    time: 1.3425  data: 0.0011  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:59    time: 1.3482  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:46    time: 1.4984  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:28    time: 1.5004  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:22:21    time: 1.5758  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:55    time: 1.4802  data: 0.0011  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:34    time: 1.2897  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:21:16    time: 1.3771  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:59    time: 1.4244  data: 0.0011  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:49    time: 1.5451  data: 0.0011  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:25    time: 1.4294  data: 0.0011  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:20:05    time: 1.2666  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:56    time: 1.5000  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:32    time: 1.4153  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:19:10    time: 1.1834  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:52    time: 1.2571  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:33    time: 1.3014  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:20    time: 1.4056  data: 0.0011  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:18:05    time: 1.4750  data: 0.0012  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:45    time: 1.3295  data: 0.0011  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:29    time: 1.2875  data: 0.0011  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:16    time: 1.4590  data: 0.0011  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:17:02    time: 1.5018  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:48    time: 1.4660  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:32    time: 1.4341  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:17    time: 1.4092  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:16:02    time: 1.4287  data: 0.0009  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:42    time: 1.2400  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:26    time: 1.2192  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:11    time: 1.3804  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:55    time: 1.3458  data: 0.0009  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:41    time: 1.3746  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:27    time: 1.4753  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:15    time: 1.5544  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:14:02    time: 1.6018  data: 0.0011  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:47    time: 1.5045  data: 0.0012  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:33    time: 1.4343  data: 0.0011  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:19    time: 1.4644  data: 0.0011  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:04    time: 1.4357  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:46    time: 1.2435  data: 0.0011  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:31    time: 1.2195  data: 0.0010  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:16    time: 1.3615  data: 0.0011  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:59    time: 1.2435  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:45    time: 1.3244  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:31    time: 1.5030  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:17    time: 1.4267  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:02    time: 1.3938  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:47    time: 1.3700  data: 0.0009  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:33    time: 1.4252  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:18    time: 1.3892  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:05    time: 1.4542  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:51    time: 1.5414  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:38    time: 1.5312  data: 0.0011  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:23    time: 1.5126  data: 0.0011  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:08    time: 1.3331  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:53    time: 1.3400  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:37    time: 1.2382  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:22    time: 1.1025  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:08    time: 1.3029  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:53    time: 1.4181  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:39    time: 1.3703  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:25    time: 1.4685  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:10    time: 1.3378  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:56    time: 1.3614  data: 0.0011  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:42    time: 1.5334  data: 0.0012  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:28    time: 1.4326  data: 0.0011  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:14    time: 1.4964  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:06:00    time: 1.4561  data: 0.0011  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:45    time: 1.3798  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:31    time: 1.3349  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:16    time: 1.3386  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:03    time: 1.5058  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:48    time: 1.3782  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:34    time: 1.2964  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:20    time: 1.4489  data: 0.0011  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:05    time: 1.4567  data: 0.0011  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:51    time: 1.3687  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:37    time: 1.4606  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:23    time: 1.4948  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:09    time: 1.3895  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:54    time: 1.4202  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:40    time: 1.3655  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:26    time: 1.3841  data: 0.0011  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:12    time: 1.4943  data: 0.0011  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:58    time: 1.4938  data: 0.0011  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:43    time: 1.3372  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:29    time: 1.1596  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:15    time: 1.2423  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3445  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.4024  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4564  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3645  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3178  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.2877  data: 0.0412  max mem: 66110
Evaluation Total time: 0:25:46 (1.4147 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_50_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [51]  [   0/7110]  eta: 2 days, 5:45:27  lr: 0.000010  loss: 0.1262  time: 27.2190  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [  50/7110]  eta: 3:00:10  lr: 0.000010  loss: 0.2011  time: 0.9950  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 100/7110]  eta: 2:26:53  lr: 0.000010  loss: 1.0463  time: 1.0248  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 150/7110]  eta: 2:16:44  lr: 0.000010  loss: 0.1404  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 200/7110]  eta: 2:10:43  lr: 0.000010  loss: 0.1629  time: 0.9619  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 250/7110]  eta: 2:07:00  lr: 0.000010  loss: 0.3125  time: 1.0613  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 300/7110]  eta: 2:04:14  lr: 0.000010  loss: 0.1385  time: 1.0167  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 350/7110]  eta: 2:01:21  lr: 0.000010  loss: 0.0482  time: 0.9359  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 400/7110]  eta: 1:59:31  lr: 0.000010  loss: 0.1059  time: 1.0541  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 450/7110]  eta: 1:57:43  lr: 0.000010  loss: 0.0653  time: 1.0260  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 500/7110]  eta: 1:56:15  lr: 0.000010  loss: 0.1487  time: 1.0203  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 550/7110]  eta: 1:54:57  lr: 0.000010  loss: 0.2580  time: 1.0055  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 600/7110]  eta: 1:53:35  lr: 0.000010  loss: 0.1880  time: 1.0050  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 650/7110]  eta: 1:52:32  lr: 0.000010  loss: 0.0465  time: 1.0485  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 700/7110]  eta: 1:51:16  lr: 0.000010  loss: 0.4932  time: 0.9367  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 750/7110]  eta: 1:49:52  lr: 0.000010  loss: 0.2292  time: 0.9916  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 800/7110]  eta: 1:48:43  lr: 0.000010  loss: 0.4665  time: 0.9702  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 850/7110]  eta: 1:47:37  lr: 0.000010  loss: 0.0690  time: 1.0034  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 900/7110]  eta: 1:46:26  lr: 0.000010  loss: 0.0376  time: 0.9779  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [ 950/7110]  eta: 1:45:26  lr: 0.000010  loss: 0.3200  time: 1.0146  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1000/7110]  eta: 1:44:22  lr: 0.000010  loss: 0.4915  time: 0.9854  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1050/7110]  eta: 1:43:18  lr: 0.000010  loss: 0.1453  time: 0.9558  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1100/7110]  eta: 1:42:19  lr: 0.000010  loss: 0.0218  time: 1.0305  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1150/7110]  eta: 1:41:25  lr: 0.000010  loss: 0.0415  time: 1.0173  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1200/7110]  eta: 1:40:23  lr: 0.000010  loss: 0.1005  time: 0.9661  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1250/7110]  eta: 1:39:34  lr: 0.000010  loss: 0.3164  time: 1.0072  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1300/7110]  eta: 1:38:47  lr: 0.000010  loss: 0.0564  time: 1.0063  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1350/7110]  eta: 1:37:54  lr: 0.000010  loss: 0.0579  time: 0.9501  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1400/7110]  eta: 1:36:55  lr: 0.000010  loss: 0.0375  time: 1.0157  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1450/7110]  eta: 1:35:56  lr: 0.000010  loss: 0.1217  time: 0.9704  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1500/7110]  eta: 1:34:58  lr: 0.000010  loss: 0.2603  time: 0.9859  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1550/7110]  eta: 1:34:08  lr: 0.000010  loss: 0.1986  time: 1.0192  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1600/7110]  eta: 1:33:10  lr: 0.000010  loss: 0.3040  time: 0.9516  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1650/7110]  eta: 1:32:14  lr: 0.000010  loss: 0.1551  time: 0.9490  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1700/7110]  eta: 1:31:18  lr: 0.000010  loss: 0.1029  time: 1.0227  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1750/7110]  eta: 1:30:25  lr: 0.000010  loss: 0.0460  time: 1.0037  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1800/7110]  eta: 1:29:34  lr: 0.000010  loss: 1.1036  time: 1.0371  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1850/7110]  eta: 1:28:52  lr: 0.000010  loss: 0.3027  time: 1.1348  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1900/7110]  eta: 1:28:03  lr: 0.000010  loss: 0.3314  time: 1.0525  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [1950/7110]  eta: 1:27:13  lr: 0.000010  loss: 1.4571  time: 0.9537  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2000/7110]  eta: 1:26:21  lr: 0.000010  loss: 0.2989  time: 1.0044  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2050/7110]  eta: 1:25:34  lr: 0.000010  loss: 0.3570  time: 1.0977  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2100/7110]  eta: 1:24:39  lr: 0.000010  loss: 0.2131  time: 0.9386  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2150/7110]  eta: 1:23:46  lr: 0.000010  loss: 0.1133  time: 0.9942  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2200/7110]  eta: 1:22:52  lr: 0.000010  loss: 0.0485  time: 0.9545  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2250/7110]  eta: 1:22:00  lr: 0.000010  loss: 0.3152  time: 0.9678  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2300/7110]  eta: 1:21:08  lr: 0.000010  loss: 0.1990  time: 0.9764  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2350/7110]  eta: 1:20:15  lr: 0.000010  loss: 0.0413  time: 1.0292  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2400/7110]  eta: 1:19:26  lr: 0.000010  loss: 0.1198  time: 0.9919  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2450/7110]  eta: 1:18:35  lr: 0.000010  loss: 0.2589  time: 1.0335  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2500/7110]  eta: 1:17:45  lr: 0.000010  loss: 0.0499  time: 1.0466  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2550/7110]  eta: 1:16:52  lr: 0.000010  loss: 0.2449  time: 0.9790  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2600/7110]  eta: 1:16:01  lr: 0.000010  loss: 0.3275  time: 1.0132  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2650/7110]  eta: 1:15:11  lr: 0.000010  loss: 0.1557  time: 1.0224  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2700/7110]  eta: 1:14:19  lr: 0.000010  loss: 1.5673  time: 1.0127  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2750/7110]  eta: 1:13:26  lr: 0.000010  loss: 0.3304  time: 0.9763  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2800/7110]  eta: 1:12:35  lr: 0.000010  loss: 0.1453  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2850/7110]  eta: 1:11:41  lr: 0.000010  loss: 0.2937  time: 0.9864  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2900/7110]  eta: 1:10:56  lr: 0.000010  loss: 0.2473  time: 1.0901  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [2950/7110]  eta: 1:10:03  lr: 0.000010  loss: 0.2339  time: 0.9798  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3000/7110]  eta: 1:09:13  lr: 0.000010  loss: 0.1480  time: 1.0121  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3050/7110]  eta: 1:08:22  lr: 0.000010  loss: 0.0940  time: 0.9920  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3100/7110]  eta: 1:07:31  lr: 0.000010  loss: 0.1974  time: 0.9588  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3150/7110]  eta: 1:06:38  lr: 0.000010  loss: 0.0422  time: 0.9728  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3200/7110]  eta: 1:05:45  lr: 0.000010  loss: 0.5104  time: 0.9868  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3250/7110]  eta: 1:04:53  lr: 0.000010  loss: 0.0550  time: 0.9598  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3300/7110]  eta: 1:04:04  lr: 0.000010  loss: 0.0455  time: 1.0814  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3350/7110]  eta: 1:03:14  lr: 0.000010  loss: 0.6247  time: 0.9957  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3400/7110]  eta: 1:02:23  lr: 0.000010  loss: 0.1505  time: 0.9635  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3450/7110]  eta: 1:01:34  lr: 0.000010  loss: 1.7553  time: 1.0348  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3500/7110]  eta: 1:00:43  lr: 0.000010  loss: 0.2173  time: 1.0006  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3550/7110]  eta: 0:59:53  lr: 0.000010  loss: 0.2490  time: 1.0280  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3600/7110]  eta: 0:59:00  lr: 0.000010  loss: 1.4488  time: 0.9502  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3650/7110]  eta: 0:58:09  lr: 0.000010  loss: 0.3178  time: 0.9684  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3700/7110]  eta: 0:57:20  lr: 0.000010  loss: 0.1373  time: 1.0677  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3750/7110]  eta: 0:56:30  lr: 0.000010  loss: 0.3869  time: 0.9641  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3800/7110]  eta: 0:55:40  lr: 0.000010  loss: 1.2357  time: 1.0445  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3850/7110]  eta: 0:54:48  lr: 0.000010  loss: 0.1543  time: 0.9868  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3900/7110]  eta: 0:53:59  lr: 0.000010  loss: 0.0630  time: 0.9851  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [3950/7110]  eta: 0:53:09  lr: 0.000010  loss: 0.3109  time: 1.0317  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4000/7110]  eta: 0:52:18  lr: 0.000010  loss: 0.5158  time: 0.9947  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4050/7110]  eta: 0:51:27  lr: 0.000010  loss: 0.0855  time: 0.9737  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4100/7110]  eta: 0:50:36  lr: 0.000010  loss: 0.2405  time: 1.0268  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4150/7110]  eta: 0:49:45  lr: 0.000010  loss: 0.6305  time: 0.9511  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4200/7110]  eta: 0:48:54  lr: 0.000010  loss: 0.2338  time: 0.9673  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4250/7110]  eta: 0:48:04  lr: 0.000010  loss: 1.7585  time: 1.0502  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4300/7110]  eta: 0:47:13  lr: 0.000010  loss: 0.1126  time: 1.0159  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4350/7110]  eta: 0:46:22  lr: 0.000010  loss: 0.2888  time: 0.9900  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4400/7110]  eta: 0:45:30  lr: 0.000010  loss: 0.0284  time: 0.9761  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4450/7110]  eta: 0:44:39  lr: 0.000010  loss: 0.1404  time: 1.0390  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4500/7110]  eta: 0:43:49  lr: 0.000010  loss: 0.2260  time: 0.9533  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4550/7110]  eta: 0:42:59  lr: 0.000010  loss: 0.3842  time: 1.0491  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4600/7110]  eta: 0:42:08  lr: 0.000010  loss: 0.1680  time: 1.0279  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4650/7110]  eta: 0:41:18  lr: 0.000010  loss: 0.0780  time: 1.0613  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4700/7110]  eta: 0:40:27  lr: 0.000010  loss: 0.2541  time: 0.9886  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4750/7110]  eta: 0:39:36  lr: 0.000010  loss: 0.1832  time: 0.9890  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4800/7110]  eta: 0:38:46  lr: 0.000010  loss: 0.0827  time: 1.0603  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4850/7110]  eta: 0:37:56  lr: 0.000010  loss: 0.4092  time: 1.0101  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4900/7110]  eta: 0:37:05  lr: 0.000010  loss: 0.1472  time: 0.9972  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [4950/7110]  eta: 0:36:14  lr: 0.000010  loss: 0.2278  time: 1.0012  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5000/7110]  eta: 0:35:24  lr: 0.000010  loss: 0.2548  time: 1.0402  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5050/7110]  eta: 0:34:34  lr: 0.000010  loss: 0.1609  time: 0.9697  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5100/7110]  eta: 0:33:44  lr: 0.000010  loss: 0.1126  time: 1.0550  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5150/7110]  eta: 0:32:54  lr: 0.000010  loss: 0.2038  time: 1.0269  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5200/7110]  eta: 0:32:03  lr: 0.000010  loss: 0.1335  time: 0.9621  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5250/7110]  eta: 0:31:12  lr: 0.000010  loss: 0.1692  time: 0.9558  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5300/7110]  eta: 0:30:22  lr: 0.000010  loss: 0.0279  time: 1.0028  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5350/7110]  eta: 0:29:31  lr: 0.000010  loss: 0.5158  time: 0.9734  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5400/7110]  eta: 0:28:40  lr: 0.000010  loss: 0.0876  time: 0.9439  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5450/7110]  eta: 0:27:49  lr: 0.000010  loss: 0.1967  time: 1.0041  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5500/7110]  eta: 0:26:59  lr: 0.000010  loss: 0.3581  time: 0.9486  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5550/7110]  eta: 0:26:08  lr: 0.000010  loss: 0.1172  time: 0.9986  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5600/7110]  eta: 0:25:18  lr: 0.000010  loss: 0.1233  time: 0.9454  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5650/7110]  eta: 0:24:27  lr: 0.000010  loss: 0.3840  time: 0.9309  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5700/7110]  eta: 0:23:36  lr: 0.000010  loss: 0.0698  time: 0.9342  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5750/7110]  eta: 0:22:46  lr: 0.000010  loss: 0.2469  time: 0.9687  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5800/7110]  eta: 0:21:55  lr: 0.000010  loss: 0.5387  time: 1.0281  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5850/7110]  eta: 0:21:05  lr: 0.000010  loss: 0.0932  time: 1.0118  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5900/7110]  eta: 0:20:15  lr: 0.000010  loss: 0.3008  time: 1.0058  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [5950/7110]  eta: 0:19:25  lr: 0.000010  loss: 0.4631  time: 0.9905  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6000/7110]  eta: 0:18:34  lr: 0.000010  loss: 0.3079  time: 1.0029  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6050/7110]  eta: 0:17:44  lr: 0.000010  loss: 0.3712  time: 0.9631  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6100/7110]  eta: 0:16:54  lr: 0.000010  loss: 0.2137  time: 0.9622  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6150/7110]  eta: 0:16:03  lr: 0.000010  loss: 0.0309  time: 0.9423  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.0215  time: 1.0093  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.2415  time: 0.9575  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6300/7110]  eta: 0:13:32  lr: 0.000010  loss: 0.1683  time: 0.9882  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.0442  time: 0.9723  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.2077  time: 0.9757  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.0380  time: 1.0241  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.1416  time: 0.9626  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.2604  time: 0.9685  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.3571  time: 0.9980  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.3064  time: 1.0672  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.2634  time: 0.9994  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 1.6500  time: 1.0364  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.3321  time: 1.0285  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.0925  time: 0.9833  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.0434  time: 0.9242  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.7102  time: 0.9800  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.3538  time: 0.9949  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1669  time: 1.0426  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.2496  time: 0.9667  data: 0.0000  max mem: 66110
Train: data epoch: [51]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.0875  time: 1.0319  data: 0.0000  max mem: 66110
Train: data epoch: [51] Total time: 1:58:53 (1.0034 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:18:38    time: 20.7854  data: 19.5454  max mem: 66110
Evaluation  [  10/1093]  eta: 0:58:45    time: 3.2556  data: 1.7777  max mem: 66110
Evaluation  [  20/1093]  eta: 0:42:47    time: 1.4733  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:35:36    time: 1.3250  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:48    time: 1.3198  data: 0.0011  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:55    time: 1.4219  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:44    time: 1.2592  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:57    time: 1.2854  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:27:14    time: 1.4418  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:50    time: 1.4827  data: 0.0012  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:34    time: 1.5750  data: 0.0012  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:26:04    time: 1.5282  data: 0.0011  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:24    time: 1.3701  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:25:06    time: 1.4121  data: 0.0011  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:32    time: 1.4148  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:24:01    time: 1.2977  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:39    time: 1.3555  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:23:09    time: 1.3255  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:48    time: 1.3193  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:38    time: 1.4985  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:15    time: 1.4613  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:22:10    time: 1.5274  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:49    time: 1.5478  data: 0.0011  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:25    time: 1.3126  data: 0.0012  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:21:02    time: 1.2509  data: 0.0012  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:46    time: 1.3442  data: 0.0011  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:38    time: 1.5717  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:18    time: 1.5032  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:58    time: 1.3074  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:48    time: 1.4852  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:25    time: 1.4142  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:19:04    time: 1.1813  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:48    time: 1.3090  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:30    time: 1.3586  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:16    time: 1.4002  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:18:01    time: 1.4653  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:42    time: 1.3257  data: 0.0011  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:25    time: 1.2878  data: 0.0013  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:13    time: 1.4644  data: 0.0012  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:57    time: 1.4674  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:43    time: 1.4252  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:28    time: 1.4271  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:13    time: 1.4042  data: 0.0010  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:57    time: 1.4015  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:38    time: 1.2412  data: 0.0011  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:23    time: 1.2578  data: 0.0011  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:09    time: 1.4140  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:52    time: 1.3548  data: 0.0011  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:38    time: 1.3562  data: 0.0011  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:24    time: 1.4695  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:12    time: 1.5705  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:14:00    time: 1.6178  data: 0.0011  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:46    time: 1.5506  data: 0.0011  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:32    time: 1.4811  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:18    time: 1.4709  data: 0.0012  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:03    time: 1.4399  data: 0.0012  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:45    time: 1.2548  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:31    time: 1.2825  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:16    time: 1.4229  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:12:00    time: 1.2821  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:47    time: 1.3703  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:33    time: 1.5181  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:18    time: 1.4073  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:03    time: 1.3721  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:48    time: 1.3558  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:34    time: 1.4376  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:19    time: 1.4125  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:05    time: 1.4211  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:51    time: 1.5116  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:38    time: 1.5142  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:23    time: 1.4487  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:07    time: 1.2194  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:52    time: 1.2591  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:36    time: 1.2288  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:21    time: 1.1448  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:07    time: 1.3454  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:53    time: 1.4176  data: 0.0009  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:38    time: 1.3701  data: 0.0009  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:25    time: 1.4690  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:09    time: 1.3551  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:56    time: 1.3678  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:41    time: 1.4906  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:27    time: 1.3997  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:13    time: 1.4945  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:59    time: 1.4587  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:45    time: 1.4089  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:30    time: 1.3990  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:16    time: 1.3809  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:03    time: 1.5110  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:48    time: 1.3664  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:33    time: 1.2747  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:19    time: 1.4221  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:05    time: 1.4369  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:51    time: 1.3663  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:37    time: 1.4045  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:22    time: 1.4133  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:08    time: 1.3832  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:54    time: 1.3895  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:40    time: 1.3015  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3711  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4935  data: 0.0011  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.4665  data: 0.0011  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:43    time: 1.2822  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1408  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2546  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3554  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.4257  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4791  data: 0.0011  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3941  data: 0.0011  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3580  data: 0.0011  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3307  data: 0.0438  max mem: 66110
Evaluation Total time: 0:25:42 (1.4117 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_51_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [52]  [   0/7110]  eta: 2 days, 5:12:54  lr: 0.000010  loss: 0.3454  time: 26.9444  data: 0.0001  max mem: 66110
Train: data epoch: [52]  [  50/7110]  eta: 2:56:18  lr: 0.000010  loss: 0.3547  time: 0.9966  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 100/7110]  eta: 2:23:56  lr: 0.000010  loss: 0.7885  time: 0.9466  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 150/7110]  eta: 2:13:54  lr: 0.000010  loss: 0.0725  time: 1.0205  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 200/7110]  eta: 2:08:20  lr: 0.000010  loss: 0.1288  time: 0.9985  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 250/7110]  eta: 2:04:19  lr: 0.000010  loss: 0.0338  time: 0.9642  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 300/7110]  eta: 2:01:39  lr: 0.000010  loss: 0.0675  time: 0.9656  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 350/7110]  eta: 1:59:58  lr: 0.000010  loss: 0.1432  time: 0.9852  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 400/7110]  eta: 1:58:25  lr: 0.000010  loss: 0.0509  time: 1.0236  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 450/7110]  eta: 1:56:56  lr: 0.000010  loss: 0.3667  time: 1.0289  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 500/7110]  eta: 1:55:30  lr: 0.000010  loss: 0.1509  time: 0.9958  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 550/7110]  eta: 1:53:53  lr: 0.000010  loss: 0.8413  time: 0.9645  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 600/7110]  eta: 1:52:34  lr: 0.000010  loss: 0.2475  time: 0.9758  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 650/7110]  eta: 1:51:27  lr: 0.000010  loss: 0.0436  time: 1.0215  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 700/7110]  eta: 1:50:44  lr: 0.000010  loss: 1.3086  time: 1.0777  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 750/7110]  eta: 1:49:30  lr: 0.000010  loss: 0.0698  time: 0.9430  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 800/7110]  eta: 1:48:25  lr: 0.000010  loss: 0.7741  time: 1.0263  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 850/7110]  eta: 1:47:37  lr: 0.000010  loss: 0.2202  time: 1.0420  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 900/7110]  eta: 1:46:33  lr: 0.000010  loss: 0.2042  time: 0.9881  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [ 950/7110]  eta: 1:45:35  lr: 0.000010  loss: 0.2513  time: 1.0362  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1000/7110]  eta: 1:44:36  lr: 0.000010  loss: 0.0306  time: 1.0432  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1050/7110]  eta: 1:43:41  lr: 0.000010  loss: 0.3691  time: 0.9780  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1100/7110]  eta: 1:42:41  lr: 0.000010  loss: 0.2096  time: 0.9933  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1150/7110]  eta: 1:41:44  lr: 0.000010  loss: 0.0237  time: 1.0085  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1200/7110]  eta: 1:40:48  lr: 0.000010  loss: 0.0517  time: 1.0674  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1250/7110]  eta: 1:39:49  lr: 0.000010  loss: 0.0669  time: 0.9619  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1300/7110]  eta: 1:38:48  lr: 0.000010  loss: 0.0614  time: 0.9636  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1350/7110]  eta: 1:37:48  lr: 0.000010  loss: 0.8292  time: 1.0398  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1400/7110]  eta: 1:36:50  lr: 0.000010  loss: 0.1325  time: 0.9739  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1450/7110]  eta: 1:35:47  lr: 0.000010  loss: 0.0962  time: 0.9754  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1500/7110]  eta: 1:34:49  lr: 0.000010  loss: 0.1015  time: 0.9722  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1550/7110]  eta: 1:33:54  lr: 0.000010  loss: 0.4930  time: 0.9908  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1600/7110]  eta: 1:33:01  lr: 0.000010  loss: 0.2096  time: 0.9980  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1650/7110]  eta: 1:32:09  lr: 0.000010  loss: 0.2096  time: 1.0027  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1700/7110]  eta: 1:31:20  lr: 0.000010  loss: 0.1935  time: 1.0207  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1750/7110]  eta: 1:30:25  lr: 0.000010  loss: 0.5188  time: 0.9881  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1800/7110]  eta: 1:29:31  lr: 0.000010  loss: 0.4190  time: 0.9882  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1850/7110]  eta: 1:28:38  lr: 0.000010  loss: 0.0251  time: 0.9872  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1900/7110]  eta: 1:27:54  lr: 0.000010  loss: 0.2460  time: 1.0599  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [1950/7110]  eta: 1:26:59  lr: 0.000010  loss: 0.2433  time: 1.0157  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2000/7110]  eta: 1:26:03  lr: 0.000010  loss: 0.0908  time: 0.9633  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2050/7110]  eta: 1:25:12  lr: 0.000010  loss: 0.8996  time: 1.0383  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2100/7110]  eta: 1:24:20  lr: 0.000010  loss: 0.1127  time: 0.9900  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2150/7110]  eta: 1:23:27  lr: 0.000010  loss: 0.3421  time: 0.9747  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2200/7110]  eta: 1:22:34  lr: 0.000010  loss: 0.6526  time: 1.0580  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2250/7110]  eta: 1:21:41  lr: 0.000010  loss: 0.0559  time: 1.0123  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2300/7110]  eta: 1:20:48  lr: 0.000010  loss: 0.1145  time: 0.9941  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2350/7110]  eta: 1:19:58  lr: 0.000010  loss: 0.2625  time: 1.0602  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2400/7110]  eta: 1:19:06  lr: 0.000010  loss: 0.4001  time: 1.0159  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2450/7110]  eta: 1:18:11  lr: 0.000010  loss: 0.2726  time: 0.9402  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2500/7110]  eta: 1:17:21  lr: 0.000010  loss: 0.1991  time: 1.0097  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2550/7110]  eta: 1:16:28  lr: 0.000010  loss: 0.5508  time: 0.9895  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2600/7110]  eta: 1:15:37  lr: 0.000010  loss: 0.3175  time: 0.9673  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2650/7110]  eta: 1:14:47  lr: 0.000010  loss: 0.1057  time: 1.0218  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2700/7110]  eta: 1:13:55  lr: 0.000010  loss: 0.1523  time: 0.9813  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2750/7110]  eta: 1:13:06  lr: 0.000010  loss: 0.3778  time: 0.9943  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2800/7110]  eta: 1:12:12  lr: 0.000010  loss: 0.1129  time: 0.9281  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2850/7110]  eta: 1:11:21  lr: 0.000010  loss: 0.2585  time: 1.0132  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2900/7110]  eta: 1:10:29  lr: 0.000010  loss: 0.1741  time: 0.9794  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [2950/7110]  eta: 1:09:42  lr: 0.000010  loss: 0.2795  time: 1.0728  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3000/7110]  eta: 1:08:52  lr: 0.000010  loss: 0.5631  time: 0.9589  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3050/7110]  eta: 1:08:02  lr: 0.000010  loss: 0.0414  time: 1.0333  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3100/7110]  eta: 1:07:14  lr: 0.000010  loss: 0.1393  time: 1.0721  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3150/7110]  eta: 1:06:22  lr: 0.000010  loss: 0.3172  time: 0.9324  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3200/7110]  eta: 1:05:32  lr: 0.000010  loss: 0.2640  time: 1.0047  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3250/7110]  eta: 1:04:42  lr: 0.000010  loss: 0.4705  time: 1.0518  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3300/7110]  eta: 1:03:50  lr: 0.000010  loss: 0.4669  time: 0.9742  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3350/7110]  eta: 1:03:01  lr: 0.000010  loss: 0.4973  time: 1.0572  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3400/7110]  eta: 1:02:09  lr: 0.000010  loss: 0.1347  time: 0.9740  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3450/7110]  eta: 1:01:20  lr: 0.000010  loss: 0.1526  time: 1.0732  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3500/7110]  eta: 1:00:27  lr: 0.000010  loss: 0.0575  time: 0.9562  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3550/7110]  eta: 0:59:36  lr: 0.000010  loss: 0.2645  time: 0.9759  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3600/7110]  eta: 0:58:47  lr: 0.000010  loss: 0.0650  time: 1.0621  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3650/7110]  eta: 0:57:56  lr: 0.000010  loss: 0.3103  time: 1.0431  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3700/7110]  eta: 0:57:08  lr: 0.000010  loss: 0.0204  time: 1.0368  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3750/7110]  eta: 0:56:16  lr: 0.000010  loss: 0.1567  time: 0.9135  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3800/7110]  eta: 0:55:24  lr: 0.000010  loss: 0.6568  time: 1.0315  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3850/7110]  eta: 0:54:35  lr: 0.000010  loss: 0.7258  time: 1.0018  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3900/7110]  eta: 0:53:44  lr: 0.000010  loss: 0.1467  time: 1.0284  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [3950/7110]  eta: 0:52:54  lr: 0.000010  loss: 0.1997  time: 0.9854  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4000/7110]  eta: 0:52:03  lr: 0.000010  loss: 0.0998  time: 0.9612  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4050/7110]  eta: 0:51:12  lr: 0.000010  loss: 0.2074  time: 1.0140  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4100/7110]  eta: 0:50:21  lr: 0.000010  loss: 1.8037  time: 1.0103  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4150/7110]  eta: 0:49:30  lr: 0.000010  loss: 0.1776  time: 0.9891  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4200/7110]  eta: 0:48:40  lr: 0.000010  loss: 0.0707  time: 0.9943  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4250/7110]  eta: 0:47:50  lr: 0.000010  loss: 0.2982  time: 0.9688  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4300/7110]  eta: 0:46:59  lr: 0.000010  loss: 0.0809  time: 0.9945  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4350/7110]  eta: 0:46:08  lr: 0.000010  loss: 0.1894  time: 0.9952  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4400/7110]  eta: 0:45:18  lr: 0.000010  loss: 0.0071  time: 1.0311  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4450/7110]  eta: 0:44:28  lr: 0.000010  loss: 0.0435  time: 0.9979  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4500/7110]  eta: 0:43:37  lr: 0.000010  loss: 0.2722  time: 1.0163  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4550/7110]  eta: 0:42:48  lr: 0.000010  loss: 0.0763  time: 0.9980  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4600/7110]  eta: 0:41:57  lr: 0.000010  loss: 0.1017  time: 1.0098  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4650/7110]  eta: 0:41:07  lr: 0.000010  loss: 0.3037  time: 0.9942  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4700/7110]  eta: 0:40:16  lr: 0.000010  loss: 0.1080  time: 0.9506  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4750/7110]  eta: 0:39:25  lr: 0.000010  loss: 0.0948  time: 0.9467  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4800/7110]  eta: 0:38:34  lr: 0.000010  loss: 0.1272  time: 0.9920  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4850/7110]  eta: 0:37:44  lr: 0.000010  loss: 0.2744  time: 1.0010  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4900/7110]  eta: 0:36:54  lr: 0.000010  loss: 0.0691  time: 1.0109  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [4950/7110]  eta: 0:36:03  lr: 0.000010  loss: 0.2898  time: 1.0161  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5000/7110]  eta: 0:35:12  lr: 0.000010  loss: 0.3469  time: 0.9707  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5050/7110]  eta: 0:34:23  lr: 0.000010  loss: 0.2567  time: 1.0949  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5100/7110]  eta: 0:33:32  lr: 0.000010  loss: 0.2163  time: 0.9880  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5150/7110]  eta: 0:32:43  lr: 0.000010  loss: 0.1457  time: 1.0104  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5200/7110]  eta: 0:31:54  lr: 0.000010  loss: 0.1145  time: 1.0612  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5250/7110]  eta: 0:31:03  lr: 0.000010  loss: 0.0925  time: 1.0095  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5300/7110]  eta: 0:30:13  lr: 0.000010  loss: 0.2197  time: 1.0449  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5350/7110]  eta: 0:29:23  lr: 0.000010  loss: 0.4543  time: 1.0023  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5400/7110]  eta: 0:28:33  lr: 0.000010  loss: 0.2043  time: 1.0259  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5450/7110]  eta: 0:27:42  lr: 0.000010  loss: 0.2990  time: 0.9926  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5500/7110]  eta: 0:26:52  lr: 0.000010  loss: 0.1748  time: 1.0735  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5550/7110]  eta: 0:26:02  lr: 0.000010  loss: 0.5554  time: 1.0488  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5600/7110]  eta: 0:25:12  lr: 0.000010  loss: 0.1602  time: 0.9946  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5650/7110]  eta: 0:24:21  lr: 0.000010  loss: 0.0245  time: 0.9938  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5700/7110]  eta: 0:23:32  lr: 0.000010  loss: 0.0921  time: 1.0205  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5750/7110]  eta: 0:22:41  lr: 0.000010  loss: 0.0073  time: 0.9982  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5800/7110]  eta: 0:21:52  lr: 0.000010  loss: 0.5376  time: 1.0166  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5850/7110]  eta: 0:21:02  lr: 0.000010  loss: 0.3487  time: 0.9803  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5900/7110]  eta: 0:20:12  lr: 0.000010  loss: 0.3190  time: 0.9919  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [5950/7110]  eta: 0:19:22  lr: 0.000010  loss: 0.0799  time: 0.9869  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6000/7110]  eta: 0:18:31  lr: 0.000010  loss: 0.2884  time: 1.0041  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6050/7110]  eta: 0:17:42  lr: 0.000010  loss: 0.3025  time: 1.0313  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6100/7110]  eta: 0:16:51  lr: 0.000010  loss: 0.0494  time: 0.9867  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6150/7110]  eta: 0:16:01  lr: 0.000010  loss: 0.2135  time: 0.9901  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6200/7110]  eta: 0:15:11  lr: 0.000010  loss: 0.0212  time: 1.0330  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6250/7110]  eta: 0:14:21  lr: 0.000010  loss: 0.4142  time: 0.9309  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6300/7110]  eta: 0:13:31  lr: 0.000010  loss: 0.1349  time: 0.9859  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.1820  time: 0.9933  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.1837  time: 0.9900  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6450/7110]  eta: 0:11:01  lr: 0.000010  loss: 0.6140  time: 0.9841  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6500/7110]  eta: 0:10:11  lr: 0.000010  loss: 0.4418  time: 1.0130  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.0486  time: 0.9801  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.1352  time: 0.9636  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 1.5789  time: 1.0454  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.2810  time: 1.0422  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.0545  time: 1.0485  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.1450  time: 0.9662  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.0578  time: 1.0430  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.2574  time: 0.9690  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.0293  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.2964  time: 1.0407  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.2302  time: 1.0197  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.2808  time: 1.0724  data: 0.0000  max mem: 66110
Train: data epoch: [52]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.1736  time: 1.0936  data: 0.0000  max mem: 66110
Train: data epoch: [52] Total time: 1:58:48 (1.0026 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:22:34    time: 21.0015  data: 19.7464  max mem: 66110
Evaluation  [  10/1093]  eta: 0:59:24    time: 3.2910  data: 1.7961  max mem: 66110
Evaluation  [  20/1093]  eta: 0:43:03    time: 1.4777  data: 0.0011  max mem: 66110
Evaluation  [  30/1093]  eta: 0:35:33    time: 1.3016  data: 0.0011  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:44    time: 1.2966  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:52    time: 1.4184  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:50    time: 1.2861  data: 0.0011  max mem: 66110
Evaluation  [  70/1093]  eta: 0:28:00    time: 1.3023  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:27:16    time: 1.4331  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:56    time: 1.5017  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:42    time: 1.6055  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:26:11    time: 1.5368  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:25    time: 1.3389  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:25:07    time: 1.3801  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:26    time: 1.3619  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:55    time: 1.2417  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:36    time: 1.3699  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:23:04    time: 1.3313  data: 0.0009  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:44    time: 1.3026  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:30    time: 1.4572  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:09    time: 1.4365  data: 0.0012  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:59    time: 1.4890  data: 0.0013  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:35    time: 1.4431  data: 0.0012  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:15    time: 1.2965  data: 0.0011  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:53    time: 1.2991  data: 0.0011  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:38    time: 1.3744  data: 0.0011  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:30    time: 1.5684  data: 0.0011  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:08    time: 1.4532  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:49    time: 1.2912  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:40    time: 1.5027  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:18    time: 1.4236  data: 0.0010  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:57    time: 1.1877  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:41    time: 1.3038  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:23    time: 1.3492  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:10    time: 1.3998  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:55    time: 1.4625  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:36    time: 1.3196  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:20    time: 1.2989  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:07    time: 1.4581  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:53    time: 1.4922  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:40    time: 1.4730  data: 0.0011  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:24    time: 1.4481  data: 0.0013  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:10    time: 1.4048  data: 0.0012  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:55    time: 1.4272  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:36    time: 1.2825  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:21    time: 1.2456  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:06    time: 1.4002  data: 0.0009  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:50    time: 1.3626  data: 0.0009  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:36    time: 1.3648  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:23    time: 1.4831  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:12    time: 1.6007  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:59    time: 1.6160  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:45    time: 1.5175  data: 0.0011  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:30    time: 1.4662  data: 0.0011  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:16    time: 1.4641  data: 0.0011  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:01    time: 1.4386  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:43    time: 1.2205  data: 0.0011  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:28    time: 1.1903  data: 0.0011  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:13    time: 1.3505  data: 0.0013  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:56    time: 1.2316  data: 0.0011  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:43    time: 1.3337  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:29    time: 1.5141  data: 0.0011  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:14    time: 1.4204  data: 0.0011  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:00    time: 1.4003  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:45    time: 1.3568  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:31    time: 1.3872  data: 0.0011  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:16    time: 1.4001  data: 0.0011  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:03    time: 1.4538  data: 0.0011  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:48    time: 1.4831  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:35    time: 1.4840  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:20    time: 1.4683  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:04    time: 1.2487  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:50    time: 1.2462  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:34    time: 1.2127  data: 0.0010  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:19    time: 1.1825  data: 0.0010  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:05    time: 1.3836  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:51    time: 1.4459  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:37    time: 1.3804  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:23    time: 1.4657  data: 0.0009  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:08    time: 1.3207  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:54    time: 1.3565  data: 0.0011  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:40    time: 1.5452  data: 0.0011  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:26    time: 1.4127  data: 0.0011  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:12    time: 1.4756  data: 0.0011  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:58    time: 1.4588  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:44    time: 1.4184  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:29    time: 1.3688  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:15    time: 1.3372  data: 0.0011  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:01    time: 1.5086  data: 0.0011  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:47    time: 1.3711  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:33    time: 1.2804  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:18    time: 1.4247  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:04    time: 1.4530  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:50    time: 1.3938  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:36    time: 1.4130  data: 0.0011  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:22    time: 1.4906  data: 0.0011  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:08    time: 1.4436  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:54    time: 1.4040  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3547  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3890  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4903  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.5013  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:43    time: 1.3396  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1605  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2533  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3406  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3808  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4431  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3891  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3470  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3195  data: 0.0440  max mem: 66110
Evaluation Total time: 0:25:41 (1.4103 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_52_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [53]  [   0/7110]  eta: 2 days, 5:17:37  lr: 0.000010  loss: 0.2271  time: 26.9842  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [  50/7110]  eta: 2:54:17  lr: 0.000010  loss: 0.4425  time: 0.9879  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 100/7110]  eta: 2:23:02  lr: 0.000010  loss: 0.0249  time: 0.9956  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 150/7110]  eta: 2:13:21  lr: 0.000010  loss: 0.3882  time: 1.0168  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 200/7110]  eta: 2:07:05  lr: 0.000010  loss: 0.2233  time: 0.9995  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 250/7110]  eta: 2:03:13  lr: 0.000010  loss: 0.1817  time: 0.9540  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 300/7110]  eta: 2:00:38  lr: 0.000010  loss: 0.0555  time: 0.9818  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 350/7110]  eta: 1:58:43  lr: 0.000010  loss: 0.1362  time: 1.0125  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 400/7110]  eta: 1:56:52  lr: 0.000010  loss: 0.3634  time: 0.9548  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 450/7110]  eta: 1:55:42  lr: 0.000010  loss: 0.0754  time: 1.0165  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 500/7110]  eta: 1:54:01  lr: 0.000010  loss: 0.1952  time: 0.9511  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 550/7110]  eta: 1:52:33  lr: 0.000010  loss: 0.2447  time: 0.9506  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 600/7110]  eta: 1:51:23  lr: 0.000010  loss: 0.4310  time: 1.0214  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 650/7110]  eta: 1:50:12  lr: 0.000010  loss: 0.0541  time: 0.9854  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 700/7110]  eta: 1:49:12  lr: 0.000010  loss: 0.2880  time: 1.0097  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 750/7110]  eta: 1:48:09  lr: 0.000010  loss: 0.0266  time: 0.9947  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 800/7110]  eta: 1:47:08  lr: 0.000010  loss: 0.1953  time: 1.0236  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 850/7110]  eta: 1:46:05  lr: 0.000010  loss: 0.4401  time: 1.0203  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 900/7110]  eta: 1:45:17  lr: 0.000010  loss: 0.0331  time: 1.0138  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [ 950/7110]  eta: 1:44:31  lr: 0.000010  loss: 0.0600  time: 1.0387  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1000/7110]  eta: 1:43:26  lr: 0.000010  loss: 0.1008  time: 0.9780  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1050/7110]  eta: 1:42:20  lr: 0.000010  loss: 0.6261  time: 0.9319  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1100/7110]  eta: 1:41:25  lr: 0.000010  loss: 0.1562  time: 1.0132  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1150/7110]  eta: 1:40:30  lr: 0.000010  loss: 0.0606  time: 1.0018  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1200/7110]  eta: 1:39:42  lr: 0.000010  loss: 0.7205  time: 1.0507  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1250/7110]  eta: 1:38:49  lr: 0.000010  loss: 0.7385  time: 0.9921  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1300/7110]  eta: 1:37:57  lr: 0.000010  loss: 0.1744  time: 0.9971  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1350/7110]  eta: 1:37:12  lr: 0.000010  loss: 0.2832  time: 1.0089  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1400/7110]  eta: 1:36:23  lr: 0.000010  loss: 0.5983  time: 1.0134  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1450/7110]  eta: 1:35:38  lr: 0.000010  loss: 0.5732  time: 1.0212  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1500/7110]  eta: 1:34:46  lr: 0.000010  loss: 0.2616  time: 1.0216  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1550/7110]  eta: 1:34:00  lr: 0.000010  loss: 0.3907  time: 1.0517  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1600/7110]  eta: 1:33:09  lr: 0.000010  loss: 0.1803  time: 1.0062  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1650/7110]  eta: 1:32:13  lr: 0.000010  loss: 0.0074  time: 0.9550  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1700/7110]  eta: 1:31:23  lr: 0.000010  loss: 0.0475  time: 1.0458  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1750/7110]  eta: 1:30:25  lr: 0.000010  loss: 0.0874  time: 0.9257  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1800/7110]  eta: 1:29:32  lr: 0.000010  loss: 0.1987  time: 0.9838  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1850/7110]  eta: 1:28:43  lr: 0.000010  loss: 0.0714  time: 1.0185  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1900/7110]  eta: 1:27:54  lr: 0.000010  loss: 0.2363  time: 1.0472  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [1950/7110]  eta: 1:27:03  lr: 0.000010  loss: 0.3382  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2000/7110]  eta: 1:26:12  lr: 0.000010  loss: 0.0769  time: 1.0114  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2050/7110]  eta: 1:25:16  lr: 0.000010  loss: 0.0287  time: 0.9365  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2100/7110]  eta: 1:24:23  lr: 0.000010  loss: 0.4386  time: 0.9703  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2150/7110]  eta: 1:23:34  lr: 0.000010  loss: 0.3102  time: 1.0748  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2200/7110]  eta: 1:22:43  lr: 0.000010  loss: 0.0525  time: 0.9619  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2250/7110]  eta: 1:21:51  lr: 0.000010  loss: 0.1340  time: 0.9763  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2300/7110]  eta: 1:21:01  lr: 0.000010  loss: 0.4581  time: 1.0049  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2350/7110]  eta: 1:20:10  lr: 0.000010  loss: 0.1469  time: 0.9897  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2400/7110]  eta: 1:19:20  lr: 0.000010  loss: 0.3697  time: 0.9599  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2450/7110]  eta: 1:18:28  lr: 0.000010  loss: 0.0253  time: 0.9908  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2500/7110]  eta: 1:17:34  lr: 0.000010  loss: 0.6223  time: 0.9783  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2550/7110]  eta: 1:16:41  lr: 0.000010  loss: 0.4842  time: 0.9998  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2600/7110]  eta: 1:15:53  lr: 0.000010  loss: 0.1703  time: 1.0214  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2650/7110]  eta: 1:15:05  lr: 0.000010  loss: 0.3775  time: 1.0337  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2700/7110]  eta: 1:14:13  lr: 0.000010  loss: 0.0729  time: 0.9402  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2750/7110]  eta: 1:13:23  lr: 0.000010  loss: 0.0900  time: 1.0208  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2800/7110]  eta: 1:12:30  lr: 0.000010  loss: 0.2186  time: 0.9956  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2850/7110]  eta: 1:11:36  lr: 0.000010  loss: 0.1862  time: 0.9698  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2900/7110]  eta: 1:10:45  lr: 0.000010  loss: 0.1117  time: 1.0312  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [2950/7110]  eta: 1:09:53  lr: 0.000010  loss: 0.3352  time: 0.9623  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3000/7110]  eta: 1:09:03  lr: 0.000010  loss: 0.4675  time: 1.0287  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3050/7110]  eta: 1:08:13  lr: 0.000010  loss: 0.7750  time: 1.0259  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3100/7110]  eta: 1:07:23  lr: 0.000010  loss: 0.2070  time: 1.0310  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3150/7110]  eta: 1:06:37  lr: 0.000010  loss: 0.1321  time: 1.0910  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3200/7110]  eta: 1:05:42  lr: 0.000010  loss: 0.3297  time: 0.9009  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3250/7110]  eta: 1:04:51  lr: 0.000010  loss: 0.1552  time: 0.9902  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3300/7110]  eta: 1:03:58  lr: 0.000010  loss: 0.0326  time: 0.9448  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3350/7110]  eta: 1:03:08  lr: 0.000010  loss: 0.2243  time: 1.0387  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3400/7110]  eta: 1:02:16  lr: 0.000010  loss: 0.1766  time: 1.0341  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3450/7110]  eta: 1:01:26  lr: 0.000010  loss: 0.1301  time: 0.9893  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3500/7110]  eta: 1:00:34  lr: 0.000010  loss: 0.5190  time: 0.9860  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3550/7110]  eta: 0:59:43  lr: 0.000010  loss: 0.1173  time: 1.0182  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3600/7110]  eta: 0:58:51  lr: 0.000010  loss: 0.4773  time: 0.9729  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3650/7110]  eta: 0:58:01  lr: 0.000010  loss: 0.2845  time: 0.9916  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3700/7110]  eta: 0:57:09  lr: 0.000010  loss: 0.1842  time: 1.0216  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3750/7110]  eta: 0:56:19  lr: 0.000010  loss: 0.2684  time: 1.0045  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3800/7110]  eta: 0:55:28  lr: 0.000010  loss: 0.0447  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3850/7110]  eta: 0:54:39  lr: 0.000010  loss: 0.2309  time: 1.0437  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3900/7110]  eta: 0:53:49  lr: 0.000010  loss: 0.0274  time: 1.0051  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [3950/7110]  eta: 0:52:59  lr: 0.000010  loss: 0.0302  time: 1.0065  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4000/7110]  eta: 0:52:09  lr: 0.000010  loss: 1.3971  time: 1.0109  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4050/7110]  eta: 0:51:19  lr: 0.000010  loss: 0.1307  time: 1.0011  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4100/7110]  eta: 0:50:29  lr: 0.000010  loss: 0.1511  time: 0.9938  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4150/7110]  eta: 0:49:37  lr: 0.000010  loss: 0.1011  time: 0.9689  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4200/7110]  eta: 0:48:47  lr: 0.000010  loss: 0.0604  time: 1.0088  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4250/7110]  eta: 0:47:57  lr: 0.000010  loss: 0.0698  time: 0.9633  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4300/7110]  eta: 0:47:06  lr: 0.000010  loss: 0.1508  time: 0.9889  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4350/7110]  eta: 0:46:15  lr: 0.000010  loss: 0.2017  time: 0.9688  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4400/7110]  eta: 0:45:24  lr: 0.000010  loss: 0.0799  time: 0.9562  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4450/7110]  eta: 0:44:33  lr: 0.000010  loss: 0.1928  time: 1.0239  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4500/7110]  eta: 0:43:44  lr: 0.000010  loss: 0.2124  time: 1.0875  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4550/7110]  eta: 0:42:52  lr: 0.000010  loss: 0.0712  time: 0.9831  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4600/7110]  eta: 0:42:02  lr: 0.000010  loss: 0.2008  time: 0.9760  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4650/7110]  eta: 0:41:11  lr: 0.000010  loss: 0.0086  time: 0.9907  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4700/7110]  eta: 0:40:20  lr: 0.000010  loss: 0.0758  time: 1.0374  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4750/7110]  eta: 0:39:30  lr: 0.000010  loss: 1.3582  time: 0.9875  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4800/7110]  eta: 0:38:40  lr: 0.000010  loss: 0.0793  time: 1.0279  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4850/7110]  eta: 0:37:50  lr: 0.000010  loss: 0.0499  time: 1.0110  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4900/7110]  eta: 0:37:00  lr: 0.000010  loss: 0.1536  time: 1.0195  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [4950/7110]  eta: 0:36:10  lr: 0.000010  loss: 0.1003  time: 1.0224  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5000/7110]  eta: 0:35:19  lr: 0.000010  loss: 0.2855  time: 1.0249  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5050/7110]  eta: 0:34:29  lr: 0.000010  loss: 0.3068  time: 1.0242  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5100/7110]  eta: 0:33:39  lr: 0.000010  loss: 0.1730  time: 1.0078  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5150/7110]  eta: 0:32:48  lr: 0.000010  loss: 0.2048  time: 0.9840  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5200/7110]  eta: 0:31:57  lr: 0.000010  loss: 0.4750  time: 1.0418  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5250/7110]  eta: 0:31:06  lr: 0.000010  loss: 0.1231  time: 0.9675  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5300/7110]  eta: 0:30:15  lr: 0.000010  loss: 0.0840  time: 0.9437  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5350/7110]  eta: 0:29:25  lr: 0.000010  loss: 0.2088  time: 1.0805  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5400/7110]  eta: 0:28:35  lr: 0.000010  loss: 0.1050  time: 0.9878  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5450/7110]  eta: 0:27:45  lr: 0.000010  loss: 0.4760  time: 1.0776  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5500/7110]  eta: 0:26:55  lr: 0.000010  loss: 0.3438  time: 1.0797  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5550/7110]  eta: 0:26:05  lr: 0.000010  loss: 0.0851  time: 0.9914  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5600/7110]  eta: 0:25:15  lr: 0.000010  loss: 0.0261  time: 1.0021  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5650/7110]  eta: 0:24:24  lr: 0.000010  loss: 0.2626  time: 1.0395  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5700/7110]  eta: 0:23:34  lr: 0.000010  loss: 0.5581  time: 0.9804  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5750/7110]  eta: 0:22:43  lr: 0.000010  loss: 0.1117  time: 0.9552  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5800/7110]  eta: 0:21:52  lr: 0.000010  loss: 0.3503  time: 0.9317  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5850/7110]  eta: 0:21:02  lr: 0.000010  loss: 0.0614  time: 0.9831  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5900/7110]  eta: 0:20:12  lr: 0.000010  loss: 0.0355  time: 0.9410  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [5950/7110]  eta: 0:19:21  lr: 0.000010  loss: 0.0677  time: 0.9786  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6000/7110]  eta: 0:18:31  lr: 0.000010  loss: 0.2565  time: 1.0564  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6050/7110]  eta: 0:17:41  lr: 0.000010  loss: 0.6294  time: 0.9874  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6100/7110]  eta: 0:16:51  lr: 0.000010  loss: 0.8154  time: 1.0553  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6150/7110]  eta: 0:16:01  lr: 0.000010  loss: 0.0918  time: 1.0821  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6200/7110]  eta: 0:15:11  lr: 0.000010  loss: 0.6328  time: 0.9647  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6250/7110]  eta: 0:14:21  lr: 0.000010  loss: 0.1101  time: 1.0408  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6300/7110]  eta: 0:13:31  lr: 0.000010  loss: 1.3909  time: 0.9713  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6350/7110]  eta: 0:12:41  lr: 0.000010  loss: 0.7283  time: 1.0573  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6400/7110]  eta: 0:11:51  lr: 0.000010  loss: 0.0638  time: 0.9964  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6450/7110]  eta: 0:11:00  lr: 0.000010  loss: 0.1732  time: 0.9613  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6500/7110]  eta: 0:10:10  lr: 0.000010  loss: 0.3665  time: 0.9645  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6550/7110]  eta: 0:09:20  lr: 0.000010  loss: 0.2823  time: 0.9822  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6600/7110]  eta: 0:08:30  lr: 0.000010  loss: 0.0163  time: 1.0029  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6650/7110]  eta: 0:07:40  lr: 0.000010  loss: 0.0735  time: 0.9575  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6700/7110]  eta: 0:06:50  lr: 0.000010  loss: 0.2103  time: 1.0584  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6750/7110]  eta: 0:06:00  lr: 0.000010  loss: 0.2688  time: 1.0571  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6800/7110]  eta: 0:05:10  lr: 0.000010  loss: 0.1355  time: 0.9915  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1175  time: 1.0685  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.1033  time: 1.0182  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.0536  time: 0.9919  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 1.2994  time: 1.0348  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1418  time: 1.0048  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.1316  time: 1.0052  data: 0.0000  max mem: 66110
Train: data epoch: [53]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.1511  time: 1.1099  data: 0.0000  max mem: 66110
Train: data epoch: [53] Total time: 1:58:45 (1.0022 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:07:05    time: 20.1513  data: 18.8960  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:12    time: 3.1143  data: 1.7192  max mem: 66110
Evaluation  [  20/1093]  eta: 0:42:15    time: 1.4731  data: 0.0013  max mem: 66110
Evaluation  [  30/1093]  eta: 0:35:08    time: 1.3622  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:32:20    time: 1.2962  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:31    time: 1.4034  data: 0.0010  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:34    time: 1.2864  data: 0.0010  max mem: 66110
Evaluation  [  70/1093]  eta: 0:28:03    time: 1.3621  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:27:28    time: 1.5290  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:27:08    time: 1.5475  data: 0.0009  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:51    time: 1.6010  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:26:23    time: 1.5518  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:38    time: 1.3699  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:25:21    time: 1.4101  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:46    time: 1.4310  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:24:12    time: 1.2799  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:49    time: 1.3425  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:23:16    time: 1.3083  data: 0.0009  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:55    time: 1.2956  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:43    time: 1.4791  data: 0.0009  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:22    time: 1.4727  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:22:14    time: 1.5234  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:52    time: 1.5087  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:28    time: 1.3021  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:21:08    time: 1.3075  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:52    time: 1.4087  data: 0.0009  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:43    time: 1.5649  data: 0.0009  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:21    time: 1.4604  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:20:04    time: 1.3283  data: 0.0009  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:54    time: 1.5333  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:31    time: 1.4179  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:19:09    time: 1.1839  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:54    time: 1.3335  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:36    time: 1.3857  data: 0.0009  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:23    time: 1.4126  data: 0.0009  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:18:07    time: 1.4721  data: 0.0009  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:47    time: 1.3102  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:31    time: 1.2952  data: 0.0011  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:18    time: 1.4608  data: 0.0011  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:17:03    time: 1.4869  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:49    time: 1.4555  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:32    time: 1.3954  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:17    time: 1.3649  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:16:01    time: 1.3918  data: 0.0009  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:42    time: 1.2718  data: 0.0009  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:28    time: 1.2921  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:13    time: 1.4048  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:56    time: 1.3369  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:41    time: 1.3570  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:28    time: 1.4666  data: 0.0009  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:16    time: 1.5735  data: 0.0011  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:14:05    time: 1.6872  data: 0.0011  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:51    time: 1.6029  data: 0.0010  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:36    time: 1.4661  data: 0.0010  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:21    time: 1.4498  data: 0.0010  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:06    time: 1.4209  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:49    time: 1.2550  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:34    time: 1.2823  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:19    time: 1.4100  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:12:02    time: 1.2417  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:49    time: 1.3706  data: 0.0009  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:35    time: 1.5472  data: 0.0009  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:20    time: 1.4119  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:11:05    time: 1.3833  data: 0.0009  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:50    time: 1.3513  data: 0.0009  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:36    time: 1.3985  data: 0.0009  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:21    time: 1.3786  data: 0.0009  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:07    time: 1.4040  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:53    time: 1.5051  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:39    time: 1.5084  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:24    time: 1.4554  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:09    time: 1.3169  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:54    time: 1.2955  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:38    time: 1.1824  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:23    time: 1.1551  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:09    time: 1.3686  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:54    time: 1.4287  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:40    time: 1.3951  data: 0.0009  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:26    time: 1.4901  data: 0.0009  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:11    time: 1.3073  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:57    time: 1.3315  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:43    time: 1.5466  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:29    time: 1.4733  data: 0.0009  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:15    time: 1.5230  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:06:00    time: 1.4551  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:46    time: 1.3892  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:32    time: 1.3894  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:17    time: 1.3617  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:04    time: 1.4848  data: 0.0009  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:49    time: 1.3683  data: 0.0009  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:34    time: 1.2827  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:20    time: 1.4407  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:06    time: 1.4672  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:52    time: 1.3851  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:38    time: 1.4525  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:23    time: 1.4941  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:09    time: 1.4176  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:55    time: 1.4317  data: 0.0009  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:40    time: 1.3636  data: 0.0009  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:26    time: 1.4112  data: 0.0009  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:12    time: 1.5064  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:58    time: 1.4429  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:43    time: 1.2861  data: 0.0010  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:29    time: 1.1515  data: 0.0009  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:15    time: 1.2460  data: 0.0009  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:01    time: 1.3807  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.4486  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4689  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3750  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3528  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3254  data: 0.0404  max mem: 66110
Evaluation Total time: 0:25:51 (1.4192 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_53_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [54]  [   0/7110]  eta: 2 days, 6:19:05  lr: 0.000010  loss: 0.2117  time: 27.5029  data: 0.0001  max mem: 66110
Train: data epoch: [54]  [  50/7110]  eta: 2:52:10  lr: 0.000010  loss: 0.4591  time: 0.9052  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 100/7110]  eta: 2:25:25  lr: 0.000010  loss: 0.0447  time: 1.0501  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 150/7110]  eta: 2:16:52  lr: 0.000010  loss: 0.0694  time: 1.0030  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 200/7110]  eta: 2:10:19  lr: 0.000010  loss: 0.1288  time: 0.9485  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 250/7110]  eta: 2:06:00  lr: 0.000010  loss: 0.2379  time: 0.9938  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 300/7110]  eta: 2:02:56  lr: 0.000010  loss: 0.1095  time: 1.0354  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 350/7110]  eta: 2:00:43  lr: 0.000010  loss: 0.0486  time: 1.0324  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 400/7110]  eta: 1:58:38  lr: 0.000010  loss: 0.1850  time: 0.9615  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 450/7110]  eta: 1:57:16  lr: 0.000010  loss: 0.0779  time: 1.0351  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 500/7110]  eta: 1:55:50  lr: 0.000010  loss: 0.2043  time: 0.9521  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 550/7110]  eta: 1:54:34  lr: 0.000010  loss: 0.3540  time: 1.0064  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 600/7110]  eta: 1:53:28  lr: 0.000010  loss: 0.5622  time: 1.0319  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 650/7110]  eta: 1:52:15  lr: 0.000010  loss: 0.5032  time: 0.9803  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 700/7110]  eta: 1:50:58  lr: 0.000010  loss: 0.2249  time: 0.9999  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 750/7110]  eta: 1:50:04  lr: 0.000010  loss: 0.3286  time: 1.0300  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 800/7110]  eta: 1:49:00  lr: 0.000010  loss: 0.4147  time: 0.9912  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 850/7110]  eta: 1:47:58  lr: 0.000010  loss: 0.0697  time: 1.0138  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 900/7110]  eta: 1:46:55  lr: 0.000010  loss: 0.2060  time: 0.9956  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [ 950/7110]  eta: 1:45:53  lr: 0.000010  loss: 0.3015  time: 1.0174  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1000/7110]  eta: 1:44:57  lr: 0.000010  loss: 0.4755  time: 1.0126  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1050/7110]  eta: 1:43:47  lr: 0.000010  loss: 0.1583  time: 0.9593  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1100/7110]  eta: 1:43:00  lr: 0.000010  loss: 0.0635  time: 0.9592  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1150/7110]  eta: 1:42:01  lr: 0.000010  loss: 0.0489  time: 1.0622  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1200/7110]  eta: 1:40:58  lr: 0.000010  loss: 0.1959  time: 1.0305  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1250/7110]  eta: 1:39:58  lr: 0.000010  loss: 0.1690  time: 0.9988  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1300/7110]  eta: 1:39:03  lr: 0.000010  loss: 0.0613  time: 0.9928  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1350/7110]  eta: 1:37:58  lr: 0.000010  loss: 0.0956  time: 0.9360  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1400/7110]  eta: 1:37:03  lr: 0.000010  loss: 0.1631  time: 0.9989  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1450/7110]  eta: 1:36:17  lr: 0.000010  loss: 0.4580  time: 1.0676  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1500/7110]  eta: 1:35:25  lr: 0.000010  loss: 0.1620  time: 1.0337  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1550/7110]  eta: 1:34:31  lr: 0.000010  loss: 0.1151  time: 1.0002  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1600/7110]  eta: 1:33:33  lr: 0.000010  loss: 0.4263  time: 0.9946  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1650/7110]  eta: 1:32:44  lr: 0.000010  loss: 0.1079  time: 1.0404  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1700/7110]  eta: 1:31:53  lr: 0.000010  loss: 0.4348  time: 1.0053  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1750/7110]  eta: 1:31:09  lr: 0.000010  loss: 0.0640  time: 1.0335  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1800/7110]  eta: 1:30:19  lr: 0.000010  loss: 0.2471  time: 0.9882  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1850/7110]  eta: 1:29:21  lr: 0.000010  loss: 0.0390  time: 0.9566  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1900/7110]  eta: 1:28:26  lr: 0.000010  loss: 0.3128  time: 0.9600  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [1950/7110]  eta: 1:27:29  lr: 0.000010  loss: 0.1620  time: 0.9413  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2000/7110]  eta: 1:26:35  lr: 0.000010  loss: 0.3414  time: 1.0302  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2050/7110]  eta: 1:25:46  lr: 0.000010  loss: 0.1085  time: 0.9854  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2100/7110]  eta: 1:25:01  lr: 0.000010  loss: 0.2701  time: 1.0929  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2150/7110]  eta: 1:24:08  lr: 0.000010  loss: 0.2441  time: 0.9775  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2200/7110]  eta: 1:23:17  lr: 0.000010  loss: 0.4456  time: 0.9872  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2250/7110]  eta: 1:22:25  lr: 0.000010  loss: 0.3269  time: 1.0473  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2300/7110]  eta: 1:21:31  lr: 0.000010  loss: 0.1092  time: 0.9767  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2350/7110]  eta: 1:20:35  lr: 0.000010  loss: 0.0716  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2400/7110]  eta: 1:19:42  lr: 0.000010  loss: 0.0165  time: 1.0129  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2450/7110]  eta: 1:18:50  lr: 0.000010  loss: 0.3288  time: 0.9974  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2500/7110]  eta: 1:17:59  lr: 0.000010  loss: 0.3356  time: 0.9886  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2550/7110]  eta: 1:17:05  lr: 0.000010  loss: 0.4769  time: 0.9374  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2600/7110]  eta: 1:16:14  lr: 0.000010  loss: 0.4111  time: 1.0088  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2650/7110]  eta: 1:15:23  lr: 0.000010  loss: 0.4183  time: 1.0113  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2700/7110]  eta: 1:14:34  lr: 0.000010  loss: 0.0482  time: 1.0186  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2750/7110]  eta: 1:13:42  lr: 0.000010  loss: 0.4790  time: 0.9819  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2800/7110]  eta: 1:12:51  lr: 0.000010  loss: 0.0417  time: 0.9985  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2850/7110]  eta: 1:11:58  lr: 0.000010  loss: 0.2700  time: 0.9736  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2900/7110]  eta: 1:11:08  lr: 0.000010  loss: 0.0690  time: 1.0362  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [2950/7110]  eta: 1:10:15  lr: 0.000010  loss: 0.2142  time: 0.9355  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3000/7110]  eta: 1:09:25  lr: 0.000010  loss: 0.0153  time: 0.9940  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3050/7110]  eta: 1:08:32  lr: 0.000010  loss: 0.2505  time: 0.9781  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3100/7110]  eta: 1:07:40  lr: 0.000010  loss: 0.1011  time: 0.9940  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3150/7110]  eta: 1:06:50  lr: 0.000010  loss: 0.1030  time: 1.0719  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3200/7110]  eta: 1:05:59  lr: 0.000010  loss: 0.1865  time: 1.0128  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3250/7110]  eta: 1:05:06  lr: 0.000010  loss: 0.4743  time: 0.9866  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3300/7110]  eta: 1:04:15  lr: 0.000010  loss: 0.1387  time: 1.0054  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3350/7110]  eta: 1:03:24  lr: 0.000010  loss: 0.5288  time: 0.9857  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3400/7110]  eta: 1:02:32  lr: 0.000010  loss: 0.0779  time: 1.0170  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3450/7110]  eta: 1:01:41  lr: 0.000010  loss: 0.0444  time: 1.0428  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3500/7110]  eta: 1:00:50  lr: 0.000010  loss: 0.0580  time: 1.0214  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3550/7110]  eta: 0:59:59  lr: 0.000010  loss: 0.1065  time: 1.0673  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3600/7110]  eta: 0:59:06  lr: 0.000010  loss: 0.1118  time: 0.9358  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3650/7110]  eta: 0:58:16  lr: 0.000010  loss: 0.0225  time: 1.0237  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3700/7110]  eta: 0:57:25  lr: 0.000010  loss: 0.1615  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3750/7110]  eta: 0:56:33  lr: 0.000010  loss: 0.2890  time: 0.9639  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3800/7110]  eta: 0:55:44  lr: 0.000010  loss: 0.0394  time: 1.0379  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3850/7110]  eta: 0:54:52  lr: 0.000010  loss: 0.0941  time: 0.9590  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3900/7110]  eta: 0:53:59  lr: 0.000010  loss: 0.0242  time: 0.9601  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [3950/7110]  eta: 0:53:09  lr: 0.000010  loss: 0.1363  time: 0.9994  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4000/7110]  eta: 0:52:18  lr: 0.000010  loss: 0.0442  time: 0.9888  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4050/7110]  eta: 0:51:29  lr: 0.000010  loss: 0.0755  time: 0.9913  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4100/7110]  eta: 0:50:38  lr: 0.000010  loss: 0.0513  time: 0.9646  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4150/7110]  eta: 0:49:47  lr: 0.000010  loss: 0.3430  time: 1.0391  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4200/7110]  eta: 0:48:56  lr: 0.000010  loss: 0.3689  time: 1.0329  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4250/7110]  eta: 0:48:04  lr: 0.000010  loss: 0.0415  time: 0.8929  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4300/7110]  eta: 0:47:13  lr: 0.000010  loss: 0.0313  time: 0.9697  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4350/7110]  eta: 0:46:21  lr: 0.000010  loss: 0.0362  time: 0.9575  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4400/7110]  eta: 0:45:31  lr: 0.000010  loss: 0.3301  time: 1.0467  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4450/7110]  eta: 0:44:40  lr: 0.000010  loss: 0.2661  time: 1.0258  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4500/7110]  eta: 0:43:50  lr: 0.000010  loss: 0.4092  time: 1.0569  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4550/7110]  eta: 0:42:59  lr: 0.000010  loss: 0.2420  time: 0.9843  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4600/7110]  eta: 0:42:08  lr: 0.000010  loss: 0.2223  time: 0.9570  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4650/7110]  eta: 0:41:16  lr: 0.000010  loss: 0.6447  time: 0.9693  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4700/7110]  eta: 0:40:25  lr: 0.000010  loss: 0.1222  time: 0.9767  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4750/7110]  eta: 0:39:35  lr: 0.000010  loss: 0.2662  time: 1.0422  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4800/7110]  eta: 0:38:43  lr: 0.000010  loss: 0.2068  time: 0.9896  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4850/7110]  eta: 0:37:53  lr: 0.000010  loss: 0.5103  time: 1.0030  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4900/7110]  eta: 0:37:02  lr: 0.000010  loss: 0.4964  time: 1.0189  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [4950/7110]  eta: 0:36:12  lr: 0.000010  loss: 0.0593  time: 0.9584  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5000/7110]  eta: 0:35:22  lr: 0.000010  loss: 0.0346  time: 0.9923  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5050/7110]  eta: 0:34:32  lr: 0.000010  loss: 0.0294  time: 1.0125  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5100/7110]  eta: 0:33:42  lr: 0.000010  loss: 0.3584  time: 1.0159  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5150/7110]  eta: 0:32:50  lr: 0.000010  loss: 0.2060  time: 0.9708  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5200/7110]  eta: 0:32:00  lr: 0.000010  loss: 0.0517  time: 0.9770  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5250/7110]  eta: 0:31:09  lr: 0.000010  loss: 0.0265  time: 1.0002  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5300/7110]  eta: 0:30:19  lr: 0.000010  loss: 0.0306  time: 1.0209  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5350/7110]  eta: 0:29:29  lr: 0.000010  loss: 0.0372  time: 1.0315  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5400/7110]  eta: 0:28:38  lr: 0.000010  loss: 0.4005  time: 0.9731  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5450/7110]  eta: 0:27:48  lr: 0.000010  loss: 0.2551  time: 0.9890  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5500/7110]  eta: 0:26:58  lr: 0.000010  loss: 0.4449  time: 0.9787  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5550/7110]  eta: 0:26:07  lr: 0.000010  loss: 0.1860  time: 1.0112  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5600/7110]  eta: 0:25:17  lr: 0.000010  loss: 0.0464  time: 0.9955  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5650/7110]  eta: 0:24:27  lr: 0.000010  loss: 0.3537  time: 1.0272  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5700/7110]  eta: 0:23:36  lr: 0.000010  loss: 0.1011  time: 1.0220  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5750/7110]  eta: 0:22:46  lr: 0.000010  loss: 0.2431  time: 1.0396  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5800/7110]  eta: 0:21:56  lr: 0.000010  loss: 0.0483  time: 1.0174  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5850/7110]  eta: 0:21:06  lr: 0.000010  loss: 0.2224  time: 1.0358  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5900/7110]  eta: 0:20:15  lr: 0.000010  loss: 0.2349  time: 0.9319  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [5950/7110]  eta: 0:19:25  lr: 0.000010  loss: 0.1489  time: 0.9917  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6000/7110]  eta: 0:18:34  lr: 0.000010  loss: 0.4343  time: 0.9828  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6050/7110]  eta: 0:17:44  lr: 0.000010  loss: 0.5324  time: 1.0026  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6100/7110]  eta: 0:16:54  lr: 0.000010  loss: 0.7485  time: 1.0201  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6150/7110]  eta: 0:16:04  lr: 0.000010  loss: 0.0551  time: 1.0068  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.2757  time: 1.0472  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.0800  time: 1.0007  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6300/7110]  eta: 0:13:33  lr: 0.000010  loss: 0.1570  time: 0.9506  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6350/7110]  eta: 0:12:43  lr: 0.000010  loss: 0.1133  time: 0.9972  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.2895  time: 1.0229  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.4685  time: 1.0148  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.0493  time: 0.9941  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6550/7110]  eta: 0:09:22  lr: 0.000010  loss: 0.1113  time: 0.9610  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6600/7110]  eta: 0:08:32  lr: 0.000010  loss: 0.0467  time: 1.0379  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 1.3228  time: 0.9550  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.0091  time: 1.0292  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.1921  time: 1.0037  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.0535  time: 1.0288  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.1467  time: 0.9653  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.2986  time: 1.0226  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.0571  time: 0.9488  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.0240  time: 1.0797  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.2218  time: 1.0242  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.2975  time: 0.9862  data: 0.0000  max mem: 66110
Train: data epoch: [54]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.1507  time: 1.1422  data: 0.0000  max mem: 66110
Train: data epoch: [54] Total time: 1:58:59 (1.0041 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:02:57    time: 19.9247  data: 18.6626  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:38    time: 3.1377  data: 1.6975  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:18    time: 1.4296  data: 0.0009  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:27    time: 1.2895  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:55    time: 1.3037  data: 0.0010  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:12    time: 1.4156  data: 0.0009  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:17    time: 1.2815  data: 0.0009  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:32    time: 1.3025  data: 0.0009  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:54    time: 1.4421  data: 0.0009  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:34    time: 1.4985  data: 0.0009  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:20    time: 1.5834  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:57    time: 1.5597  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:14    time: 1.3796  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:57    time: 1.3961  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:24    time: 1.4167  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:54    time: 1.2979  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:34    time: 1.3698  data: 0.0009  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:23:02    time: 1.3213  data: 0.0009  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:44    time: 1.3213  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:32    time: 1.4975  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:22:11    time: 1.4566  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:22:00    time: 1.4738  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:35    time: 1.4201  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:12    time: 1.2554  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:51    time: 1.2737  data: 0.0010  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:36    time: 1.3722  data: 0.0010  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:29    time: 1.5806  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:20:05    time: 1.4492  data: 0.0011  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:47    time: 1.2630  data: 0.0011  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:38    time: 1.4974  data: 0.0012  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:15    time: 1.4207  data: 0.0011  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:54    time: 1.1871  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:42    time: 1.3721  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:24    time: 1.4217  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:11    time: 1.4047  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:57    time: 1.4798  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:38    time: 1.3502  data: 0.0010  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:22    time: 1.3003  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:10    time: 1.4753  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:56    time: 1.5140  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:42    time: 1.4505  data: 0.0010  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:25    time: 1.3926  data: 0.0010  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:10    time: 1.3587  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:54    time: 1.3871  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:35    time: 1.2302  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:19    time: 1.2166  data: 0.0009  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:04    time: 1.3678  data: 0.0009  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:48    time: 1.3472  data: 0.0009  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:34    time: 1.3717  data: 0.0009  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:21    time: 1.4696  data: 0.0009  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:09    time: 1.5718  data: 0.0009  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:57    time: 1.6188  data: 0.0009  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:42    time: 1.5237  data: 0.0009  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:28    time: 1.4534  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:14    time: 1.4673  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:13:00    time: 1.4390  data: 0.0009  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:41    time: 1.1777  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:26    time: 1.1575  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:11    time: 1.3491  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:54    time: 1.2514  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:42    time: 1.3941  data: 0.0009  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:28    time: 1.5474  data: 0.0009  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:13    time: 1.4105  data: 0.0009  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:59    time: 1.3830  data: 0.0009  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:44    time: 1.3537  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:30    time: 1.3806  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:15    time: 1.3911  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:10:01    time: 1.4356  data: 0.0009  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:47    time: 1.4925  data: 0.0009  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:34    time: 1.5015  data: 0.0009  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:19    time: 1.4525  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:03    time: 1.2446  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:49    time: 1.2645  data: 0.0010  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:33    time: 1.2415  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:18    time: 1.1957  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:04    time: 1.3687  data: 0.0010  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:50    time: 1.4154  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:36    time: 1.3687  data: 0.0009  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:22    time: 1.4598  data: 0.0009  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:07    time: 1.3182  data: 0.0009  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:54    time: 1.3777  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:40    time: 1.5624  data: 0.0010  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:26    time: 1.4364  data: 0.0009  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:12    time: 1.5189  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:58    time: 1.4711  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:44    time: 1.4186  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:29    time: 1.4023  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:15    time: 1.3533  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:01    time: 1.4824  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:47    time: 1.3695  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:32    time: 1.2721  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:18    time: 1.3964  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:04    time: 1.3757  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:50    time: 1.3168  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:36    time: 1.4015  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:21    time: 1.3903  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:07    time: 1.3808  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:53    time: 1.4415  data: 0.0009  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:39    time: 1.3596  data: 0.0009  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:25    time: 1.3861  data: 0.0009  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:11    time: 1.4908  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:57    time: 1.4736  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.3022  data: 0.0009  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1462  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2354  data: 0.0009  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3363  data: 0.0009  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3856  data: 0.0009  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4402  data: 0.0009  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3670  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3446  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3179  data: 0.0414  max mem: 66110
Evaluation Total time: 0:25:36 (1.4057 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_54_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [55]  [   0/7110]  eta: 2 days, 5:51:50  lr: 0.000010  loss: 0.0250  time: 27.2730  data: 0.0001  max mem: 66110
Train: data epoch: [55]  [  50/7110]  eta: 3:00:01  lr: 0.000010  loss: 0.1064  time: 1.0116  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 100/7110]  eta: 2:29:56  lr: 0.000010  loss: 0.1323  time: 0.9875  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 150/7110]  eta: 2:17:13  lr: 0.000010  loss: 0.1902  time: 1.0200  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 200/7110]  eta: 2:10:57  lr: 0.000010  loss: 0.1064  time: 0.9871  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 250/7110]  eta: 2:06:44  lr: 0.000010  loss: 0.3088  time: 0.9808  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 300/7110]  eta: 2:03:06  lr: 0.000010  loss: 0.2466  time: 0.9276  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 350/7110]  eta: 2:00:54  lr: 0.000010  loss: 0.0771  time: 0.9698  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 400/7110]  eta: 1:59:01  lr: 0.000010  loss: 0.1733  time: 0.9536  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 450/7110]  eta: 1:57:32  lr: 0.000010  loss: 0.2509  time: 1.0832  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 500/7110]  eta: 1:55:49  lr: 0.000010  loss: 0.1971  time: 0.9854  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 550/7110]  eta: 1:54:25  lr: 0.000010  loss: 0.1911  time: 1.0446  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 600/7110]  eta: 1:52:52  lr: 0.000010  loss: 0.1116  time: 0.9693  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 650/7110]  eta: 1:51:35  lr: 0.000010  loss: 0.2960  time: 0.9804  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 700/7110]  eta: 1:50:01  lr: 0.000010  loss: 0.1497  time: 0.9599  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 750/7110]  eta: 1:48:57  lr: 0.000010  loss: 1.3307  time: 1.0087  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 800/7110]  eta: 1:47:47  lr: 0.000010  loss: 0.0194  time: 0.9946  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 850/7110]  eta: 1:46:49  lr: 0.000010  loss: 0.1651  time: 0.9648  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 900/7110]  eta: 1:45:49  lr: 0.000010  loss: 0.2197  time: 0.9783  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [ 950/7110]  eta: 1:44:38  lr: 0.000010  loss: 0.2059  time: 0.9542  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1000/7110]  eta: 1:43:50  lr: 0.000010  loss: 0.1538  time: 1.0438  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1050/7110]  eta: 1:43:03  lr: 0.000010  loss: 0.0431  time: 1.0216  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1100/7110]  eta: 1:42:09  lr: 0.000010  loss: 0.1202  time: 1.0170  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1150/7110]  eta: 1:41:11  lr: 0.000010  loss: 0.1349  time: 0.9708  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1200/7110]  eta: 1:40:18  lr: 0.000010  loss: 0.6953  time: 1.0098  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1250/7110]  eta: 1:39:13  lr: 0.000010  loss: 0.1545  time: 0.9667  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1300/7110]  eta: 1:38:18  lr: 0.000010  loss: 0.3402  time: 1.0237  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1350/7110]  eta: 1:37:18  lr: 0.000010  loss: 0.1893  time: 0.9204  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1400/7110]  eta: 1:36:23  lr: 0.000010  loss: 0.0610  time: 0.9776  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1450/7110]  eta: 1:35:32  lr: 0.000010  loss: 0.3648  time: 1.0266  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1500/7110]  eta: 1:34:39  lr: 0.000010  loss: 0.0731  time: 0.9860  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1550/7110]  eta: 1:33:45  lr: 0.000010  loss: 0.1511  time: 1.0300  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1600/7110]  eta: 1:32:55  lr: 0.000010  loss: 0.0473  time: 1.0252  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1650/7110]  eta: 1:32:04  lr: 0.000010  loss: 0.2368  time: 1.0300  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1700/7110]  eta: 1:31:15  lr: 0.000010  loss: 0.4312  time: 1.0032  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1750/7110]  eta: 1:30:25  lr: 0.000010  loss: 0.1972  time: 1.0076  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1800/7110]  eta: 1:29:31  lr: 0.000010  loss: 0.2191  time: 0.9878  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1850/7110]  eta: 1:28:39  lr: 0.000010  loss: 0.4680  time: 1.0127  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1900/7110]  eta: 1:27:54  lr: 0.000010  loss: 0.0711  time: 1.0485  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [1950/7110]  eta: 1:27:03  lr: 0.000010  loss: 0.0718  time: 1.0091  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2000/7110]  eta: 1:26:13  lr: 0.000010  loss: 0.3885  time: 1.0244  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2050/7110]  eta: 1:25:21  lr: 0.000010  loss: 0.2182  time: 0.9887  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2100/7110]  eta: 1:24:31  lr: 0.000010  loss: 0.1062  time: 1.0204  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2150/7110]  eta: 1:23:35  lr: 0.000010  loss: 0.3530  time: 0.9919  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2200/7110]  eta: 1:22:45  lr: 0.000010  loss: 0.3662  time: 1.0072  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2250/7110]  eta: 1:21:50  lr: 0.000010  loss: 0.0489  time: 0.9710  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2300/7110]  eta: 1:21:00  lr: 0.000010  loss: 0.3802  time: 1.0361  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2350/7110]  eta: 1:20:07  lr: 0.000010  loss: 0.0639  time: 0.9102  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2400/7110]  eta: 1:19:19  lr: 0.000010  loss: 0.3107  time: 1.0278  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2450/7110]  eta: 1:18:28  lr: 0.000010  loss: 0.4636  time: 1.0127  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2500/7110]  eta: 1:17:36  lr: 0.000010  loss: 0.0700  time: 0.9538  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2550/7110]  eta: 1:16:44  lr: 0.000010  loss: 0.0955  time: 0.9745  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2600/7110]  eta: 1:15:51  lr: 0.000010  loss: 0.7236  time: 1.0063  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2650/7110]  eta: 1:14:59  lr: 0.000010  loss: 0.0041  time: 1.0033  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2700/7110]  eta: 1:14:08  lr: 0.000010  loss: 0.0675  time: 0.9590  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2750/7110]  eta: 1:13:19  lr: 0.000010  loss: 0.0791  time: 1.0594  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2800/7110]  eta: 1:12:27  lr: 0.000010  loss: 0.1785  time: 1.0096  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2850/7110]  eta: 1:11:37  lr: 0.000010  loss: 0.1460  time: 1.0396  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2900/7110]  eta: 1:10:47  lr: 0.000010  loss: 0.0649  time: 0.9567  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [2950/7110]  eta: 1:09:55  lr: 0.000010  loss: 1.6022  time: 1.0031  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3000/7110]  eta: 1:09:05  lr: 0.000010  loss: 0.2745  time: 0.9991  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3050/7110]  eta: 1:08:10  lr: 0.000010  loss: 0.1317  time: 0.9496  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3100/7110]  eta: 1:07:20  lr: 0.000010  loss: 0.2249  time: 1.0694  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3150/7110]  eta: 1:06:29  lr: 0.000010  loss: 0.6899  time: 1.0066  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3200/7110]  eta: 1:05:38  lr: 0.000010  loss: 0.4977  time: 1.0396  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3250/7110]  eta: 1:04:46  lr: 0.000010  loss: 0.4502  time: 0.9776  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3300/7110]  eta: 1:03:55  lr: 0.000010  loss: 0.1521  time: 0.9681  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3350/7110]  eta: 1:03:05  lr: 0.000010  loss: 0.1723  time: 0.9991  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3400/7110]  eta: 1:02:14  lr: 0.000010  loss: 0.0716  time: 0.9986  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3450/7110]  eta: 1:01:24  lr: 0.000010  loss: 0.0745  time: 0.9855  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3500/7110]  eta: 1:00:34  lr: 0.000010  loss: 0.2381  time: 1.0051  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3550/7110]  eta: 0:59:44  lr: 0.000010  loss: 0.3415  time: 1.0266  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3600/7110]  eta: 0:58:55  lr: 0.000010  loss: 0.2683  time: 1.0594  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3650/7110]  eta: 0:58:04  lr: 0.000010  loss: 0.1637  time: 0.9878  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3700/7110]  eta: 0:57:13  lr: 0.000010  loss: 0.0174  time: 1.0070  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3750/7110]  eta: 0:56:24  lr: 0.000010  loss: 0.0885  time: 1.0167  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3800/7110]  eta: 0:55:33  lr: 0.000010  loss: 1.1393  time: 0.9442  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3850/7110]  eta: 0:54:42  lr: 0.000010  loss: 0.2637  time: 1.0190  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3900/7110]  eta: 0:53:51  lr: 0.000010  loss: 0.2240  time: 0.9981  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [3950/7110]  eta: 0:53:01  lr: 0.000010  loss: 0.9865  time: 1.0224  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4000/7110]  eta: 0:52:11  lr: 0.000010  loss: 0.4373  time: 1.0157  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4050/7110]  eta: 0:51:22  lr: 0.000010  loss: 0.0366  time: 1.0431  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4100/7110]  eta: 0:50:32  lr: 0.000010  loss: 0.1346  time: 0.9940  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4150/7110]  eta: 0:49:41  lr: 0.000010  loss: 0.1091  time: 0.9974  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4200/7110]  eta: 0:48:49  lr: 0.000010  loss: 0.0973  time: 1.0196  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4250/7110]  eta: 0:47:58  lr: 0.000010  loss: 0.0821  time: 0.9452  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4300/7110]  eta: 0:47:07  lr: 0.000010  loss: 0.0831  time: 1.0217  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4350/7110]  eta: 0:46:17  lr: 0.000010  loss: 0.1052  time: 0.9604  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4400/7110]  eta: 0:45:27  lr: 0.000010  loss: 0.0577  time: 1.0071  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4450/7110]  eta: 0:44:36  lr: 0.000010  loss: 0.0432  time: 1.0614  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4500/7110]  eta: 0:43:46  lr: 0.000010  loss: 1.6596  time: 1.0549  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4550/7110]  eta: 0:42:55  lr: 0.000010  loss: 0.2662  time: 0.9243  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4600/7110]  eta: 0:42:03  lr: 0.000010  loss: 0.1846  time: 0.9523  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4650/7110]  eta: 0:41:14  lr: 0.000010  loss: 0.2941  time: 1.0691  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4700/7110]  eta: 0:40:23  lr: 0.000010  loss: 0.1017  time: 0.9909  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4750/7110]  eta: 0:39:32  lr: 0.000010  loss: 0.2307  time: 1.0045  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4800/7110]  eta: 0:38:42  lr: 0.000010  loss: 0.2904  time: 1.0216  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4850/7110]  eta: 0:37:52  lr: 0.000010  loss: 0.6834  time: 1.0001  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4900/7110]  eta: 0:37:02  lr: 0.000010  loss: 0.0790  time: 1.0313  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [4950/7110]  eta: 0:36:11  lr: 0.000010  loss: 0.0563  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5000/7110]  eta: 0:35:21  lr: 0.000010  loss: 0.0657  time: 1.0146  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5050/7110]  eta: 0:34:30  lr: 0.000010  loss: 0.0628  time: 0.9139  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5100/7110]  eta: 0:33:40  lr: 0.000010  loss: 0.2402  time: 0.9960  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5150/7110]  eta: 0:32:49  lr: 0.000010  loss: 0.1522  time: 1.0148  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5200/7110]  eta: 0:31:59  lr: 0.000010  loss: 0.6999  time: 1.0583  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5250/7110]  eta: 0:31:10  lr: 0.000010  loss: 0.3654  time: 1.0711  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5300/7110]  eta: 0:30:19  lr: 0.000010  loss: 0.0580  time: 1.0098  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5350/7110]  eta: 0:29:29  lr: 0.000010  loss: 0.1840  time: 0.9464  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5400/7110]  eta: 0:28:38  lr: 0.000010  loss: 0.3133  time: 1.0027  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5450/7110]  eta: 0:27:48  lr: 0.000010  loss: 0.1263  time: 0.9626  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5500/7110]  eta: 0:26:57  lr: 0.000010  loss: 0.2287  time: 0.9548  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5550/7110]  eta: 0:26:07  lr: 0.000010  loss: 0.1229  time: 1.0431  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5600/7110]  eta: 0:25:17  lr: 0.000010  loss: 0.0310  time: 1.0223  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5650/7110]  eta: 0:24:27  lr: 0.000010  loss: 0.2352  time: 1.0078  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5700/7110]  eta: 0:23:36  lr: 0.000010  loss: 0.1368  time: 0.9675  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5750/7110]  eta: 0:22:46  lr: 0.000010  loss: 0.0607  time: 1.0227  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5800/7110]  eta: 0:21:57  lr: 0.000010  loss: 1.5835  time: 1.0748  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5850/7110]  eta: 0:21:06  lr: 0.000010  loss: 0.1493  time: 0.9844  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5900/7110]  eta: 0:20:16  lr: 0.000010  loss: 0.0062  time: 1.0659  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [5950/7110]  eta: 0:19:25  lr: 0.000010  loss: 0.0693  time: 0.9813  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6000/7110]  eta: 0:18:35  lr: 0.000010  loss: 0.0276  time: 0.9594  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6050/7110]  eta: 0:17:45  lr: 0.000010  loss: 0.4176  time: 1.0366  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6100/7110]  eta: 0:16:54  lr: 0.000010  loss: 0.0742  time: 0.9829  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6150/7110]  eta: 0:16:04  lr: 0.000010  loss: 0.2359  time: 0.9223  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6200/7110]  eta: 0:15:13  lr: 0.000010  loss: 0.1635  time: 0.9389  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6250/7110]  eta: 0:14:23  lr: 0.000010  loss: 0.0374  time: 1.0271  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6300/7110]  eta: 0:13:33  lr: 0.000010  loss: 0.3813  time: 0.9484  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6350/7110]  eta: 0:12:42  lr: 0.000010  loss: 0.6666  time: 0.9968  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6400/7110]  eta: 0:11:52  lr: 0.000010  loss: 0.0551  time: 0.9971  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6450/7110]  eta: 0:11:02  lr: 0.000010  loss: 0.0870  time: 0.9508  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6500/7110]  eta: 0:10:12  lr: 0.000010  loss: 0.2207  time: 0.9461  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6550/7110]  eta: 0:09:21  lr: 0.000010  loss: 0.2378  time: 1.0489  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6600/7110]  eta: 0:08:31  lr: 0.000010  loss: 0.1386  time: 1.0368  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6650/7110]  eta: 0:07:41  lr: 0.000010  loss: 0.1680  time: 1.0463  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6700/7110]  eta: 0:06:51  lr: 0.000010  loss: 0.2159  time: 1.0224  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.3537  time: 0.9807  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.6854  time: 1.0135  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6850/7110]  eta: 0:04:20  lr: 0.000010  loss: 0.0668  time: 1.0022  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6900/7110]  eta: 0:03:30  lr: 0.000010  loss: 0.0282  time: 0.9886  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.4377  time: 0.9912  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.2364  time: 0.9896  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.2794  time: 0.9241  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.3460  time: 0.9894  data: 0.0000  max mem: 66110
Train: data epoch: [55]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.3944  time: 1.1218  data: 0.0000  max mem: 66110
Train: data epoch: [55] Total time: 1:58:54 (1.0035 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:08:15    time: 20.2157  data: 18.9690  max mem: 66110
Evaluation  [  10/1093]  eta: 0:56:32    time: 3.1323  data: 1.7253  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:09    time: 1.4058  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:18    time: 1.2785  data: 0.0009  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:47    time: 1.2962  data: 0.0009  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:05    time: 1.4124  data: 0.0009  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:11    time: 1.2808  data: 0.0009  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:27    time: 1.3017  data: 0.0009  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:52    time: 1.4522  data: 0.0009  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:33    time: 1.5151  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:11    time: 1.5440  data: 0.0009  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:44    time: 1.4911  data: 0.0009  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:25:03    time: 1.3566  data: 0.0009  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:46    time: 1.3863  data: 0.0009  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:09    time: 1.3766  data: 0.0009  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:40    time: 1.2626  data: 0.0010  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:18    time: 1.3436  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:49    time: 1.3153  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:31    time: 1.3297  data: 0.0009  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:16    time: 1.4497  data: 0.0009  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:54    time: 1.4016  data: 0.0009  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:45    time: 1.4580  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:21    time: 1.4252  data: 0.0009  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:20:59    time: 1.2616  data: 0.0009  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:37    time: 1.2629  data: 0.0009  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:23    time: 1.3623  data: 0.0009  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:14    time: 1.5473  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:52    time: 1.4137  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:36    time: 1.2999  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:28    time: 1.5373  data: 0.0009  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:06    time: 1.4168  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:45    time: 1.1811  data: 0.0010  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:33    time: 1.3697  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:15    time: 1.4137  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:03    time: 1.4119  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:48    time: 1.4759  data: 0.0010  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:30    time: 1.3391  data: 0.0009  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:14    time: 1.3124  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:02    time: 1.4613  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:48    time: 1.5056  data: 0.0010  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:35    time: 1.4615  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:20    time: 1.4339  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:05    time: 1.4111  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:50    time: 1.3892  data: 0.0010  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:30    time: 1.2152  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:16    time: 1.2467  data: 0.0010  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:15:00    time: 1.3755  data: 0.0010  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:44    time: 1.3062  data: 0.0010  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:30    time: 1.3654  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:17    time: 1.4847  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:06    time: 1.5769  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:53    time: 1.5956  data: 0.0009  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:39    time: 1.5017  data: 0.0009  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:25    time: 1.4504  data: 0.0009  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:11    time: 1.4567  data: 0.0009  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:56    time: 1.4290  data: 0.0010  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:37    time: 1.1869  data: 0.0010  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:23    time: 1.1705  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:08    time: 1.3641  data: 0.0010  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:51    time: 1.2455  data: 0.0010  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:39    time: 1.3685  data: 0.0010  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:25    time: 1.5326  data: 0.0010  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:10    time: 1.4079  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:56    time: 1.3925  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:41    time: 1.3551  data: 0.0010  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:27    time: 1.3757  data: 0.0010  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:12    time: 1.3625  data: 0.0010  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:58    time: 1.3506  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:44    time: 1.4372  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:30    time: 1.4747  data: 0.0009  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:16    time: 1.4394  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:00    time: 1.2410  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:46    time: 1.2231  data: 0.0009  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:30    time: 1.2057  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:15    time: 1.1735  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:01    time: 1.3757  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:47    time: 1.4422  data: 0.0009  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:33    time: 1.3869  data: 0.0009  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:20    time: 1.4734  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:04    time: 1.3130  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:51    time: 1.3339  data: 0.0009  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:37    time: 1.5246  data: 0.0009  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:23    time: 1.4144  data: 0.0009  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:10    time: 1.4812  data: 0.0009  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:55    time: 1.4595  data: 0.0009  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:41    time: 1.3951  data: 0.0009  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:27    time: 1.3618  data: 0.0009  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:13    time: 1.3266  data: 0.0009  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:04:59    time: 1.4869  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:45    time: 1.3682  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:30    time: 1.2506  data: 0.0009  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:16    time: 1.4024  data: 0.0009  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:02    time: 1.4178  data: 0.0009  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:48    time: 1.3520  data: 0.0009  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:34    time: 1.3760  data: 0.0009  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:20    time: 1.3729  data: 0.0009  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:06    time: 1.3697  data: 0.0009  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:52    time: 1.4277  data: 0.0009  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:38    time: 1.3614  data: 0.0009  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.4225  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:10    time: 1.5396  data: 0.0009  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4640  data: 0.0010  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.2693  data: 0.0011  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1328  data: 0.0011  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2540  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3509  data: 0.0011  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.3729  data: 0.0011  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4421  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3732  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3440  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3140  data: 0.0409  max mem: 66110
Evaluation Total time: 0:25:28 (1.3982 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_55_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [56]  [   0/7110]  eta: 2 days, 5:51:35  lr: 0.000010  loss: 0.2255  time: 27.2708  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [  50/7110]  eta: 2:56:24  lr: 0.000010  loss: 0.2721  time: 0.9837  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 100/7110]  eta: 2:27:18  lr: 0.000010  loss: 0.2091  time: 1.0158  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 150/7110]  eta: 2:16:23  lr: 0.000010  loss: 0.4662  time: 1.0330  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 200/7110]  eta: 2:10:35  lr: 0.000010  loss: 0.1858  time: 1.0103  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 250/7110]  eta: 2:07:27  lr: 0.000010  loss: 0.0839  time: 1.0939  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 300/7110]  eta: 2:04:45  lr: 0.000010  loss: 0.2474  time: 0.9884  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 350/7110]  eta: 2:01:57  lr: 0.000010  loss: 0.0798  time: 0.9466  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 400/7110]  eta: 2:00:02  lr: 0.000010  loss: 0.0458  time: 1.0133  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 450/7110]  eta: 1:58:14  lr: 0.000010  loss: 0.3041  time: 1.0044  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 500/7110]  eta: 1:56:31  lr: 0.000010  loss: 0.0833  time: 1.0156  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 550/7110]  eta: 1:55:21  lr: 0.000010  loss: 0.2507  time: 0.9916  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 600/7110]  eta: 1:54:00  lr: 0.000010  loss: 1.6626  time: 1.0224  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 650/7110]  eta: 1:52:34  lr: 0.000010  loss: 0.2324  time: 0.9944  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 700/7110]  eta: 1:51:17  lr: 0.000010  loss: 0.1951  time: 0.9919  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 750/7110]  eta: 1:50:21  lr: 0.000010  loss: 0.4021  time: 1.0316  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 800/7110]  eta: 1:49:32  lr: 0.000010  loss: 0.6204  time: 1.0322  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 850/7110]  eta: 1:48:21  lr: 0.000010  loss: 0.3151  time: 0.9879  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 900/7110]  eta: 1:47:19  lr: 0.000010  loss: 0.1978  time: 1.0146  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [ 950/7110]  eta: 1:46:13  lr: 0.000010  loss: 0.1724  time: 1.0120  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1000/7110]  eta: 1:45:01  lr: 0.000010  loss: 0.2581  time: 1.0067  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1050/7110]  eta: 1:43:54  lr: 0.000010  loss: 0.2741  time: 0.9580  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1100/7110]  eta: 1:42:56  lr: 0.000010  loss: 0.3480  time: 1.0201  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1150/7110]  eta: 1:41:55  lr: 0.000010  loss: 0.2071  time: 1.0230  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1200/7110]  eta: 1:40:52  lr: 0.000010  loss: 0.2442  time: 0.9394  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1250/7110]  eta: 1:39:52  lr: 0.000010  loss: 0.1456  time: 0.9773  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1300/7110]  eta: 1:38:54  lr: 0.000010  loss: 0.1334  time: 0.9865  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1350/7110]  eta: 1:37:56  lr: 0.000010  loss: 0.0529  time: 0.9919  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1400/7110]  eta: 1:37:09  lr: 0.000010  loss: 0.0213  time: 1.0642  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1450/7110]  eta: 1:36:13  lr: 0.000010  loss: 0.3551  time: 1.0226  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1500/7110]  eta: 1:35:12  lr: 0.000010  loss: 0.2726  time: 0.9767  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1550/7110]  eta: 1:34:22  lr: 0.000010  loss: 0.1920  time: 1.0325  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1600/7110]  eta: 1:33:28  lr: 0.000010  loss: 0.0699  time: 0.9887  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1650/7110]  eta: 1:32:29  lr: 0.000010  loss: 0.6537  time: 0.9658  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1700/7110]  eta: 1:31:28  lr: 0.000010  loss: 0.3403  time: 0.9348  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1750/7110]  eta: 1:30:33  lr: 0.000010  loss: 0.0967  time: 1.0097  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1800/7110]  eta: 1:29:45  lr: 0.000010  loss: 0.1332  time: 1.0446  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1850/7110]  eta: 1:29:02  lr: 0.000010  loss: 0.0170  time: 1.0127  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1900/7110]  eta: 1:28:11  lr: 0.000010  loss: 0.2272  time: 0.9695  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [1950/7110]  eta: 1:27:16  lr: 0.000010  loss: 0.2331  time: 0.9516  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2000/7110]  eta: 1:26:24  lr: 0.000010  loss: 0.2884  time: 0.9988  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2050/7110]  eta: 1:25:37  lr: 0.000010  loss: 0.2197  time: 1.0564  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2100/7110]  eta: 1:24:49  lr: 0.000010  loss: 0.3356  time: 1.0238  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2150/7110]  eta: 1:23:55  lr: 0.000010  loss: 0.4860  time: 1.0098  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2200/7110]  eta: 1:23:03  lr: 0.000010  loss: 0.2180  time: 1.0036  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2250/7110]  eta: 1:22:11  lr: 0.000010  loss: 0.0590  time: 1.0177  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2300/7110]  eta: 1:21:21  lr: 0.000010  loss: 0.0444  time: 1.0477  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2350/7110]  eta: 1:20:31  lr: 0.000010  loss: 0.3330  time: 1.0069  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2400/7110]  eta: 1:19:39  lr: 0.000010  loss: 0.0731  time: 0.9915  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2450/7110]  eta: 1:18:46  lr: 0.000010  loss: 0.3279  time: 1.0237  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2500/7110]  eta: 1:17:54  lr: 0.000010  loss: 0.3044  time: 0.9805  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2550/7110]  eta: 1:16:58  lr: 0.000010  loss: 0.1286  time: 0.9745  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2600/7110]  eta: 1:16:06  lr: 0.000010  loss: 0.0153  time: 0.9809  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2650/7110]  eta: 1:15:11  lr: 0.000010  loss: 0.1161  time: 0.9714  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2700/7110]  eta: 1:14:22  lr: 0.000010  loss: 0.4348  time: 1.0191  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2750/7110]  eta: 1:13:31  lr: 0.000010  loss: 0.2234  time: 0.9889  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2800/7110]  eta: 1:12:37  lr: 0.000010  loss: 0.3020  time: 0.9646  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2850/7110]  eta: 1:11:46  lr: 0.000010  loss: 0.3451  time: 1.0567  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2900/7110]  eta: 1:10:55  lr: 0.000010  loss: 0.1267  time: 1.0324  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [2950/7110]  eta: 1:10:05  lr: 0.000010  loss: 0.0496  time: 0.9879  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3000/7110]  eta: 1:09:14  lr: 0.000010  loss: 0.3441  time: 1.0548  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3050/7110]  eta: 1:08:25  lr: 0.000010  loss: 0.4432  time: 0.9949  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3100/7110]  eta: 1:07:36  lr: 0.000010  loss: 1.7305  time: 1.0029  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3150/7110]  eta: 1:06:46  lr: 0.000010  loss: 0.0566  time: 1.0131  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3200/7110]  eta: 1:05:54  lr: 0.000010  loss: 0.0246  time: 1.0077  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3250/7110]  eta: 1:05:06  lr: 0.000010  loss: 0.1378  time: 1.0282  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3300/7110]  eta: 1:04:13  lr: 0.000010  loss: 0.1674  time: 0.9713  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3350/7110]  eta: 1:03:23  lr: 0.000010  loss: 0.1297  time: 1.0138  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3400/7110]  eta: 1:02:33  lr: 0.000010  loss: 0.2392  time: 1.0168  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3450/7110]  eta: 1:01:41  lr: 0.000010  loss: 0.1691  time: 0.9686  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3500/7110]  eta: 1:00:49  lr: 0.000010  loss: 0.0790  time: 0.9947  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3550/7110]  eta: 0:59:59  lr: 0.000010  loss: 0.6483  time: 1.0768  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3600/7110]  eta: 0:59:07  lr: 0.000010  loss: 0.6323  time: 1.0096  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3650/7110]  eta: 0:58:17  lr: 0.000010  loss: 0.1365  time: 1.0852  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3700/7110]  eta: 0:57:26  lr: 0.000010  loss: 0.0241  time: 0.9472  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3750/7110]  eta: 0:56:34  lr: 0.000010  loss: 0.1433  time: 0.9987  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3800/7110]  eta: 0:55:44  lr: 0.000010  loss: 0.0960  time: 1.0269  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3850/7110]  eta: 0:54:55  lr: 0.000010  loss: 0.1697  time: 1.0416  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3900/7110]  eta: 0:54:06  lr: 0.000010  loss: 0.1513  time: 1.0518  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [3950/7110]  eta: 0:53:15  lr: 0.000010  loss: 0.1008  time: 1.0118  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4000/7110]  eta: 0:52:24  lr: 0.000010  loss: 1.7283  time: 1.0136  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4050/7110]  eta: 0:51:33  lr: 0.000010  loss: 0.1990  time: 1.0368  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4100/7110]  eta: 0:50:41  lr: 0.000010  loss: 0.0984  time: 0.9143  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4150/7110]  eta: 0:49:49  lr: 0.000010  loss: 0.6724  time: 1.0064  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4200/7110]  eta: 0:48:59  lr: 0.000010  loss: 0.1866  time: 1.0121  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4250/7110]  eta: 0:48:08  lr: 0.000010  loss: 1.1201  time: 1.0417  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4300/7110]  eta: 0:47:17  lr: 0.000010  loss: 0.1697  time: 0.9562  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4350/7110]  eta: 0:46:27  lr: 0.000010  loss: 0.5196  time: 1.1002  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4400/7110]  eta: 0:45:36  lr: 0.000010  loss: 0.1694  time: 1.0200  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4450/7110]  eta: 0:44:47  lr: 0.000010  loss: 0.1020  time: 1.0221  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4500/7110]  eta: 0:43:56  lr: 0.000010  loss: 0.0930  time: 1.0259  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4550/7110]  eta: 0:43:05  lr: 0.000010  loss: 0.1236  time: 0.9866  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4600/7110]  eta: 0:42:14  lr: 0.000010  loss: 0.2825  time: 0.9791  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4650/7110]  eta: 0:41:24  lr: 0.000010  loss: 0.0590  time: 1.0062  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4700/7110]  eta: 0:40:34  lr: 0.000010  loss: 0.2987  time: 0.9904  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4750/7110]  eta: 0:39:43  lr: 0.000010  loss: 0.3904  time: 0.9834  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4800/7110]  eta: 0:38:52  lr: 0.000010  loss: 0.1702  time: 0.9811  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4850/7110]  eta: 0:38:00  lr: 0.000010  loss: 0.0616  time: 0.9550  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4900/7110]  eta: 0:37:09  lr: 0.000010  loss: 0.4289  time: 0.9572  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [4950/7110]  eta: 0:36:19  lr: 0.000010  loss: 0.0847  time: 1.0080  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5000/7110]  eta: 0:35:28  lr: 0.000010  loss: 0.0585  time: 1.0582  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5050/7110]  eta: 0:34:38  lr: 0.000010  loss: 0.4447  time: 1.0122  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5100/7110]  eta: 0:33:47  lr: 0.000010  loss: 0.1547  time: 1.0369  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5150/7110]  eta: 0:32:57  lr: 0.000010  loss: 0.3239  time: 1.0074  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5200/7110]  eta: 0:32:06  lr: 0.000010  loss: 0.4650  time: 1.0009  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5250/7110]  eta: 0:31:16  lr: 0.000010  loss: 1.1565  time: 1.0350  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5300/7110]  eta: 0:30:26  lr: 0.000010  loss: 0.4543  time: 1.0190  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5350/7110]  eta: 0:29:35  lr: 0.000010  loss: 0.0692  time: 0.9616  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5400/7110]  eta: 0:28:43  lr: 0.000010  loss: 0.0733  time: 0.9669  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5450/7110]  eta: 0:27:53  lr: 0.000010  loss: 0.4071  time: 0.9934  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5500/7110]  eta: 0:27:02  lr: 0.000010  loss: 0.1119  time: 0.9577  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5550/7110]  eta: 0:26:11  lr: 0.000010  loss: 0.4927  time: 0.9359  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5600/7110]  eta: 0:25:20  lr: 0.000010  loss: 0.2967  time: 0.9595  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5650/7110]  eta: 0:24:30  lr: 0.000010  loss: 0.3402  time: 1.0030  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5700/7110]  eta: 0:23:39  lr: 0.000010  loss: 0.0544  time: 0.9262  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5750/7110]  eta: 0:22:49  lr: 0.000010  loss: 0.1419  time: 1.0398  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5800/7110]  eta: 0:21:58  lr: 0.000010  loss: 0.0622  time: 1.0291  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5850/7110]  eta: 0:21:08  lr: 0.000010  loss: 0.1070  time: 0.9655  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5900/7110]  eta: 0:20:17  lr: 0.000010  loss: 0.1290  time: 0.9379  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [5950/7110]  eta: 0:19:27  lr: 0.000010  loss: 0.0624  time: 1.0414  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6000/7110]  eta: 0:18:37  lr: 0.000010  loss: 0.2303  time: 0.9887  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6050/7110]  eta: 0:17:46  lr: 0.000010  loss: 0.1006  time: 0.9489  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6100/7110]  eta: 0:16:56  lr: 0.000010  loss: 0.1891  time: 0.9743  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6150/7110]  eta: 0:16:05  lr: 0.000010  loss: 0.3239  time: 0.9560  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6200/7110]  eta: 0:15:14  lr: 0.000010  loss: 0.4375  time: 0.9426  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6250/7110]  eta: 0:14:24  lr: 0.000010  loss: 0.1226  time: 1.0212  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6300/7110]  eta: 0:13:34  lr: 0.000010  loss: 0.0289  time: 1.0142  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6350/7110]  eta: 0:12:44  lr: 0.000010  loss: 0.0176  time: 0.9580  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6400/7110]  eta: 0:11:53  lr: 0.000010  loss: 0.2582  time: 0.9665  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6450/7110]  eta: 0:11:03  lr: 0.000010  loss: 0.0927  time: 1.0981  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6500/7110]  eta: 0:10:13  lr: 0.000010  loss: 0.0912  time: 1.0420  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6550/7110]  eta: 0:09:23  lr: 0.000010  loss: 0.3552  time: 1.0324  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6600/7110]  eta: 0:08:32  lr: 0.000010  loss: 0.0795  time: 0.9324  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6650/7110]  eta: 0:07:42  lr: 0.000010  loss: 0.6487  time: 0.9979  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6700/7110]  eta: 0:06:52  lr: 0.000010  loss: 0.2936  time: 1.0148  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.0468  time: 1.0266  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.1953  time: 0.9497  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6850/7110]  eta: 0:04:21  lr: 0.000010  loss: 0.2755  time: 1.0888  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6900/7110]  eta: 0:03:31  lr: 0.000010  loss: 0.0822  time: 1.0299  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [6950/7110]  eta: 0:02:40  lr: 0.000010  loss: 0.0723  time: 0.9851  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [7000/7110]  eta: 0:01:50  lr: 0.000010  loss: 0.1313  time: 1.0221  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [7050/7110]  eta: 0:01:00  lr: 0.000010  loss: 0.1472  time: 0.9354  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [7100/7110]  eta: 0:00:10  lr: 0.000010  loss: 0.0346  time: 0.9863  data: 0.0000  max mem: 66110
Train: data epoch: [56]  [7109/7110]  eta: 0:00:01  lr: 0.000010  loss: 0.2186  time: 1.0767  data: 0.0000  max mem: 66110
Train: data epoch: [56] Total time: 1:59:08 (1.0054 s / it)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Evaluation  [   0/1093]  eta: 6:27:55    time: 21.2950  data: 20.0370  max mem: 66110
Evaluation  [  10/1093]  eta: 0:57:55    time: 3.2088  data: 1.8225  max mem: 66110
Evaluation  [  20/1093]  eta: 0:41:36    time: 1.3780  data: 0.0010  max mem: 66110
Evaluation  [  30/1093]  eta: 0:34:34    time: 1.2606  data: 0.0010  max mem: 66110
Evaluation  [  40/1093]  eta: 0:31:58    time: 1.2919  data: 0.0009  max mem: 66110
Evaluation  [  50/1093]  eta: 0:30:21    time: 1.4270  data: 0.0009  max mem: 66110
Evaluation  [  60/1093]  eta: 0:28:24    time: 1.2973  data: 0.0009  max mem: 66110
Evaluation  [  70/1093]  eta: 0:27:38    time: 1.3023  data: 0.0010  max mem: 66110
Evaluation  [  80/1093]  eta: 0:26:56    time: 1.4308  data: 0.0010  max mem: 66110
Evaluation  [  90/1093]  eta: 0:26:24    time: 1.4322  data: 0.0010  max mem: 66110
Evaluation  [ 100/1093]  eta: 0:26:02    time: 1.4816  data: 0.0010  max mem: 66110
Evaluation  [ 110/1093]  eta: 0:25:38    time: 1.4989  data: 0.0010  max mem: 66110
Evaluation  [ 120/1093]  eta: 0:24:57    time: 1.3663  data: 0.0010  max mem: 66110
Evaluation  [ 130/1093]  eta: 0:24:40    time: 1.3822  data: 0.0010  max mem: 66110
Evaluation  [ 140/1093]  eta: 0:24:07    time: 1.3998  data: 0.0010  max mem: 66110
Evaluation  [ 150/1093]  eta: 0:23:39    time: 1.2936  data: 0.0009  max mem: 66110
Evaluation  [ 160/1093]  eta: 0:23:16    time: 1.3382  data: 0.0010  max mem: 66110
Evaluation  [ 170/1093]  eta: 0:22:47    time: 1.3031  data: 0.0010  max mem: 66110
Evaluation  [ 180/1093]  eta: 0:22:28    time: 1.3213  data: 0.0010  max mem: 66110
Evaluation  [ 190/1093]  eta: 0:22:19    time: 1.4990  data: 0.0010  max mem: 66110
Evaluation  [ 200/1093]  eta: 0:21:57    time: 1.4565  data: 0.0010  max mem: 66110
Evaluation  [ 210/1093]  eta: 0:21:47    time: 1.4591  data: 0.0010  max mem: 66110
Evaluation  [ 220/1093]  eta: 0:21:23    time: 1.4161  data: 0.0010  max mem: 66110
Evaluation  [ 230/1093]  eta: 0:21:00    time: 1.2513  data: 0.0010  max mem: 66110
Evaluation  [ 240/1093]  eta: 0:20:41    time: 1.2955  data: 0.0009  max mem: 66110
Evaluation  [ 250/1093]  eta: 0:20:29    time: 1.4267  data: 0.0009  max mem: 66110
Evaluation  [ 260/1093]  eta: 0:20:21    time: 1.5969  data: 0.0010  max mem: 66110
Evaluation  [ 270/1093]  eta: 0:19:58    time: 1.4368  data: 0.0010  max mem: 66110
Evaluation  [ 280/1093]  eta: 0:19:40    time: 1.2694  data: 0.0010  max mem: 66110
Evaluation  [ 290/1093]  eta: 0:19:32    time: 1.5077  data: 0.0010  max mem: 66110
Evaluation  [ 300/1093]  eta: 0:19:10    time: 1.4281  data: 0.0009  max mem: 66110
Evaluation  [ 310/1093]  eta: 0:18:49    time: 1.1892  data: 0.0009  max mem: 66110
Evaluation  [ 320/1093]  eta: 0:18:33    time: 1.2920  data: 0.0010  max mem: 66110
Evaluation  [ 330/1093]  eta: 0:18:16    time: 1.3401  data: 0.0010  max mem: 66110
Evaluation  [ 340/1093]  eta: 0:18:03    time: 1.4139  data: 0.0010  max mem: 66110
Evaluation  [ 350/1093]  eta: 0:17:49    time: 1.4760  data: 0.0011  max mem: 66110
Evaluation  [ 360/1093]  eta: 0:17:30    time: 1.3340  data: 0.0011  max mem: 66110
Evaluation  [ 370/1093]  eta: 0:17:14    time: 1.2972  data: 0.0010  max mem: 66110
Evaluation  [ 380/1093]  eta: 0:17:03    time: 1.4611  data: 0.0010  max mem: 66110
Evaluation  [ 390/1093]  eta: 0:16:46    time: 1.4352  data: 0.0009  max mem: 66110
Evaluation  [ 400/1093]  eta: 0:16:33    time: 1.3991  data: 0.0009  max mem: 66110
Evaluation  [ 410/1093]  eta: 0:16:18    time: 1.4544  data: 0.0009  max mem: 66110
Evaluation  [ 420/1093]  eta: 0:16:03    time: 1.3882  data: 0.0009  max mem: 66110
Evaluation  [ 430/1093]  eta: 0:15:47    time: 1.3642  data: 0.0009  max mem: 66110
Evaluation  [ 440/1093]  eta: 0:15:28    time: 1.2422  data: 0.0010  max mem: 66110
Evaluation  [ 450/1093]  eta: 0:15:14    time: 1.2752  data: 0.0011  max mem: 66110
Evaluation  [ 460/1093]  eta: 0:14:59    time: 1.3986  data: 0.0011  max mem: 66110
Evaluation  [ 470/1093]  eta: 0:14:44    time: 1.3400  data: 0.0011  max mem: 66110
Evaluation  [ 480/1093]  eta: 0:14:30    time: 1.3855  data: 0.0010  max mem: 66110
Evaluation  [ 490/1093]  eta: 0:14:17    time: 1.4787  data: 0.0010  max mem: 66110
Evaluation  [ 500/1093]  eta: 0:14:05    time: 1.5575  data: 0.0010  max mem: 66110
Evaluation  [ 510/1093]  eta: 0:13:52    time: 1.6055  data: 0.0010  max mem: 66110
Evaluation  [ 520/1093]  eta: 0:13:38    time: 1.5137  data: 0.0011  max mem: 66110
Evaluation  [ 530/1093]  eta: 0:13:24    time: 1.4443  data: 0.0011  max mem: 66110
Evaluation  [ 540/1093]  eta: 0:13:10    time: 1.4528  data: 0.0011  max mem: 66110
Evaluation  [ 550/1093]  eta: 0:12:56    time: 1.4304  data: 0.0011  max mem: 66110
Evaluation  [ 560/1093]  eta: 0:12:38    time: 1.2630  data: 0.0009  max mem: 66110
Evaluation  [ 570/1093]  eta: 0:12:23    time: 1.2336  data: 0.0009  max mem: 66110
Evaluation  [ 580/1093]  eta: 0:12:09    time: 1.3692  data: 0.0009  max mem: 66110
Evaluation  [ 590/1093]  eta: 0:11:52    time: 1.2558  data: 0.0009  max mem: 66110
Evaluation  [ 600/1093]  eta: 0:11:39    time: 1.3509  data: 0.0009  max mem: 66110
Evaluation  [ 610/1093]  eta: 0:11:25    time: 1.5226  data: 0.0009  max mem: 66110
Evaluation  [ 620/1093]  eta: 0:11:11    time: 1.4190  data: 0.0010  max mem: 66110
Evaluation  [ 630/1093]  eta: 0:10:57    time: 1.3821  data: 0.0010  max mem: 66110
Evaluation  [ 640/1093]  eta: 0:10:42    time: 1.3559  data: 0.0011  max mem: 66110
Evaluation  [ 650/1093]  eta: 0:10:28    time: 1.3925  data: 0.0012  max mem: 66110
Evaluation  [ 660/1093]  eta: 0:10:13    time: 1.3646  data: 0.0011  max mem: 66110
Evaluation  [ 670/1093]  eta: 0:09:59    time: 1.3597  data: 0.0010  max mem: 66110
Evaluation  [ 680/1093]  eta: 0:09:45    time: 1.4424  data: 0.0010  max mem: 66110
Evaluation  [ 690/1093]  eta: 0:09:31    time: 1.4944  data: 0.0010  max mem: 66110
Evaluation  [ 700/1093]  eta: 0:09:17    time: 1.4568  data: 0.0010  max mem: 66110
Evaluation  [ 710/1093]  eta: 0:09:01    time: 1.2425  data: 0.0010  max mem: 66110
Evaluation  [ 720/1093]  eta: 0:08:46    time: 1.2392  data: 0.0009  max mem: 66110
Evaluation  [ 730/1093]  eta: 0:08:31    time: 1.1991  data: 0.0009  max mem: 66110
Evaluation  [ 740/1093]  eta: 0:08:16    time: 1.1500  data: 0.0009  max mem: 66110
Evaluation  [ 750/1093]  eta: 0:08:02    time: 1.3471  data: 0.0009  max mem: 66110
Evaluation  [ 760/1093]  eta: 0:07:48    time: 1.4140  data: 0.0010  max mem: 66110
Evaluation  [ 770/1093]  eta: 0:07:33    time: 1.3792  data: 0.0010  max mem: 66110
Evaluation  [ 780/1093]  eta: 0:07:20    time: 1.4414  data: 0.0010  max mem: 66110
Evaluation  [ 790/1093]  eta: 0:07:05    time: 1.2952  data: 0.0010  max mem: 66110
Evaluation  [ 800/1093]  eta: 0:06:51    time: 1.3520  data: 0.0010  max mem: 66110
Evaluation  [ 810/1093]  eta: 0:06:37    time: 1.5304  data: 0.0011  max mem: 66110
Evaluation  [ 820/1093]  eta: 0:06:24    time: 1.4559  data: 0.0010  max mem: 66110
Evaluation  [ 830/1093]  eta: 0:06:10    time: 1.5493  data: 0.0010  max mem: 66110
Evaluation  [ 840/1093]  eta: 0:05:56    time: 1.4914  data: 0.0010  max mem: 66110
Evaluation  [ 850/1093]  eta: 0:05:42    time: 1.4221  data: 0.0010  max mem: 66110
Evaluation  [ 860/1093]  eta: 0:05:27    time: 1.3648  data: 0.0010  max mem: 66110
Evaluation  [ 870/1093]  eta: 0:05:13    time: 1.3347  data: 0.0010  max mem: 66110
Evaluation  [ 880/1093]  eta: 0:05:00    time: 1.4881  data: 0.0010  max mem: 66110
Evaluation  [ 890/1093]  eta: 0:04:45    time: 1.3412  data: 0.0010  max mem: 66110
Evaluation  [ 900/1093]  eta: 0:04:31    time: 1.2775  data: 0.0010  max mem: 66110
Evaluation  [ 910/1093]  eta: 0:04:17    time: 1.4310  data: 0.0010  max mem: 66110
Evaluation  [ 920/1093]  eta: 0:04:03    time: 1.4156  data: 0.0010  max mem: 66110
Evaluation  [ 930/1093]  eta: 0:03:49    time: 1.3485  data: 0.0010  max mem: 66110
Evaluation  [ 940/1093]  eta: 0:03:35    time: 1.3803  data: 0.0010  max mem: 66110
Evaluation  [ 950/1093]  eta: 0:03:20    time: 1.3804  data: 0.0010  max mem: 66110
Evaluation  [ 960/1093]  eta: 0:03:06    time: 1.3916  data: 0.0010  max mem: 66110
Evaluation  [ 970/1093]  eta: 0:02:52    time: 1.4301  data: 0.0010  max mem: 66110
Evaluation  [ 980/1093]  eta: 0:02:38    time: 1.2829  data: 0.0010  max mem: 66110
Evaluation  [ 990/1093]  eta: 0:02:24    time: 1.3393  data: 0.0010  max mem: 66110
Evaluation  [1000/1093]  eta: 0:02:10    time: 1.4989  data: 0.0010  max mem: 66110
Evaluation  [1010/1093]  eta: 0:01:56    time: 1.4440  data: 0.0009  max mem: 66110
Evaluation  [1020/1093]  eta: 0:01:42    time: 1.2785  data: 0.0009  max mem: 66110
Evaluation  [1030/1093]  eta: 0:01:28    time: 1.1403  data: 0.0010  max mem: 66110
Evaluation  [1040/1093]  eta: 0:01:14    time: 1.2310  data: 0.0010  max mem: 66110
Evaluation  [1050/1093]  eta: 0:01:00    time: 1.3514  data: 0.0010  max mem: 66110
Evaluation  [1060/1093]  eta: 0:00:46    time: 1.4243  data: 0.0010  max mem: 66110
Evaluation  [1070/1093]  eta: 0:00:32    time: 1.4616  data: 0.0010  max mem: 66110
Evaluation  [1080/1093]  eta: 0:00:18    time: 1.3645  data: 0.0010  max mem: 66110
Evaluation  [1090/1093]  eta: 0:00:04    time: 1.3368  data: 0.0010  max mem: 66110
Evaluation  [1092/1093]  eta: 0:00:01    time: 1.3142  data: 0.0463  max mem: 66110
Evaluation Total time: 0:25:29 (1.3993 s / it)
result file saved to /cpfs01/user/zhouyunsong/zhouys/Git_repos/3DVLM/LAVIS/lavis/output/BLIP2/3DQA/20230907071/result/val_56_vqa_result.json
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Train: data epoch: [57]  [   0/7110]  eta: 2 days, 5:02:01  lr: 0.000010  loss: 0.0911  time: 26.8525  data: 0.0001  max mem: 66110
Train: data epoch: [57]  [  50/7110]  eta: 2:56:24  lr: 0.000010  loss: 0.2639  time: 0.9653  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 100/7110]  eta: 2:25:13  lr: 0.000010  loss: 0.1929  time: 1.0593  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 150/7110]  eta: 2:14:38  lr: 0.000010  loss: 0.0388  time: 0.9891  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 200/7110]  eta: 2:09:00  lr: 0.000010  loss: 0.0462  time: 1.0192  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 250/7110]  eta: 2:05:13  lr: 0.000010  loss: 1.3683  time: 0.9595  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 300/7110]  eta: 2:02:51  lr: 0.000010  loss: 0.0087  time: 1.0266  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 350/7110]  eta: 2:00:40  lr: 0.000010  loss: 0.0397  time: 0.9969  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 400/7110]  eta: 1:59:05  lr: 0.000010  loss: 0.3784  time: 1.0068  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 450/7110]  eta: 1:57:14  lr: 0.000010  loss: 0.1776  time: 1.0267  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 500/7110]  eta: 1:56:31  lr: 0.000010  loss: 0.3211  time: 1.0450  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 550/7110]  eta: 1:55:00  lr: 0.000010  loss: 0.0556  time: 1.0012  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 600/7110]  eta: 1:53:55  lr: 0.000010  loss: 0.2312  time: 1.0375  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 650/7110]  eta: 1:52:31  lr: 0.000010  loss: 0.1275  time: 0.9477  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 700/7110]  eta: 1:51:29  lr: 0.000010  loss: 0.0634  time: 1.0150  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 750/7110]  eta: 1:49:59  lr: 0.000010  loss: 0.1361  time: 0.9400  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 800/7110]  eta: 1:48:37  lr: 0.000010  loss: 0.1038  time: 0.9238  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 850/7110]  eta: 1:47:35  lr: 0.000010  loss: 0.0173  time: 0.9509  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 900/7110]  eta: 1:46:49  lr: 0.000010  loss: 0.2376  time: 1.0150  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [ 950/7110]  eta: 1:45:40  lr: 0.000010  loss: 0.1802  time: 0.9523  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1000/7110]  eta: 1:44:37  lr: 0.000010  loss: 0.0370  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1050/7110]  eta: 1:43:41  lr: 0.000010  loss: 0.3738  time: 0.9910  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1100/7110]  eta: 1:42:34  lr: 0.000010  loss: 0.0713  time: 0.9535  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1150/7110]  eta: 1:41:30  lr: 0.000010  loss: 0.1793  time: 0.9969  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1200/7110]  eta: 1:40:27  lr: 0.000010  loss: 0.0151  time: 0.9903  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1250/7110]  eta: 1:39:39  lr: 0.000010  loss: 0.3578  time: 1.0420  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1300/7110]  eta: 1:38:45  lr: 0.000010  loss: 0.0245  time: 1.0490  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1350/7110]  eta: 1:37:50  lr: 0.000010  loss: 0.0167  time: 0.9656  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1400/7110]  eta: 1:36:51  lr: 0.000010  loss: 0.3396  time: 0.9600  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1450/7110]  eta: 1:35:57  lr: 0.000010  loss: 0.2204  time: 0.9774  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1500/7110]  eta: 1:35:01  lr: 0.000010  loss: 0.0256  time: 0.9821  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1550/7110]  eta: 1:34:09  lr: 0.000010  loss: 0.2094  time: 0.9778  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1600/7110]  eta: 1:33:21  lr: 0.000010  loss: 0.0535  time: 1.0317  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1650/7110]  eta: 1:32:30  lr: 0.000010  loss: 0.8335  time: 1.0411  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1700/7110]  eta: 1:31:40  lr: 0.000010  loss: 0.2217  time: 1.0814  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1750/7110]  eta: 1:30:43  lr: 0.000010  loss: 0.0772  time: 0.9902  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1800/7110]  eta: 1:29:51  lr: 0.000010  loss: 0.3537  time: 1.0071  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1850/7110]  eta: 1:28:58  lr: 0.000010  loss: 0.0421  time: 0.9967  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1900/7110]  eta: 1:28:08  lr: 0.000010  loss: 0.0874  time: 1.0614  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [1950/7110]  eta: 1:27:13  lr: 0.000010  loss: 0.2857  time: 1.0120  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2000/7110]  eta: 1:26:20  lr: 0.000010  loss: 0.1190  time: 0.9389  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2050/7110]  eta: 1:25:29  lr: 0.000010  loss: 0.0062  time: 1.0327  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2100/7110]  eta: 1:24:32  lr: 0.000010  loss: 0.1938  time: 0.9949  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2150/7110]  eta: 1:23:36  lr: 0.000010  loss: 0.0119  time: 0.9944  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2200/7110]  eta: 1:22:41  lr: 0.000010  loss: 0.1967  time: 0.9856  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2250/7110]  eta: 1:21:47  lr: 0.000010  loss: 0.0393  time: 0.9390  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2300/7110]  eta: 1:20:53  lr: 0.000010  loss: 0.0704  time: 0.9562  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2350/7110]  eta: 1:20:03  lr: 0.000010  loss: 0.0810  time: 1.0024  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2400/7110]  eta: 1:19:11  lr: 0.000010  loss: 0.2763  time: 1.0465  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2450/7110]  eta: 1:18:18  lr: 0.000010  loss: 0.1595  time: 1.0326  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2500/7110]  eta: 1:17:25  lr: 0.000010  loss: 0.2689  time: 0.9634  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2550/7110]  eta: 1:16:34  lr: 0.000010  loss: 0.1201  time: 1.0835  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2600/7110]  eta: 1:15:43  lr: 0.000010  loss: 0.1883  time: 0.9548  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2650/7110]  eta: 1:14:53  lr: 0.000010  loss: 0.0604  time: 1.0075  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2700/7110]  eta: 1:13:59  lr: 0.000010  loss: 0.1908  time: 0.9338  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2750/7110]  eta: 1:13:10  lr: 0.000010  loss: 0.0446  time: 0.9997  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2800/7110]  eta: 1:12:19  lr: 0.000010  loss: 0.2884  time: 0.9848  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2850/7110]  eta: 1:11:33  lr: 0.000010  loss: 0.2628  time: 1.0758  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2900/7110]  eta: 1:10:44  lr: 0.000010  loss: 0.0545  time: 1.0302  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [2950/7110]  eta: 1:09:55  lr: 0.000010  loss: 0.1673  time: 1.0832  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3000/7110]  eta: 1:09:06  lr: 0.000010  loss: 0.3774  time: 1.0210  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3050/7110]  eta: 1:08:14  lr: 0.000010  loss: 0.1642  time: 0.9487  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3100/7110]  eta: 1:07:25  lr: 0.000010  loss: 0.5698  time: 0.9753  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3150/7110]  eta: 1:06:35  lr: 0.000010  loss: 0.2442  time: 1.0154  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3200/7110]  eta: 1:05:42  lr: 0.000010  loss: 0.2503  time: 1.0060  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3250/7110]  eta: 1:04:51  lr: 0.000010  loss: 0.1671  time: 1.0387  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3300/7110]  eta: 1:03:59  lr: 0.000010  loss: 0.0470  time: 0.9958  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3350/7110]  eta: 1:03:07  lr: 0.000010  loss: 0.2570  time: 0.9404  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3400/7110]  eta: 1:02:13  lr: 0.000010  loss: 0.2360  time: 0.9415  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3450/7110]  eta: 1:01:23  lr: 0.000010  loss: 0.0575  time: 0.9989  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3500/7110]  eta: 1:00:32  lr: 0.000010  loss: 0.1548  time: 1.0000  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3550/7110]  eta: 0:59:41  lr: 0.000010  loss: 0.0104  time: 0.9760  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3600/7110]  eta: 0:58:50  lr: 0.000010  loss: 0.1812  time: 0.9768  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3650/7110]  eta: 0:58:00  lr: 0.000010  loss: 0.4602  time: 0.9361  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3700/7110]  eta: 0:57:10  lr: 0.000010  loss: 0.0138  time: 1.0125  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3750/7110]  eta: 0:56:18  lr: 0.000010  loss: 0.2712  time: 0.9505  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3800/7110]  eta: 0:55:28  lr: 0.000010  loss: 1.6392  time: 1.0311  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3850/7110]  eta: 0:54:37  lr: 0.000010  loss: 0.0257  time: 0.9422  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3900/7110]  eta: 0:53:47  lr: 0.000010  loss: 0.1819  time: 1.0140  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [3950/7110]  eta: 0:52:55  lr: 0.000010  loss: 1.6108  time: 0.9566  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4000/7110]  eta: 0:52:06  lr: 0.000010  loss: 0.0098  time: 1.0343  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4050/7110]  eta: 0:51:15  lr: 0.000010  loss: 0.5716  time: 0.9347  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4100/7110]  eta: 0:50:28  lr: 0.000010  loss: 0.2797  time: 1.0754  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4150/7110]  eta: 0:49:37  lr: 0.000010  loss: 0.7171  time: 1.0273  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4200/7110]  eta: 0:48:47  lr: 0.000010  loss: 0.3514  time: 1.0211  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4250/7110]  eta: 0:47:56  lr: 0.000010  loss: 0.2990  time: 1.0318  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4300/7110]  eta: 0:47:06  lr: 0.000010  loss: 0.2846  time: 1.0157  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4350/7110]  eta: 0:46:15  lr: 0.000010  loss: 0.2151  time: 0.9766  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4400/7110]  eta: 0:45:25  lr: 0.000010  loss: 0.0333  time: 1.0210  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4450/7110]  eta: 0:44:35  lr: 0.000010  loss: 0.0174  time: 1.0827  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4500/7110]  eta: 0:43:45  lr: 0.000010  loss: 0.0757  time: 1.0210  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4550/7110]  eta: 0:42:55  lr: 0.000010  loss: 0.0901  time: 1.0240  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4600/7110]  eta: 0:42:05  lr: 0.000010  loss: 0.0338  time: 1.0006  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4650/7110]  eta: 0:41:15  lr: 0.000010  loss: 0.1829  time: 1.0151  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4700/7110]  eta: 0:40:26  lr: 0.000010  loss: 0.2456  time: 1.0659  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4750/7110]  eta: 0:39:36  lr: 0.000010  loss: 0.1510  time: 1.0351  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4800/7110]  eta: 0:38:46  lr: 0.000010  loss: 0.0955  time: 0.9559  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4850/7110]  eta: 0:37:54  lr: 0.000010  loss: 0.0251  time: 1.0297  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4900/7110]  eta: 0:37:04  lr: 0.000010  loss: 0.7135  time: 0.9719  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [4950/7110]  eta: 0:36:13  lr: 0.000010  loss: 0.1581  time: 1.0076  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5000/7110]  eta: 0:35:23  lr: 0.000010  loss: 0.0818  time: 0.9993  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5050/7110]  eta: 0:34:32  lr: 0.000010  loss: 0.5354  time: 0.9655  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5100/7110]  eta: 0:33:42  lr: 0.000010  loss: 0.2693  time: 0.9739  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5150/7110]  eta: 0:32:51  lr: 0.000010  loss: 0.2548  time: 0.9943  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5200/7110]  eta: 0:32:01  lr: 0.000010  loss: 0.3637  time: 1.0017  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5250/7110]  eta: 0:31:11  lr: 0.000010  loss: 0.4523  time: 1.0219  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5300/7110]  eta: 0:30:21  lr: 0.000010  loss: 0.0353  time: 1.0353  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5350/7110]  eta: 0:29:30  lr: 0.000010  loss: 0.2489  time: 1.0106  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5400/7110]  eta: 0:28:40  lr: 0.000010  loss: 0.0766  time: 1.0024  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5450/7110]  eta: 0:27:50  lr: 0.000010  loss: 0.0480  time: 1.0862  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5500/7110]  eta: 0:27:00  lr: 0.000010  loss: 0.1578  time: 1.0253  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5550/7110]  eta: 0:26:10  lr: 0.000010  loss: 0.4066  time: 1.0100  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5600/7110]  eta: 0:25:19  lr: 0.000010  loss: 0.1971  time: 1.0143  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5650/7110]  eta: 0:24:29  lr: 0.000010  loss: 0.1425  time: 1.0470  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5700/7110]  eta: 0:23:39  lr: 0.000010  loss: 0.2028  time: 1.0448  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5750/7110]  eta: 0:22:48  lr: 0.000010  loss: 0.1779  time: 0.9965  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5800/7110]  eta: 0:21:58  lr: 0.000010  loss: 0.1867  time: 1.0234  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5850/7110]  eta: 0:21:08  lr: 0.000010  loss: 0.0610  time: 1.0137  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5900/7110]  eta: 0:20:17  lr: 0.000010  loss: 0.2661  time: 0.9444  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [5950/7110]  eta: 0:19:27  lr: 0.000010  loss: 0.2336  time: 0.9706  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6000/7110]  eta: 0:18:36  lr: 0.000010  loss: 0.0178  time: 0.9257  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6050/7110]  eta: 0:17:46  lr: 0.000010  loss: 0.5988  time: 1.0036  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6100/7110]  eta: 0:16:56  lr: 0.000010  loss: 0.0433  time: 0.9553  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6150/7110]  eta: 0:16:05  lr: 0.000010  loss: 0.3681  time: 1.0271  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6200/7110]  eta: 0:15:15  lr: 0.000010  loss: 0.2743  time: 0.9609  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6250/7110]  eta: 0:14:24  lr: 0.000010  loss: 0.0400  time: 1.0373  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6300/7110]  eta: 0:13:34  lr: 0.000010  loss: 0.2167  time: 0.9704  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6350/7110]  eta: 0:12:44  lr: 0.000010  loss: 0.0417  time: 0.9570  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6400/7110]  eta: 0:11:53  lr: 0.000010  loss: 0.0212  time: 1.0200  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6450/7110]  eta: 0:11:03  lr: 0.000010  loss: 0.0416  time: 1.0209  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6500/7110]  eta: 0:10:13  lr: 0.000010  loss: 0.0359  time: 1.0552  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6550/7110]  eta: 0:09:23  lr: 0.000010  loss: 0.2958  time: 1.0732  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6600/7110]  eta: 0:08:32  lr: 0.000010  loss: 0.3853  time: 0.9987  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6650/7110]  eta: 0:07:42  lr: 0.000010  loss: 0.1816  time: 1.0332  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6700/7110]  eta: 0:06:52  lr: 0.000010  loss: 0.1363  time: 0.9585  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6750/7110]  eta: 0:06:01  lr: 0.000010  loss: 0.5464  time: 1.0757  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6800/7110]  eta: 0:05:11  lr: 0.000010  loss: 0.0838  time: 1.0124  data: 0.0000  max mem: 66110
Train: data epoch: [57]  [6850/7110]  eta: 0:04:21  lr: 0.000010  loss: 0.3029  time: 0.9233  data: 0.0000  max mem: 66110
